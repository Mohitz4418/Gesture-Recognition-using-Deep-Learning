{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gesture Recognition: Rahul Anand\n",
    "In this group project, we are going to build a 3D model that will be able to predict the 5 gestures correctly. Please import the following libraries to get started."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "from scipy.misc import imread, imresize\n",
    "import datetime\n",
    "import os\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Dense, GRU, Flatten, TimeDistributed, Flatten, BatchNormalization, Activation,Dropout,Input,LSTM\n",
    "from keras.layers.convolutional import Conv3D, MaxPooling3D,Conv2D, MaxPooling2D\n",
    "from keras.callbacks import ModelCheckpoint, ReduceLROnPlateau\n",
    "from keras import optimizers\n",
    "import warnings\n",
    "import abc\n",
    "import cv2\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We set the random seed so that the results don't vary drastically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(30)\n",
    "import random as rn\n",
    "rn.seed(30)\n",
    "from keras import backend as K\n",
    "import tensorflow as tf\n",
    "tf.set_random_seed(30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this block, you read the folder names for training and validation. You also set the `batch_size` here. Note that you set the batch size in such a way that you are able to use the GPU in full capacity. You keep increasing the batch size until the machine throws an error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.getcwd()\n",
    "project_folder='Project_data'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_doc = np.random.permutation(open('/mnt/disks/user/project/PROJECT/Project_data/train.csv').readlines())\n",
    "val_doc = np.random.permutation(open('/mnt/disks/user/project/PROJECT/Project_data/val.csv').readlines())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Image Processor\n",
    "The objective of this function is: Resize the images as per transform_size given by user, or take it as (120,120)\n",
    "\n",
    "Reason\n",
    "\n",
    "In the training data we have 2 types of video, where video frame sizes are as (360,360) and (160,120) Therefore to keep things consistent, we build a image processor which perform cropping of the image.\n",
    "\n",
    "#### Note\n",
    "\n",
    "Cropping is done as a centered image, i.e., we take the center as the reference and crop the images from both sides. For (360,360) we can use skimage.transform.resize function. However, for (160,120) we will have to some manual processing to crop images by the center."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skimage.transform import resize\n",
    "def image_processor(orig_image,transform_size=(120,120)):\n",
    "    new_image = orig_image\n",
    "    if orig_image.shape == (360,360,3):\n",
    "        new_image = resize(orig_image,transform_size)\n",
    "    else:\n",
    "        start_row = (orig_image.shape[0] - transform_size[0])//2\n",
    "        end_row = start_row + transform_size[0]\n",
    "        \n",
    "        start_col = (orig_image.shape[1] - transform_size[1])//2\n",
    "        end_col = start_col + transform_size[1]\n",
    "        new_image = orig_image[start_row:end_row,start_col:end_col,:]\n",
    "    return(new_image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sequence Generator\n",
    "This function provides a list of frame sequences we would like to use training\n",
    "\n",
    "Arguments:choiceoflist\n",
    "\n",
    "choicelist\n",
    "0 --> range(0,30,1)<br>\n",
    "1 --> range(0,30,2)<br>\n",
    "2 --> [0,1,2,3,4,5,6,9,12,15,18,21,24,25,26,27,28,29]\n",
    "\t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getframeselectionlist(choiceoflist=0):\n",
    "    if choiceoflist==0:\n",
    "        return [frame for frame in range(0,30,1)] # Returns 100% of frames, number of frames=30\n",
    "    elif choiceoflist==1:\n",
    "        return [frame for frame in range(0,30,2)] # returns 50% of frames, number of frames=15\n",
    "    elif choiceoflist==2:\n",
    "                \n",
    "        frame_sequence=[]\n",
    "        \n",
    "        startframesequence=[0,1,2,3,4,5]\n",
    "        endframesequence=[25,26,27,28,29]\n",
    "        skip_sequence=3  #\n",
    "        middleframesequnce=[k for k in range(6,25,skip_sequence)]\n",
    "        \n",
    "        frame_sequence.extend(startframesequence)\n",
    "        frame_sequence.extend(middleframesequnce)\n",
    "        frame_sequence.extend(endframesequence)\n",
    "        \n",
    "        return frame_sequence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generator\n",
    "This is heart of complete training process. It pumps batched data to network during learning and prediction both. The function description is given below\n",
    "\n",
    "#### Arguments\n",
    "\n",
    "1. Source Path - Directory path to be considered for reading video/images frames\n",
    "2. folder_list - Lines from the train_doc we read above.\n",
    "3. batch_size - The batch_size we want to select.\n",
    "4. transform_size - The image transformation size we (Default - (120,120)\n",
    "5. frame_selection_list - frame_list obtained from frame_generator (Default - range(30))."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generator(source_path,\n",
    "              folder_list,\n",
    "              batch_size,\n",
    "              transform_size = (120,120),\n",
    "              frame_selection_list = range(30)):\n",
    "    print( 'Source path = ', source_path, '; batch size =', batch_size)\n",
    "    img_idx = frame_selection_list\n",
    "    channels = 3\n",
    "    while True:\n",
    "        t = np.random.permutation(folder_list)\n",
    "        num_batches = int(len(t) / batch_size) if len(t) % batch_size == 0 else (len(t) // batch_size)\n",
    "        for batch in range(num_batches): # we iterate over the number of batches\n",
    "            batch_data = np.zeros((batch_size,len(frame_selection_list),transform_size[0],transform_size[1],channels)) # x is the number of images you use for each video, (y,z) is the final size of the input images and 3 is the number of channels RGB\n",
    "            batch_labels = np.zeros((batch_size,5)) # batch_labels is the one hot representation of the output\n",
    "            for folder in range(batch_size): # iterate over the batch_size\n",
    "                imgs = os.listdir(source_path+'/'+ t[folder + (batch*batch_size)].split(';')[0]) # read all the images in the folder\n",
    "                #print(t[folder + (batch*batch_size)].split(';')[0])\n",
    "                for idx,item in enumerate(img_idx): #  Iterate iver the frames/images of a folder to read them in\n",
    "                    image = imread(source_path+'/'+ t[folder + (batch*batch_size)].strip().split(';')[0]+'/'+imgs[item]).astype(np.float32)\n",
    "                    #crop the images and resize them. Note that the images are of 2 different shape \n",
    "                    #and the conv3D will throw error if the inputs in a batch have different shapes\n",
    "                    image = image_processor(image,transform_size)\n",
    "                    # This is when we are building a Conv3D network\n",
    "                    image = image/255\n",
    "\n",
    "                    batch_data[folder,idx,:,:,0] = image[:,:,0]\n",
    "                    batch_data[folder,idx,:,:,1] = image[:,:,1]\n",
    "                    batch_data[folder,idx,:,:,2] = image[:,:,2]\n",
    "                    \n",
    "                batch_labels[folder, int(t[folder + (batch*batch_size)].strip().split(';')[2])] = 1\n",
    "            yield batch_data, batch_labels #you yield the batch_data and the batch_labels, remember what does yield do\n",
    "        \n",
    "        # write the code for the remaining data points which are left after full batches\n",
    "        # remaining data #\n",
    "        if len(t) % batch_size != 0:                                      # Execute only if, we need to\n",
    "            batch_data = np.zeros((len(t) % batch_size,len(frame_selection_list),transform_size[0],transform_size[1],channels))     # fix the last batch size\n",
    "            batch_labels = np.zeros((len(t) % batch_size,5))              # Similarly, for labels\n",
    "            for v_idx,folder in enumerate(t[(num_batches*batch_size):]):\n",
    "                #print(folder.split(';')[0])\n",
    "                imgs = os.listdir(source_path+\"/\"+folder.split(';')[0])\n",
    "                for idx, item in enumerate(img_idx):\n",
    "                    image = imread(source_path+'/'+folder.split(';')[0]+\"/\"+imgs[item]).astype(np.float32)\n",
    "                    image = image_processor(image,transform_size)\n",
    "                    # This is when we are building a Conv3D network\n",
    "                    image = image/255\n",
    "\n",
    "                    batch_data[v_idx,idx,:,:,0] = image[:,:,0]\n",
    "                    batch_data[v_idx,idx,:,:,1] = image[:,:,1]\n",
    "                    batch_data[v_idx,idx,:,:,0] = image[:,:,2]\n",
    "                batch_labels[v_idx,int(folder.split(\";\")[2])] = 1\n",
    "            yield batch_data,batch_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note here that a video is represented above in the generator as (number of images, height, width, number of channels). Take this into consideration while creating the model architecture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# training sequences = 663\n",
      "# validation sequences = 100\n"
     ]
    }
   ],
   "source": [
    "curr_dt_time = datetime.datetime.now()\n",
    "train_path = '/mnt/disks/user/project/PROJECT/Project_data/train'\n",
    "val_path = '/mnt/disks/user/project/PROJECT/Project_data/val'\n",
    "num_train_sequences = len(train_doc)\n",
    "print('# training sequences =', num_train_sequences)\n",
    "num_val_sequences = len(val_doc)\n",
    "print('# validation sequences =', num_val_sequences)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model\n",
    "Now we will start making different models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Simple CNN3D model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source path =  /mnt/disks/user/project/PROJECT/Project_data/train ; batch size = 16\n",
      "Input Sample shape : (16, 30, 120, 120, 3)\n"
     ]
    }
   ],
   "source": [
    "train_data = train_doc\n",
    "val_data = val_doc\n",
    "transform_size = (120,120)\n",
    "batch_size = 16\n",
    "num_epochs = 5\n",
    "frame_selection_list = getframeselectionlist(choiceoflist=0)\n",
    "model_name = \"simpleCNN3D\"\n",
    "input_sample = next(generator(train_path,\n",
    "                              train_data, \n",
    "                              batch_size=batch_size,\n",
    "                              transform_size=transform_size,\n",
    "                              frame_selection_list=frame_selection_list))\n",
    "print(\"Input Sample shape : %s\" %(str(input_sample[0].shape)))\n",
    "model = Sequential()\n",
    "model.add(Conv3D(16,(3,3,3),padding=\"same\",input_shape=(input_sample[0].shape[1],input_sample[0].shape[2],input_sample[0].shape[3],input_sample[0].shape[4])))\n",
    "model.add(Activation(\"relu\"))\n",
    "model.add(MaxPooling3D(pool_size=(2,2,2)))\n",
    "\n",
    "model.add(Conv3D(8,(3,3,3),padding=\"same\"))\n",
    "model.add(Activation(\"relu\"))\n",
    "model.add(MaxPooling3D(pool_size=(3,3,3)))\n",
    "\n",
    "model.add(Conv3D(8,(3,3,3),padding=\"same\"))\n",
    "model.add(Activation(\"relu\"))\n",
    "model.add(MaxPooling3D(pool_size=(2,2,2)))\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dense(128,activation='relu'))\n",
    "model.add(BatchNormalization())\n",
    "\n",
    "model.add(Dense(64,activation='relu'))\n",
    "model.add(BatchNormalization())\n",
    "\n",
    "model.add(Dense(input_sample[1].shape[1], activation='softmax'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that you have written the model, the next step is to `compile` the model. When you print the `summary` of the model, you'll see the total number of parameters you have to train."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv3d_10 (Conv3D)           (None, 30, 120, 120, 16)  1312      \n",
      "_________________________________________________________________\n",
      "activation_10 (Activation)   (None, 30, 120, 120, 16)  0         \n",
      "_________________________________________________________________\n",
      "max_pooling3d_10 (MaxPooling (None, 15, 60, 60, 16)    0         \n",
      "_________________________________________________________________\n",
      "conv3d_11 (Conv3D)           (None, 15, 60, 60, 8)     3464      \n",
      "_________________________________________________________________\n",
      "activation_11 (Activation)   (None, 15, 60, 60, 8)     0         \n",
      "_________________________________________________________________\n",
      "max_pooling3d_11 (MaxPooling (None, 5, 20, 20, 8)      0         \n",
      "_________________________________________________________________\n",
      "conv3d_12 (Conv3D)           (None, 5, 20, 20, 8)      1736      \n",
      "_________________________________________________________________\n",
      "activation_12 (Activation)   (None, 5, 20, 20, 8)      0         \n",
      "_________________________________________________________________\n",
      "max_pooling3d_12 (MaxPooling (None, 2, 10, 10, 8)      0         \n",
      "_________________________________________________________________\n",
      "flatten_5 (Flatten)          (None, 1600)              0         \n",
      "_________________________________________________________________\n",
      "dense_13 (Dense)             (None, 128)               204928    \n",
      "_________________________________________________________________\n",
      "batch_normalization_9 (Batch (None, 128)               512       \n",
      "_________________________________________________________________\n",
      "dense_14 (Dense)             (None, 64)                8256      \n",
      "_________________________________________________________________\n",
      "batch_normalization_10 (Batc (None, 64)                256       \n",
      "_________________________________________________________________\n",
      "dense_15 (Dense)             (None, 5)                 325       \n",
      "=================================================================\n",
      "Total params: 220,789\n",
      "Trainable params: 220,405\n",
      "Non-trainable params: 384\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "optimiser = optimizers.Adam(lr=0.0002)#write your optimizer\n",
    "model.compile(optimizer=optimiser, loss='categorical_crossentropy', metrics=['categorical_accuracy'])\n",
    "print (model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us create the `train_generator` and the `val_generator` which will be used in `.fit_generator`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_generator = generator(train_path,\n",
    "                            train_data, \n",
    "                            batch_size=batch_size,\n",
    "                            transform_size=transform_size,\n",
    "                            frame_selection_list=frame_selection_list)\n",
    "val_generator = generator(val_path,\n",
    "                          val_data, \n",
    "                          batch_size=batch_size,\n",
    "                          transform_size=transform_size,\n",
    "                          frame_selection_list=frame_selection_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### Setting up extra parameter for Neural Network\n",
    "##### ModelCheckPoint\n",
    "We build modelcheckpoint to save models after every epoch, so that we can refer to models to best model later on for final testing.\n",
    "\n",
    "##### ReduceLROnPlateau\n",
    "If the val_loss value stops improving after patience number of epochs, we reduce the learning rate so as to take smaller steps down the gradient route for raeaching global minimum and avoid getting stuch at local minimum.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'model_init' + '_' + str(curr_dt_time).replace(' ','').replace(':','_') + '/'\n",
    "    \n",
    "if not os.path.exists(model_name):\n",
    "    os.mkdir(model_name)\n",
    "        \n",
    "filepath = model_name + 'model-{epoch:05d}-{loss:.5f}-{categorical_accuracy:.5f}-{val_loss:.5f}-{val_categorical_accuracy:.5f}.h5'\n",
    "\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='val_loss', verbose=1, save_best_only=False, save_weights_only=False, mode='auto', period=1)\n",
    "\n",
    "LR = ReduceLROnPlateau(monitor='val_loss', factor=0.2,\n",
    "                       patience=5, min_lr=0.001,verbose=1)\n",
    "callbacks_list = [checkpoint, LR]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `steps_per_epoch` and `validation_steps` are used by `fit_generator` to decide the number of next() calls it need to make."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "if (num_train_sequences%batch_size) == 0:\n",
    "    steps_per_epoch = int(num_train_sequences/batch_size)\n",
    "else:\n",
    "    steps_per_epoch = (num_train_sequences//batch_size) + 1\n",
    "\n",
    "if (num_val_sequences%batch_size) == 0:\n",
    "    validation_steps = int(num_val_sequences/batch_size)\n",
    "else:\n",
    "    validation_steps = (num_val_sequences//batch_size) + 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us now fit the model. This will start training the model and with the help of the checkpoints, you'll be able to save the model at the end of each epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source path =  Source path =  /mnt/disks/user/project/PROJECT/Project_data/trainEpoch 1/5\n",
      " ; batch size = 16\n",
      "/mnt/disks/user/project/PROJECT/Project_data/val ; batch size = 16\n",
      "42/42 [==============================] - 182s 4s/step - loss: 1.5451 - categorical_accuracy: 0.3722 - val_loss: 3.2571 - val_categorical_accuracy: 0.2700\n",
      "\n",
      "Epoch 00001: saving model to model_init_2021-02-0801_33_00.731094/model-00001-1.55079-0.36953-3.25710-0.27000.h5\n",
      "Epoch 2/5\n",
      "42/42 [==============================] - 109s 3s/step - loss: 0.9757 - categorical_accuracy: 0.6159 - val_loss: 2.3337 - val_categorical_accuracy: 0.3900\n",
      "\n",
      "Epoch 00002: saving model to model_init_2021-02-0801_33_00.731094/model-00002-0.96918-0.61840-2.33371-0.39000.h5\n",
      "Epoch 3/5\n",
      "42/42 [==============================] - 81s 2s/step - loss: 0.7070 - categorical_accuracy: 0.7339 - val_loss: 2.5649 - val_categorical_accuracy: 0.4300\n",
      "\n",
      "Epoch 00003: saving model to model_init_2021-02-0801_33_00.731094/model-00003-0.70557-0.73605-2.56494-0.43000.h5\n",
      "Epoch 4/5\n",
      "42/42 [==============================] - 78s 2s/step - loss: 0.5263 - categorical_accuracy: 0.8147 - val_loss: 1.7890 - val_categorical_accuracy: 0.4500\n",
      "\n",
      "Epoch 00004: saving model to model_init_2021-02-0801_33_00.731094/model-00004-0.51588-0.81599-1.78904-0.45000.h5\n",
      "Epoch 5/5\n",
      "42/42 [==============================] - 81s 2s/step - loss: 0.4558 - categorical_accuracy: 0.8418 - val_loss: 1.1702 - val_categorical_accuracy: 0.5400\n",
      "\n",
      "Epoch 00005: saving model to model_init_2021-02-0801_33_00.731094/model-00005-0.45644-0.84163-1.17024-0.54000.h5\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fd98c210e48>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit_generator(train_generator, steps_per_epoch=steps_per_epoch, epochs=num_epochs, verbose=1, \n",
    "                    callbacks=callbacks_list, validation_data=val_generator, \n",
    "                    validation_steps=validation_steps, class_weight=None, workers=1, initial_epoch=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not a very good accuracy, This model was used to check everything is wroking fine.\n",
    "#### Now repeating some of the above cells for different models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. CNN3D model\n",
    "**Increasing the complexity of the CNN3D model, adding more layers of CNN **<br>\n",
    "**Increasing the batch size to decrease the number of steps**<br>\n",
    "**Reducing the Transform Size**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source path =  /mnt/disks/user/project/PROJECT/Project_data/train ; batch size = 32\n",
      "Input Sample shape : (32, 30, 100, 100, 3)\n"
     ]
    }
   ],
   "source": [
    "train_data = train_doc\n",
    "val_data = val_doc\n",
    "transform_size = (100,100)\n",
    "batch_size = 32\n",
    "num_epochs = 8\n",
    "frame_selection_list = getframeselectionlist(choiceoflist=0)\n",
    "model_name = \"CNN3D_v2_complex\"\n",
    "input_sample = next(generator(train_path,\n",
    "                              train_data, \n",
    "                              batch_size=batch_size,\n",
    "                              transform_size=transform_size,\n",
    "                              frame_selection_list=frame_selection_list))\n",
    "print(\"Input Sample shape : %s\" %(str(input_sample[0].shape)))\n",
    "model = Sequential()\n",
    "model.add(Conv3D(16,(3,3,3),padding=\"same\",input_shape=(input_sample[0].shape[1],input_sample[0].shape[2],input_sample[0].shape[3],input_sample[0].shape[4])))\n",
    "model.add(Activation(\"relu\"))\n",
    "model.add(MaxPooling3D(pool_size=(2,2,2)))\n",
    "\n",
    "model.add(Conv3D(32,(3,3,3),padding=\"same\"))\n",
    "model.add(Activation(\"relu\"))\n",
    "model.add(MaxPooling3D(pool_size=(2,2,2)))\n",
    "\n",
    "model.add(Conv3D(64,(3,3,3),padding=\"same\"))\n",
    "model.add(Activation(\"relu\"))\n",
    "model.add(MaxPooling3D(pool_size=(2,2,2)))\n",
    "\n",
    "model.add(Conv3D(32,(3,3,3),padding=\"same\"))\n",
    "model.add(Activation(\"relu\"))\n",
    "model.add(MaxPooling3D(pool_size=(2,2,2)))\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dense(128,activation='relu'))\n",
    "model.add(BatchNormalization())\n",
    "\n",
    "model.add(Dense(64,activation='relu'))\n",
    "model.add(BatchNormalization())\n",
    "\n",
    "model.add(Dense(input_sample[1].shape[1], activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv3d_13 (Conv3D)           (None, 30, 100, 100, 16)  1312      \n",
      "_________________________________________________________________\n",
      "activation_13 (Activation)   (None, 30, 100, 100, 16)  0         \n",
      "_________________________________________________________________\n",
      "max_pooling3d_13 (MaxPooling (None, 15, 50, 50, 16)    0         \n",
      "_________________________________________________________________\n",
      "conv3d_14 (Conv3D)           (None, 15, 50, 50, 32)    13856     \n",
      "_________________________________________________________________\n",
      "activation_14 (Activation)   (None, 15, 50, 50, 32)    0         \n",
      "_________________________________________________________________\n",
      "max_pooling3d_14 (MaxPooling (None, 7, 25, 25, 32)     0         \n",
      "_________________________________________________________________\n",
      "conv3d_15 (Conv3D)           (None, 7, 25, 25, 64)     55360     \n",
      "_________________________________________________________________\n",
      "activation_15 (Activation)   (None, 7, 25, 25, 64)     0         \n",
      "_________________________________________________________________\n",
      "max_pooling3d_15 (MaxPooling (None, 3, 12, 12, 64)     0         \n",
      "_________________________________________________________________\n",
      "conv3d_16 (Conv3D)           (None, 3, 12, 12, 32)     55328     \n",
      "_________________________________________________________________\n",
      "activation_16 (Activation)   (None, 3, 12, 12, 32)     0         \n",
      "_________________________________________________________________\n",
      "max_pooling3d_16 (MaxPooling (None, 1, 6, 6, 32)       0         \n",
      "_________________________________________________________________\n",
      "flatten_6 (Flatten)          (None, 1152)              0         \n",
      "_________________________________________________________________\n",
      "dense_16 (Dense)             (None, 128)               147584    \n",
      "_________________________________________________________________\n",
      "batch_normalization_11 (Batc (None, 128)               512       \n",
      "_________________________________________________________________\n",
      "dense_17 (Dense)             (None, 64)                8256      \n",
      "_________________________________________________________________\n",
      "batch_normalization_12 (Batc (None, 64)                256       \n",
      "_________________________________________________________________\n",
      "dense_18 (Dense)             (None, 5)                 325       \n",
      "=================================================================\n",
      "Total params: 282,789\n",
      "Trainable params: 282,405\n",
      "Non-trainable params: 384\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "optimiser = optimizers.Adam(lr=0.0002)#write your optimizer\n",
    "model.compile(optimizer=optimiser, loss='categorical_crossentropy', metrics=['categorical_accuracy'])\n",
    "print (model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us create the `train_generator` and the `val_generator` which will be used in `.fit_generator`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_generator = generator(train_path,\n",
    "                            train_data, \n",
    "                            batch_size=batch_size,\n",
    "                            transform_size=transform_size,\n",
    "                            frame_selection_list=frame_selection_list)\n",
    "val_generator = generator(val_path,\n",
    "                          val_data, \n",
    "                          batch_size=batch_size,\n",
    "                          transform_size=transform_size,\n",
    "                          frame_selection_list=frame_selection_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### Setting up extra parameter for Neural Network\n",
    "##### ModelCheckPoint\n",
    "We build modelcheckpoint to save models after every epoch, so that we can refer to models to best model later on for final testing.\n",
    "\n",
    "##### ReduceLROnPlateau\n",
    "If the val_loss value stops improving after patience number of epochs, we reduce the learning rate so as to take smaller steps down the gradient route for raeaching global minimum and avoid getting stuch at local minimum.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'model_init' + '_' + str(curr_dt_time).replace(' ','').replace(':','_') + '/'\n",
    "    \n",
    "if not os.path.exists(model_name):\n",
    "    os.mkdir(model_name)\n",
    "        \n",
    "filepath = model_name + 'model-{epoch:05d}-{loss:.5f}-{categorical_accuracy:.5f}-{val_loss:.5f}-{val_categorical_accuracy:.5f}.h5'\n",
    "\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='val_loss', verbose=1, save_best_only=False, save_weights_only=False, mode='auto', period=1)\n",
    "\n",
    "LR = ReduceLROnPlateau(monitor='val_loss', factor=0.2,\n",
    "                       patience=5, min_lr=0.001,verbose=1)\n",
    "callbacks_list = [checkpoint, LR]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `steps_per_epoch` and `validation_steps` are used by `fit_generator` to decide the number of next() calls it need to make."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "if (num_train_sequences%batch_size) == 0:\n",
    "    steps_per_epoch = int(num_train_sequences/batch_size)\n",
    "else:\n",
    "    steps_per_epoch = (num_train_sequences//batch_size) + 1\n",
    "\n",
    "if (num_val_sequences%batch_size) == 0:\n",
    "    validation_steps = int(num_val_sequences/batch_size)\n",
    "else:\n",
    "    validation_steps = (num_val_sequences//batch_size) + 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us now fit the model. This will start training the model and with the help of the checkpoints, you'll be able to save the model at the end of each epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source path =  /mnt/disks/user/project/PROJECT/Project_data/val ; batch size = 32\n",
      "Source path =  /mnt/disks/user/project/PROJECT/Project_data/train ; batch size = 32\n",
      "Epoch 1/8\n",
      "21/21 [==============================] - 88s 4s/step - loss: 1.5348 - categorical_accuracy: 0.3732 - val_loss: 8.3137 - val_categorical_accuracy: 0.2300\n",
      "\n",
      "Epoch 00001: saving model to model_init_2021-02-0801_33_00.731094/model-00001-1.53243-0.37406-8.31374-0.23000.h5\n",
      "Epoch 2/8\n",
      "21/21 [==============================] - 72s 3s/step - loss: 1.1814 - categorical_accuracy: 0.5034 - val_loss: 3.0242 - val_categorical_accuracy: 0.2600\n",
      "\n",
      "Epoch 00002: saving model to model_init_2021-02-0801_33_00.731094/model-00002-1.17893-0.50377-3.02415-0.26000.h5\n",
      "Epoch 3/8\n",
      "21/21 [==============================] - 75s 4s/step - loss: 0.9948 - categorical_accuracy: 0.5943 - val_loss: 6.8883 - val_categorical_accuracy: 0.2000\n",
      "\n",
      "Epoch 00003: saving model to model_init_2021-02-0801_33_00.731094/model-00003-0.99473-0.59578-6.88825-0.20000.h5\n",
      "Epoch 4/8\n",
      "21/21 [==============================] - 74s 4s/step - loss: 0.8415 - categorical_accuracy: 0.6873 - val_loss: 2.2354 - val_categorical_accuracy: 0.2400\n",
      "\n",
      "Epoch 00004: saving model to model_init_2021-02-0801_33_00.731094/model-00004-0.84210-0.68778-2.23545-0.24000.h5\n",
      "Epoch 5/8\n",
      "21/21 [==============================] - 76s 4s/step - loss: 0.7191 - categorical_accuracy: 0.7446 - val_loss: 3.2524 - val_categorical_accuracy: 0.2800\n",
      "\n",
      "Epoch 00005: saving model to model_init_2021-02-0801_33_00.731094/model-00005-0.72024-0.74359-3.25241-0.28000.h5\n",
      "Epoch 6/8\n",
      "21/21 [==============================] - 76s 4s/step - loss: 0.5992 - categorical_accuracy: 0.7690 - val_loss: 3.2311 - val_categorical_accuracy: 0.2700\n",
      "\n",
      "Epoch 00006: saving model to model_init_2021-02-0801_33_00.731094/model-00006-0.59591-0.77225-3.23113-0.27000.h5\n",
      "Epoch 7/8\n",
      "21/21 [==============================] - 76s 4s/step - loss: 0.5773 - categorical_accuracy: 0.8094 - val_loss: 6.5470 - val_categorical_accuracy: 0.2100\n",
      "\n",
      "Epoch 00007: saving model to model_init_2021-02-0801_33_00.731094/model-00007-0.57441-0.81146-6.54698-0.21000.h5\n",
      "Epoch 8/8\n",
      "21/21 [==============================] - 76s 4s/step - loss: 0.4495 - categorical_accuracy: 0.8387 - val_loss: 5.8204 - val_categorical_accuracy: 0.2300\n",
      "\n",
      "Epoch 00008: saving model to model_init_2021-02-0801_33_00.731094/model-00008-0.44627-0.84163-5.82044-0.23000.h5\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fd9800409e8>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit_generator(train_generator, steps_per_epoch=steps_per_epoch, epochs=num_epochs, verbose=1, \n",
    "                    callbacks=callbacks_list, validation_data=val_generator, \n",
    "                    validation_steps=validation_steps, class_weight=None, workers=1, initial_epoch=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  the accuracy decrease drastically, what we can do is first take our learning high and bring validation close to it"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. CNN3D model\n",
    "**Extremely complex CNN3D model for overlearning of the model**<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source path =  /mnt/disks/user/project/PROJECT/Project_data/train ; batch size = 32\n",
      "Input Sample shape : (32, 30, 100, 100, 3)\n"
     ]
    }
   ],
   "source": [
    "train_data = train_doc\n",
    "val_data = val_doc\n",
    "transform_size = (100,100)\n",
    "batch_size = 32\n",
    "num_epochs = 8\n",
    "frame_selection_list = getframeselectionlist(choiceoflist=0)\n",
    "model_name = \"CNN3D_v2_complex\"\n",
    "input_sample = next(generator(train_path,\n",
    "                              train_data, \n",
    "                              batch_size=batch_size,\n",
    "                              transform_size=transform_size,\n",
    "                              frame_selection_list=frame_selection_list))\n",
    "print(\"Input Sample shape : %s\" %(str(input_sample[0].shape)))\n",
    "model = Sequential()\n",
    "model.add(Conv3D(16,(3,3,3),padding=\"same\",input_shape=(input_sample[0].shape[1],input_sample[0].shape[2],input_sample[0].shape[3],input_sample[0].shape[4])))\n",
    "model.add(Activation(\"relu\"))\n",
    "model.add(MaxPooling3D(pool_size=(2,2,2)))\n",
    "\n",
    "model.add(Conv3D(32,(3,3,3),padding=\"same\"))\n",
    "model.add(Activation(\"relu\"))\n",
    "model.add(MaxPooling3D(pool_size=(2,2,2)))\n",
    "\n",
    "model.add(Conv3D(64,(3,3,3),padding=\"same\"))\n",
    "model.add(Activation(\"relu\"))\n",
    "model.add(MaxPooling3D(pool_size=(2,2,2)))\n",
    "\n",
    "model.add(Conv3D(32,(3,3,3),padding=\"same\"))\n",
    "model.add(Activation(\"relu\"))\n",
    "\n",
    "model.add(Conv3D(32,(3,3,3),padding=\"same\"))\n",
    "model.add(Activation(\"relu\"))\n",
    "model.add(MaxPooling3D(pool_size=(2,2,2)))\n",
    "\n",
    "model.add(Flatten())\n",
    "\n",
    "model.add(Dense(512,activation='relu'))\n",
    "model.add(BatchNormalization())\n",
    "\n",
    "model.add(Dense(256,activation='relu'))\n",
    "model.add(BatchNormalization())\n",
    "\n",
    "model.add(Dense(128,activation='relu'))\n",
    "model.add(BatchNormalization())\n",
    "\n",
    "model.add(Dense(64,activation='relu'))\n",
    "model.add(BatchNormalization())\n",
    "\n",
    "model.add(Dense(input_sample[1].shape[1], activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv3d_22 (Conv3D)           (None, 30, 100, 100, 16)  1312      \n",
      "_________________________________________________________________\n",
      "activation_22 (Activation)   (None, 30, 100, 100, 16)  0         \n",
      "_________________________________________________________________\n",
      "max_pooling3d_21 (MaxPooling (None, 15, 50, 50, 16)    0         \n",
      "_________________________________________________________________\n",
      "conv3d_23 (Conv3D)           (None, 15, 50, 50, 32)    13856     \n",
      "_________________________________________________________________\n",
      "activation_23 (Activation)   (None, 15, 50, 50, 32)    0         \n",
      "_________________________________________________________________\n",
      "max_pooling3d_22 (MaxPooling (None, 7, 25, 25, 32)     0         \n",
      "_________________________________________________________________\n",
      "conv3d_24 (Conv3D)           (None, 7, 25, 25, 64)     55360     \n",
      "_________________________________________________________________\n",
      "activation_24 (Activation)   (None, 7, 25, 25, 64)     0         \n",
      "_________________________________________________________________\n",
      "max_pooling3d_23 (MaxPooling (None, 3, 12, 12, 64)     0         \n",
      "_________________________________________________________________\n",
      "conv3d_25 (Conv3D)           (None, 3, 12, 12, 32)     55328     \n",
      "_________________________________________________________________\n",
      "activation_25 (Activation)   (None, 3, 12, 12, 32)     0         \n",
      "_________________________________________________________________\n",
      "conv3d_26 (Conv3D)           (None, 3, 12, 12, 32)     27680     \n",
      "_________________________________________________________________\n",
      "activation_26 (Activation)   (None, 3, 12, 12, 32)     0         \n",
      "_________________________________________________________________\n",
      "max_pooling3d_24 (MaxPooling (None, 1, 6, 6, 32)       0         \n",
      "_________________________________________________________________\n",
      "flatten_8 (Flatten)          (None, 1152)              0         \n",
      "_________________________________________________________________\n",
      "dense_24 (Dense)             (None, 512)               590336    \n",
      "_________________________________________________________________\n",
      "batch_normalization_17 (Batc (None, 512)               2048      \n",
      "_________________________________________________________________\n",
      "dense_25 (Dense)             (None, 256)               131328    \n",
      "_________________________________________________________________\n",
      "batch_normalization_18 (Batc (None, 256)               1024      \n",
      "_________________________________________________________________\n",
      "dense_26 (Dense)             (None, 128)               32896     \n",
      "_________________________________________________________________\n",
      "batch_normalization_19 (Batc (None, 128)               512       \n",
      "_________________________________________________________________\n",
      "dense_27 (Dense)             (None, 64)                8256      \n",
      "_________________________________________________________________\n",
      "batch_normalization_20 (Batc (None, 64)                256       \n",
      "_________________________________________________________________\n",
      "dense_28 (Dense)             (None, 5)                 325       \n",
      "=================================================================\n",
      "Total params: 920,517\n",
      "Trainable params: 918,597\n",
      "Non-trainable params: 1,920\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "optimiser = optimizers.Adam(lr=0.0002)#write your optimizer\n",
    "model.compile(optimizer=optimiser, loss='categorical_crossentropy', metrics=['categorical_accuracy'])\n",
    "print (model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us create the `train_generator` and the `val_generator` which will be used in `.fit_generator`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_generator = generator(train_path,\n",
    "                            train_data, \n",
    "                            batch_size=batch_size,\n",
    "                            transform_size=transform_size,\n",
    "                            frame_selection_list=frame_selection_list)\n",
    "val_generator = generator(val_path,\n",
    "                          val_data, \n",
    "                          batch_size=batch_size,\n",
    "                          transform_size=transform_size,\n",
    "                          frame_selection_list=frame_selection_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### Setting up extra parameter for Neural Network\n",
    "##### ModelCheckPoint\n",
    "We build modelcheckpoint to save models after every epoch, so that we can refer to models to best model later on for final testing.\n",
    "\n",
    "##### ReduceLROnPlateau\n",
    "If the val_loss value stops improving after patience number of epochs, we reduce the learning rate so as to take smaller steps down the gradient route for raeaching global minimum and avoid getting stuch at local minimum.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'model_init' + '_' + str(curr_dt_time).replace(' ','').replace(':','_') + '/'\n",
    "    \n",
    "if not os.path.exists(model_name):\n",
    "    os.mkdir(model_name)\n",
    "        \n",
    "filepath = model_name + 'model-{epoch:05d}-{loss:.5f}-{categorical_accuracy:.5f}-{val_loss:.5f}-{val_categorical_accuracy:.5f}.h5'\n",
    "\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='val_loss', verbose=1, save_best_only=False, save_weights_only=False, mode='auto', period=1)\n",
    "\n",
    "LR = ReduceLROnPlateau(monitor='val_loss', factor=0.2,\n",
    "                       patience=5, min_lr=0.001,verbose=1)\n",
    "callbacks_list = [checkpoint, LR]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `steps_per_epoch` and `validation_steps` are used by `fit_generator` to decide the number of next() calls it need to make."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "if (num_train_sequences%batch_size) == 0:\n",
    "    steps_per_epoch = int(num_train_sequences/batch_size)\n",
    "else:\n",
    "    steps_per_epoch = (num_train_sequences//batch_size) + 1\n",
    "\n",
    "if (num_val_sequences%batch_size) == 0:\n",
    "    validation_steps = int(num_val_sequences/batch_size)\n",
    "else:\n",
    "    validation_steps = (num_val_sequences//batch_size) + 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us now fit the model. This will start training the model and with the help of the checkpoints, you'll be able to save the model at the end of each epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source path =  /mnt/disks/user/project/PROJECT/Project_data/val ; batch size = 32\n",
      "Source path =  /mnt/disks/user/project/PROJECT/Project_data/train ; batch size = 32\n",
      "Epoch 1/8\n",
      "21/21 [==============================] - 87s 4s/step - loss: 1.6972 - categorical_accuracy: 0.3542 - val_loss: 5.9841 - val_categorical_accuracy: 0.2400\n",
      "\n",
      "Epoch 00001: saving model to model_init_2021-02-0801_33_00.731094/model-00001-1.70235-0.35143-5.98409-0.24000.h5\n",
      "Epoch 2/8\n",
      "21/21 [==============================] - 73s 3s/step - loss: 1.1552 - categorical_accuracy: 0.5343 - val_loss: 4.1461 - val_categorical_accuracy: 0.2500\n",
      "\n",
      "Epoch 00002: saving model to model_init_2021-02-0801_33_00.731094/model-00002-1.15724-0.53394-4.14609-0.25000.h5\n",
      "Epoch 3/8\n",
      "21/21 [==============================] - 76s 4s/step - loss: 0.9216 - categorical_accuracy: 0.6313 - val_loss: 2.1045 - val_categorical_accuracy: 0.3500\n",
      "\n",
      "Epoch 00003: saving model to model_init_2021-02-0801_33_00.731094/model-00003-0.92078-0.63047-2.10454-0.35000.h5\n",
      "Epoch 4/8\n",
      "21/21 [==============================] - 75s 4s/step - loss: 0.7356 - categorical_accuracy: 0.7259 - val_loss: 1.4634 - val_categorical_accuracy: 0.5100\n",
      "\n",
      "Epoch 00004: saving model to model_init_2021-02-0801_33_00.731094/model-00004-0.73193-0.72851-1.46336-0.51000.h5\n",
      "Epoch 5/8\n",
      "21/21 [==============================] - 75s 4s/step - loss: 0.5570 - categorical_accuracy: 0.8165 - val_loss: 1.1254 - val_categorical_accuracy: 0.5800\n",
      "\n",
      "Epoch 00005: saving model to model_init_2021-02-0801_33_00.731094/model-00005-0.55276-0.81750-1.12538-0.58000.h5\n",
      "Epoch 6/8\n",
      "21/21 [==============================] - 74s 4s/step - loss: 0.4759 - categorical_accuracy: 0.8407 - val_loss: 2.9817 - val_categorical_accuracy: 0.3100\n",
      "\n",
      "Epoch 00006: saving model to model_init_2021-02-0801_33_00.731094/model-00006-0.47216-0.84314-2.98167-0.31000.h5\n",
      "Epoch 7/8\n",
      "21/21 [==============================] - 75s 4s/step - loss: 0.4300 - categorical_accuracy: 0.8681 - val_loss: 1.5752 - val_categorical_accuracy: 0.4100\n",
      "\n",
      "Epoch 00007: saving model to model_init_2021-02-0801_33_00.731094/model-00007-0.42538-0.87029-1.57520-0.41000.h5\n",
      "Epoch 8/8\n",
      "21/21 [==============================] - 75s 4s/step - loss: 0.3379 - categorical_accuracy: 0.9207 - val_loss: 1.4080 - val_categorical_accuracy: 0.4500\n",
      "\n",
      "Epoch 00008: saving model to model_init_2021-02-0801_33_00.731094/model-00008-0.33612-0.92308-1.40803-0.45000.h5\n"
     ]
    }
   ],
   "source": [
    "model = model.fit_generator(train_generator, steps_per_epoch=steps_per_epoch, epochs=num_epochs, verbose=1, \n",
    "                    callbacks=callbacks_list, validation_data=val_generator, \n",
    "                    validation_steps=validation_steps, class_weight=None, workers=1, initial_epoch=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "### Accuracy graph\n",
    "plt.plot(model.history[\"categorical_accuracy\"])\n",
    "plt.plot(model.history[\"val_categorical_accuracy\"])\n",
    "plt.xlabel(\"EPOCHS\")\n",
    "plt.ylabel(\"Percentage\")\n",
    "plt.ylim(0.0,1.0)\n",
    "plt.legend([\"Train\",\"Test\"],loc=\"lower right\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAEKCAYAAAD9xUlFAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4yLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvOIA7rQAAIABJREFUeJzt3Xl4lOXV+PHvyb4QCCQkAUII+xLAsAUUFwQE3G0VRdDWFbTaam1/re3vba32fVt7tb+31dq6g7tgRTS4gYLiDmFfEvYtYQtJCEsgZLt/f9yTGEJ2ZuaZmZzPdeWa7ZnnOcExZ+7t3GKMQSmllAIIcjoApZRSvkOTglJKqRqaFJRSStXQpKCUUqqGJgWllFI1NCkopZSq4bGkICIRIrJCRNaJyCYRebSeY8JFZJ6IbBeR5SKS6ql4lFJKNc2TLYXTwHhjzHlAOjBFRMbUOeZO4Igxpg/wd+AvHoxHKaVUEzyWFIx1wvUw1PVTd6XctcDLrvtvAxNERDwVk1JKqcaFePLkIhIMrAL6AP8yxiyvc0g3IBfAGFMhIkeBOKCgznlmAjMBoqOjRwwYMMCTYbcNJYfhaB4kDIKQcKejUUp52KpVqwqMMZ2bOs6jScEYUwmki0gssEBEBhtjNtY6pL5WwVl1N4wxzwHPAYwcOdKsXLnSI/G2KYU74J/D4fL7YfQsp6NRgaCqCv6VAWnXwfj/cjoaVYeI7GnOcV6ZfWSMKQY+B6bUeSkP6A4gIiFAB6DIGzG1eXG9Ia4PbF3kdCQqUOQuh8JtsPxZOH2i6eOVT/Lk7KPOrhYCIhIJTAQ21zksE/ix6/4NwFKjFfq8p+9k2P0VlJU4HYkKBDmZgMDpY7B+rtPRqFbyZEuhC/CZiKwHsoBPjDHvi8hjInKN65gXgTgR2Q48BDzswXhUXf0mQ+Vp2LnM6UiUvzMGchZC30nQ5TxY8bx9Tvkdj40pGGPWA8Pqef73te6XAlM9FYNqQsr5EBYDWz+GAVc4HY3yZ/vXwNFcGPcwSBC8ey/sWga9xrn9UuXl5eTl5VFaWur2cweCiIgIkpOTCQ0NbdX7PTrQrHxcSBj0vhS2fWK/1elsYNVaOZkgwdD/CgiNgsX/Bcuf80hSyMvLIyYmhtTUVHQG+5mMMRQWFpKXl0fPnj1bdQ4tc9HW9ZsMx/fDwQ1OR6L8lTGQnQk9L4KoThAaASNugy0fwpHdbr9caWkpcXFxmhDqISLExcWdUytKk0Jb1+cye7tNZyGpVsrPgaIdMPCa758beaftRsp6wSOX1ITQsHP9t9Gk0NbFJELX4bB1sdORKH9VPetowFXfP9ehGwy8Cla/CmUnHQtNtZwmBWW7kPKyoKSg6WOVqis7E1LG2C8YtY2+B0qLYcNbzsTlIYWFhaSnp5Oenk5SUhLdunWreVxWVtasc9x+++1s2bLFw5G2jiYFZacRYmD7p05HovxN4Q7I33Rm11G1lPMhcYgdcA6g6alxcXGsXbuWtWvXcs899/Dzn/+85nFYWBhgB3yrqqoaPMecOXPo37+/t0JuEU0KCrqkQ3SCrm5WLZeTaW8HXn32ayIweqZNGru/8m5cDti+fTuDBw/mnnvuYfjw4Rw4cICZM2cycuRI0tLSeOyxx2qOvfDCC1m7di0VFRXExsby8MMPc95553H++eeTn5/v4G+hU1IVQFAQ9JtkFx9VVkCwfixUM2VnQtdhENu9/teHTIVPfg8rnrWzk9zs0YWbyN5/zK3nHNS1PY9cndaq92ZnZzNnzhyeeeYZAB5//HE6depERUUFl156KTfccAODBg064z1Hjx7lkksu4fHHH+ehhx5i9uzZPPywc+t4taWgrL6TofSorV+jVHMU58L+1fV3HVULjYThP4LNH9jjA1zv3r0ZNWpUzeM333yT4cOHM3z4cHJycsjOzj7rPZGRkVx++eUAjBgxgt27d3sr3HrpV0Jl9b4UgkLt6ubUsU5Ho/xBzkJ721hSABh1F3zzT1j5Ikz8g1tDaO03ek+Jjo6uub9t2zaeeOIJVqxYQWxsLLfccku96weqxyEAgoODqaio8EqsDdGWgrLCY6DHBbBNp6aqZspZaPfjiO/T+HGxKXal86qXofyUd2LzAceOHSMmJob27dtz4MABFi3yjzE7TQrqe/0mw+HNcKRZZddVW3b8EOz9tulWQrXRs+BUEWx427Nx+ZDhw4czaNAgBg8ezN13383Ysf7RAhd/q1Stm+x4UPXGO1f8DTLudjoa5cuyXoQPHoJ7v4HEZnThGANPXwBBwTDry3Oqs5WTk8PAgQNb/f62oL5/IxFZZYwZ2dR7taWgvhfXGzr1tuMKSjUmZ6H9rCQMavpYsEkg425bY2vvd56NTZ0TTQrqTP0mw64vdeMd1bCTRbD7S7s2oSXf+IfeBBEd7PRU5bM0Kagz9Z1kN97Z9YXTkShfteUjqKqAQc0cT6gWFg3DbrVrG47t90xs6pxpUlBn6jEWwtrp6mbVsJxM6NDdFlJsqVF3gamClbPdH5dyC00K6kw1G+8sDqh6NcpNTh+HHUtb3nVUrVNP6DcFVs6Bct05zRdpUlBn6zsZju2DQxudjkT5mq2LoLKs/lpHzTV6JpwsgE0L3BeXchtNCupsfSfZW+1CUnXlZNriid1Ht/4cvS6F+P52wNkPW6PuKJ0NMHv2bA4ePFjz2FfKaWtSUGeLSbSVU3V1s6qt7KTdz3vgVXa9QWtVT0/dvwby/G/NUXNKZzdH3aTgK+W0NSmo+vWbYjfeOVnkdCTKV+xYCuUnm7+KuTHn3Qzh7QNueurLL79MRkYG6enp/OQnP6GqqoqKigpuvfVWhgwZwuDBg3nyySeZN28ea9eu5aabbqppYTSnnPa2bdsYPXo0GRkZ/O53vyM2Ntbtv4MWxFP16zcJlj1uN94ZeqPT0ShfkJMJEbGQeuG5nyu8HaTPgKznYdJ/Q0xS687z0cN2QZw7JQ2Byx9v8ds2btzIggUL+OabbwgJCWHmzJnMnTuX3r17U1BQwIYNNs7i4mJiY2P55z//yVNPPUV6evpZ52qonPZPf/pTfvnLXzJ16lSeeuqpc/5V66MtBVW/LsNcG+/o6mYFVJTBlo9hwJUQHOqec2bcbdc7rJzjnvM57NNPPyUrK4uRI0eSnp7OsmXL2LFjB3369GHLli088MADLFq0iA4dOjR5robKaS9fvpzrr78egOnTp3vk99CWgqpfUBD0vQw2v68b7yi7mPH0Ufd0HVWL620nNayaAxf9wk6HbqlWfKP3FGMMd9xxB3/84x/Pem39+vV89NFHPPnkk8yfP5/nnnuu0XM5WU5bWwqqYX0n2Y138lY4HYlyWs57dlFjr3HuPW/GLDhxCLLfc+95HTBx4kTeeustCgoKADtLae/evRw+fBhjDFOnTuXRRx9l9erVAMTExHD8+PEWXSMjI4MFC+xU3rlz57r3F3DRpKAa1ns8BIXo1NS2rrLC7pzWbzKERrj33L3H28J6ATDgPGTIEB555BEmTpzI0KFDmTRpEocOHSI3N5eLL76Y9PR07r77bv70pz8BdgrqXXfd1aKprE8++SR/+ctfyMjIID8/v1ldUS3lsdLZItIdeAVIAqqA54wxT9Q5ZhzwHrDL9dQ7xpjHaISWzvayl6+GE4fhPq1s2Wbt+sJ+Dqa+DGnXuf/83z0DH/8a7l4K3UY0eXhbLp1dUlJCVFQUIsJrr73GggULmD9//lnH+Wrp7ArgF8aYgcAY4D4Rqa/O7pfGmHTXT6MJQTmg72Q4nAPFe52ORDklZyGERNgxJk9In267ppY33s+uICsri2HDhjF06FCef/55/vrXv7r9Gh5LCsaYA8aY1a77x4EcoJunrqc8pN9ke6tdSG1TVZVNCn0m2iqnnhDR3iaGTe/YVqlq0Lhx41i7di3r169n2bJl9OrVy+3X8MqYgoikAsOA5fW8fL6IrBORj0TEt3bhVhDXBzr10tXNbdW+lXD8gHtnHdUnY6atqbTqpWYd7m87RnrTuf7beDwpiEg7YD7woDHmWJ2XVwM9jDHnAf8E3m3gHDNFZKWIrDx8WL9JeJWI7ULa9YUtc6Daluz3ICj0+xajp8T3tYPOK1+EyvJGD42IiKCwsFATQz2MMRQWFhIR0foJAR6dfC4iodiE8Lox5p26r9dOEsaYD0Xk3yISb4wpqHPcc8BzYAeaPRmzqke/SbD8aZsY+k9xOhrlLcbYrqNe4yDS/eUUzpIxC968ya6cHnx9g4clJyeTl5eHfkGsX0REBMnJya1+v8eSgogI8CKQY4z53waOSQIOGWOMiGRgWy6FnopJtVKPsRAaDdsWaVJoSw6uh+I9dmGZN/S9DDqm2gHnRpJCaGgoPXv29E5MbZAnWwpjgVuBDSKy1vXcb4EUAGPMM8ANwL0iUgGcAqYZbRP6npBwu/HOVtfGO63ZXEX5n+xMkCBb2sIbgoLt2MKi38KBddDlPO9cV53BY0nBGPMV0OhfD2PMU4Bnqjop9+o32Za8yM+GRJ0P0CbkLLStxOh4710zfQYs/W/bWrjuX967rqqhK5pV8+jGO23L4S1QsMXzs47qioyF86bBhv9AifYkO0GTgmqemCTbnNek0DZkZ9rbgVd5/9oZM6HyNKx+yfvXVpoUVAv0nWyL4+nGO4Ev5z1IzoD2Xb1/7YSB0PNiyJpt6y4pr9KkoJqv3xQwVbB9idORKE8q2mU3rhnk5a6j2kbfA8fyYMsHzsXQRmlSUM3XdRhEd7ZTU1Xgyllobwde7VwM/aZAbIrWQ3KAJgXVfEFB0Ocyu0WnNusDV04mJA21awacEhQMo+6CPV/BwY3OxdEGaVJQLdNvEpw6AnlZTkeiPOHYfvvf1smuo2rDboWQSFihrQVv0qSgWqZ64x3tQgpMOe/b24HXOhsHQFQnGDoV1r+lkxu8SJOCapmIDpByvl3drAJPTibE94fO/ZyOxMqYBRWnYM2rTkfSZmhSUC3XbzLkb4LiXKcjUe5UUgB7vvaNrqNqSYOhx4WQ9QJUVTodTZugSUG1XF9XGWXtQgosmz+wU469vYq5KaNn2p3/tn7sdCRtgiYF1XLxfe3MFO1CCiw5mRDbA5KGOB3JmfpfCe2TYfmzTkfSJmhSUC1Xe+Od8lNOR6Pc4VQx7Fxmu458rQpucAiMugN2LYP8HKejCXiaFFTr9JtsBwB3fel0JModtn4MVeW+MeuoPsNvg+BwnZ7qBZoUVOukXvj9xjvK/+UshJiu0G2E05HULzoOhkyFdXNtq0Z5jCYF1Toh4Xabxq2L7MY7yn+dPmFXqQ+8yq5a91WjZ0L5SVj7utORBDQf/gQon9dvEhzN1X5ef7f9E6go9b1ZR3V1OQ+6j7FdSDo91WM0KajWq954R7uQ/FvOQoiKhx4XOB1J00bPhCO7YdsnTkcSsDQpqNZr39UWTtOpqf6rvNR2AQ640hah83UDr4GYLrBCp6d6iiYFdW76TYbc5Vqbxl/t/AzKTvh+11G14FAYeSfsWAoF25yOJiBpUlDnpu9kMJX2f1Llf7IzIbyD3enMX4y4DYLDdHqqh2hSUOem23CIitO9m/1RZTls+RD6Xw4hYU5H03ztOkPaD2HtG1B6zOloAo4mBXVugoLtgPP2T+wfGeU/dn8JpcW+VQCvuUbPtN1ea99wOpKAo0lBnbu0H9qNdxY+AFVVTkejmis70y5A7D3e6UhartsISB7lmp6qnzl30qSgzl2/STDuN3ZR0aLf6mI2f1BVCZvfh76XQWik09G0TsYsKNqh41lupklBucclv4YxP4HlT8OyvzgdjWpK7nIoOeyfXUfVBl0L7RJ1eqqbaVJQ7iECk/4H0mfA53+G7552OiLVmOxMW2CuegGiPwoJgxG3w7bFULjD6WgChiYF5T5BQXD1kzDwavj4YVijNWp8kjF2FXPv8RAe43Q052bk7XbP8BXPOx1JwPBYUhCR7iLymYjkiMgmEXmgnmNERJ4Uke0isl5EhnsqHuUlwSFw/YvQ61LIvN9+I1W+Zf9qOJbn311H1WKSIO0Hdjzr9AmnowkInmwpVAC/MMYMBMYA94nIoDrHXA70df3MBLTPIRCEhMO016HbSJh/J+z4zOmIVG3Zmfbbdb8pTkfiHhmz4PQxWPem05EEBI8lBWPMAWPMatf940AO0K3OYdcCrxjrOyBWRLp4KiblRWHRMOMtiO8Hc2dA7gqnI1Lg6jrKhNSLIKqT09G4R/JI6DrMdiHpzLdz5pUxBRFJBYYBy+u81A3IrfU4j7MTByIyU0RWisjKw4cPeypM5W6RHeGWdyAmEV6/AQ5udDoidWgTFO0MjK6jaiK2tVCwBXZ+7nQ0fs/jSUFE2gHzgQeNMXXXpNe3GexZqd4Y85wxZqQxZmTnzp09EabylJhE+NF7dpHUqz/QWSJOy1kICAy4yulI3GvwD235b62HdM48mhREJBSbEF43xrxTzyF5QPdaj5OB/Z6MSTkgNgV+9K4tnPfKdXB0n9MRtV05mXbfhHYJTkfiXiHhtlDelo+gaJfT0fg1T84+EuBFIMcY878NHJYJ/Mg1C2kMcNQYc8BTMSkHde5vu5JKi+HV66CkwOmI2p6C7ZCfbacMB6JRd4IEQdYLTkfi1zzZUhgL3AqMF5G1rp8rROQeEbnHdcyHwE5gO/A88BMPxqOc1jUdbp4LxXvhteu1wqW35bxnbwM1KbTvasdK1rwKZSVOR+O3Qjx1YmPMV9Q/ZlD7GAPc56kYlA9KHQs3vgpzb4Y3p8Et8/239o6/yVloC8l1SHY6Es/JmAWbFsD6t+zCNtViuqJZeV+/SfDD52DPN/DWj7XktjcU74X9awK3lVAtZQwkDbEDzjo9tVU0KShnDL4ervo7bFsEC2bZqp3Kc3IW2lt/2Xaztaqnp+Zn2/0iVItpUlDOGXk7THwUNs6HD36h3+w8KWchJA6GuN5OR+J5Q26AyE6wXKuntoYmBeWsCx+ECx+CVXPg0z84HU1gOn4I9n4X+K2EaqGRMOLHdqvR4r1OR+N3NCko5034PYy8E77+B3zZ0Oxl1WqbFwIm8McTaht5p73NetHZOPyQJgXlPBG44m8wZCoseVT/R3a37EyI6wMJA52OxHtiu8OAK2H1y1B+yulo/IomBeUbgoLguqdt5c4PfgEb3nY6osBwsgh2f2W7jqTRGeKBJ2OW3Tt8w3+cjsSvaFJQviM4FKa+BD3G2hlJWxc5HZH/2/KhLS8SSAXwmiv1QkhIg+U6PbUlNCko3xIaCTe/aeeav/Uj+y1XtV52JnRIgS7pTkfifSIweiYc2gB7v3U6Gr+hSUH5noj2MGM+xPaAN6bBvtVOR+SfSo/Bzs/sAHNb6zqqNuRGiIjV6aktoElB+aboOFtZNaqjrZOUv9npiPzPtsVQWdY2u46qhUXB8FvtOg2tztssmhSU72rfFW591441vPoDOLLH6Yj8S/Z70C4RkjOcjsRZo+4CUwUrdVZbc2hSUL4trjfcugDKT8Ir18Lxg05H5B/KTsL2T+1mOkFt/H/zjqnQ/wpY9RKUlzodjc9r458W5RcS02DG23AiH179oZ1mqRq3Y4lNpG2566i20TPhZCFsqm+vL1Vbm0kKpysqmbtiL0anpvmn7qPg5jegcBu8cSOcPuF0RL4tO9Pukd3jQqcj8Q09L4HOA+yAs/4NaFSbSQrvrtnHw+9s4Nfz11NRWeV0OKo1eo2DG2bb2Uhzp0PFaacj8k0Vp2Hrx9D/Sgj22JYp/kUEMu6GA2shL8vpaHxam0kKN47szs8m9OWtlXnc89oqTpVpqWa/NPBquPZfsGsZvH0HVFY4HZHv2bkMTh/TrqO6hk6D8A6w/BmnI/FpbSYpiAgPXdaPP16bxpLN+dz64nKOntTNXfxS+s0w5S+w+X3I/ClUacvvDDmZEN7etqzU98LbwbBb7KysY7oVfEPaTFKoduv5qfxr+nDW5x3lxme/5eBRnY3gl8bcA+N+C+vegEW/0X7iapUVsPkD6DcZQsKdjsb3ZNxlN3RaNcfpSHxWm0sKAFcM6cJLt49iX/Eprn/6G7bn66ClX7rkVzDmPtsd8PnjTkfjG/Z8DaeK2laZ7Jbo1Av6ToKVc6CizOlofFKLkoKIhIrIMBFJ8FRA3nJBn3jmzhzD6YpKpj7zDWtzi50OSbWUCEz+H0i/BZY9Dt/+2+mInJezEEIioc9EpyPxXaNnQkk+ZL/rdCQ+qdGkICLPiEia634HYB3wCrBGRG72QnweNbhbB96+5wJiIkKZ/vx3LNt62OmQVEuJwNVP2NLQi34Da15zOiLnVFXZpNB3IoRFOx2N7+o13u4vofWQ6tVUS+EiY8wm1/3bga3GmCHACOBXHo3MS1Ljo3n73vNJjYvmzpeyeHeN1kfxO8EhcP0L0Hu8HXjOfs/piJyRlwUnDsLAa52OxLcFBdm9FvathLxVTkfjc5pKCrU73S4D3gUwxgRUrYGEmAjmzhrDyNSOPDhvLS9+tcvpkFRLhYTDTa9B8ih4+07YvsTpiLwvJxOCQqHfJKcj8X3pN0NYDKzQ1kJdTSWFYhG5SkSGAWOBjwFEJASI9HRw3tQ+IpSXbs/g8sFJ/PH9bB7/aLOufvY3YdEwfR507g/zboG9y52OyHuMsUmh96UQ0cHpaHxfeAwMmwHr58HTY2Hx72Dn57ogkqaTwizgfmAO8GCtFsIE4ANPBuaEiNBgnpo+nBmjU3hm2Q5+9baufvY7kR1tAb2YJHhjKhzc4HRE3nFgHRTvtWMrqnkm/B4mPmo/M989bQsu/iUVXr/RjjcU7miTU53F374Njxw50qxcudKj1zDG8MSSbfzj021MGJDAU9OHExkW7NFrKjcr3guzp9j9BO5YZKutBrIlj8FX/4BfbrN7UaiWOX3C7vK3/VNbTLBop30+tgf0mWBnc/W82LYw/JSIrDLGjGzyuMaSgojcDXxujNkmIgLMBq4HdgM/NsasaeS9s4GrgHxjzOB6Xh8HvAdUd+C/Y4x5rKmAvZEUqr363R5+/95GRqR05IUfjyQ2Kswr11VucngrzJkCoVFwx8fQIdnpiDzDGHhqFLTvAj9e6HQ0gaFopx2X2rEUdn0BZScgKAS6j7ZJovcESBrqV2XJ3ZUUNgLDjDHlIjId+AUwCRgGPGKMuaiR914MnABeaSQp/NIYc1VTQdbmzaQA8OGGAzw4dy2p8VG8fEcGXToE1FBK4Nu/Fl6+2m42c8fHEB3vdETul78Z/j0arvibLfqm3KuiDHKX2xbE9iVwcL19PrqznfHWe4K9bdfZ2Tib0Nyk0FSaqzDGVBcIugr7B77QGPMp0OhEaGPMF4DfF76/YkgXXrpjFPuLS7nh6W919bO/6ZpuB5+P5sJrP4TSo05H5H45mYDoKmZPCQmDnhfBxD/APV/aLrofPAu9LrXdTQtmwt/6wLMXw6ePwu6vodJ/66o11VJYDVwJHAH2AOOr1y2ISI4xZmCjJxdJBd5vpKUwH8gD9mNbDZvqHuc6diYwEyAlJWXEnj3e35Zx476j3DYni8qqKmbfNophKR29HoM6B9s+gTen2a0pb5lv9+4NFE9faH+fOxc7HUnbU1UFB9fZFsT2JZC3Aqoq7HTXnhe7xiMm2N3fHOau7qOrgGeBYGChMeZu1/OXAL8yxlzZRBCpNJwU2gNVxpgTInIF8IQxpm9TAXu7+6i2PYUl3PriCg4fP83TtwxnXH+/r/bRtmycb9cw9JkI096w3wD9XdFOeHIYTPofuOB+p6NRpcfsGET1gHXxXvt8p972c9dnAqRe6MiKc7ckBdeJQoAYY8yRWs9Fu97baF9KY0mhnmN3AyONMQWNHedkUgA4fPw0t81ZwZaDx/nb1PO4blg3x2JRrbDqJVj4gB0wnPJn6DbC6YjOzddPwCe/hwfWQ8ceTkejajPGTmutThC7v7JbpAaHQcr53w9YJ6bZci0e5s6kkADcB6QBBsgG/m2MOdSMIFJpuKWQBBwyxhgRyQDeBnqYJgJyOikAHC8tZ9arq/hmRyH/deVA7rqol6PxqBZaNw8W/18oOQxDb7Lz1f11ZtLzE2x3xaxlTkeimlJeCnu/dQ1YL4V8V295TBfXgLXrJ6qTRy7vru6jscAbwEvAKkCA4cCPgRnGmK8bee+bwDggHjgEPAKEAhhjnhGR+4F7gQrgFPCQMeabpgL2haQAds/nn89by4cbDjLrkl48PGUA4oVsr9yk9Bh8/Q/45imQILjgpzD2AbsRi784ug/+PgjG/w4u/qXT0aiWOrbfTnmtnvpaWgwIdBtuWxB9JtqWrJu2VHVXUvgOuLfuegQRSQeeNcaMPudIW8hXkgJAZZXhkcyNvPbdXm4YkczjPxxCSLD/zFtW2D7fTx+FjW/baavjfwfp0yHIDxYrLn8WPvoV3L8S4pscjlO+rKoS9q9xDVh/aov1mSpbsqTXOFeSmHBOLVp3JYVsY8yglr7mSb6UFMCufn5yyXb+/ulWXf3sz3KzYNFv7eyRxCF2n4ZelzgdVePmXAknC+G+75yORLnbqSN2r+3tn9pWxDFX9eaxD8BlTa7xrZe71imIiJw191JEOjXjvW2CiPDAxL78zw8G89mWfG55cTnFJ3VHJ7/TfZSd0nnDbLuW4ZVr4I1pULDN6cjqd+Iw7P1G1yYEqsiOkHYdXPsU/HwT/GQ5TP6TVzZPauoP+9+BxSJyiYjEuH7GAR8B//B4dH5kxuge/HvGcDbkHWXqM99y4Ogpp0NSLSUCg6+H+7PsQqXdX8G/x8BHv4aTPrYOc/P7tnthkBbAC3gikDAAzr/Prn3w9OWaMfvoKuyGOrVnH/3VGONIkRVf6z6q69sdhcx8ZSUxESG8cmcGfRL8t4BWm3fiMHz+JzuNNTwGLvk1jLrbN9Y3vPpDu0bhZ2u8Mp1R+T93dR9hjHnfGHOxMSbOGBPvur9QRB50T6iB5fzeccydNYaySsMNz3zLmr1Hmn6T8k3tOsNVf4d7vrazQBb91tYYynnf2ZLKp47ArmW2laAJQbnZuYwLPOS2KAJMWtcOvHPvBXSIDGX688v5bEu+0yGpc5E4yO7RMGO+XXg0bwa8dJUttueELR/btQm6d4LygHNJCvqopbvvAAAXS0lEQVQVpREpcVG8fc8F9Ooczd0vr2TBmjynQ1Lnqu9E22q48v/B4Rx4bhwsuNfON/emnIXQvht0He7d66o24VySgn/tzuOAzjHhzJ05hoyenfj5vHW88OVOp0NS5yo4BEbdZfvyx/7Mrm/45wj4/HEoK/H89U+fsCtiB17tV7X8lf9o9FMlIsdF5Fg9P8eBrl6K0a/FRIQy5/ZRXDmkC//9QQ5//jBH934OBBEd7Hzx+7Og32T4/M82Oax9w1bO9JRti6GiVLuOlMc0mhSMMTHGmPb1/MQYY9yz9roNCA8J5smbh/Gj83vw7Bc7+eV/1lOuez8Hho6pMPUlu+VnTBd49154fpydzuoJOZl2c5eUMZ45v2rztP3pJcFBwqPXpPHQZf2YvzqPWa+u4lRZpdNhKXdJGQN3LYEfvgAlhfDSlTB3hq2S6S7lpbB1MQy40j/KcCi/pEnBi0SEn03oy59+MITPt+Qz44XvdPVzIAkKgqFT4acrYfx/wY7P4F+j4ePf2mmk52rHUigv0a4j5VGaFBwwfXQK/54xnI37j3HDM9+yv1hXPweU0Ei4+P/Ywej0m+G7f9uNcJY/e27bNOZk2rGM1Aa3RlfqnGlScMiUwV145Y4MDh0t5fqnv2F7/nGnQ1LuFpMI1/zT7uubNNRWNP33GNjyUcsXv1WWw5YPof8VvrGiWgUsTQoOGtMrjnmzzqeiyq5+Xq2rnwNT0hD40Xtw8zxA7F7Rr1wLBzc0/xy7vrCF+rTrSHmYJgWHDeranvn3XEBsZCjTn/+Ozzbr6ueAJAL9p8BPvoXL/woH18MzF8F798Pxg02/PycTQqPtzlxKeZAmBR+QEhfF2/deQN+EGO56ZSXzV+nq54AVHAqjZ9rxhvPvg3Vz4cnhsOyvUHay/vdUVcLmD6DfJAiN8G68qs3RpOAj4tuF8+bMMYzp1Ylf/Gcdz33hxqmMyvdEdrQb+dy3HPqMh8/+G54aCevfOnvx297v7H7S2nWkvECTgg9pFx7C7NtGceXQLvzpw83c+9oqvtlRoCugA1lcb7jpNbjtQ7so7Z274YUJsOfb74/JyYSQCOg7ybk4VZuhq5J9THhIMP+cNozendvx0te7+GjjQVLjorhpVAo3jEimc0y40yEqT0gdC3d/BuvnwZJHYc4UGHQdTHzEFsDrPQHC2zkdpWoDmtxkx9f4+iY77lRaXslHGw/w5opcVuwqIiRImDgwkWkZ3bmob2eCg7RQbUAqK4FvnoKv/wEVp8FUwnXP2DUPSrVSczfZ0aTgJ3YcPsG8rFzeXpVHUUkZ3WIjuXFkd24clUyXDpFOh6c84dgBWPpHyMuCOz+ByFinI1J+TJNCgCqrqOKT7EPMzdrLl9sKCBK4pF9npmWkMH5AAqHBOkyklDqbJoU2ILfoJPOycvnPqlwOHTtN55hwpo5IZtqoFFLiopwOTynlQzQptCEVlVV8tuUw87L2snRzPlUGxvaJY9qoFCalJRIeohU1lWrrNCm0UQePlvKflbnMzcplX/EpOkaFcv3wZKZldKdPQozT4SmlHKJJoY2rqjJ8tb2AuVl7+ST7EOWVhlGpHblpVApXDulCZJi2HpRqSxxPCiIyG7gKyDfGDK7ndQGeAK4ATgK3GWNWN3VeTQotV3DiNPNX5TEvK5edBSXERIRwXXo3pmV0J61rB6fDU0p5gS8khYuBE8ArDSSFK4CfYpPCaOAJY8zops6rSaH1jDEs31XEvKxcPthwgLKKKoYmd2DaqBSuSe9Ku3Bdy6hUoHI8KbiCSAXebyApPAt8box50/V4CzDOGHOgsXNqUnCPoyfLWbAmj7lZuWw+eJyosGCuHtqVaRndSe8ei23IKaUCRXOTgpNfDbsBubUe57meOyspiMhMYCZASkqKV4ILdB2iQrltbE9+fEEqa3OLmbsil4Xr9zNvZS4DkmK4aVR3fjCsG7FRuqGLUm2Jky2FD4A/G2O+cj1eAvzKGLOqsXNqS8FzjpeWs3DdAeZm7WV93lHCQoK4YnAS0zJSGN2zk7YelPJj/tBSyAO613qcDOx3KBYFxESEMn10CtNHp7Bp/1Hmrsjl3bX7eHftfnrFR3PTqO5cPyKZ+HZalE+pQOVkS+FK4H6+H2h+0hiT0dQ5taXgXafKKvlwg209ZO0+QmiwcNmgRKaNSuHCPvEEaVE+pfyC4y0FEXkTGAfEi0ge8AgQCmCMeQb4EJsQtmOnpN7uqVhU60WGBXP9iGSuH5HM9vzjzF2Ry/zVeXy44SDdYiO5aVR3bhzZnaQOuiOYUoFAF6+pFjtdUcniTbYo39fbCwkSGNc/gcsHJzFxYCIdo3VwWilf43hLQQWu8JBgrj6vK1ef15U9hSW8tTKXBav3sXRzPsFBQkZqJyalJTIpLYlusVrWWyl/oi0F5RbGGDbuO8aiTQdZtOkg2/JPADC4W3smD0pi8uAk+ia00xlMSjnEJxaveYImBf+w8/AJFmcfYvGmg6zeWwxAalwUk9OSmJSWyLDuHXWQWikv0qSgfEb+sVI+yTnEok2H+HZHAeWVhvh24Vw2KJHJaYlc0DuesBDdHEgpT9KkoHzSsdJyPtucz+JNh/h8Sz4lZZXEhIcwbkACk9MSGdc/QWswKeUBmhSUzystr+SbHQUs3nSIT7IPUVhSRlhwEGP7xDE5LYkJAxPpHKML5ZRyB00Kyq9UVhlW7TnC4k0HWZR9kNyiU4jAyB4dmTQoiclpSbrFqFLnQJOC8lvGGDYfPO6ayXSInAPHABiQFMOktCQmpyUyqEt7ncmkVAtoUlABI7foJIs2HWRx9iFW7i6iykByx0gmDbIzmUaldiJYZzIp1ShNCiogFZ44zZKcfBZtOsiX2wsoq6iiU3QYEwYkMDktiQv7xhMRqluNKlWXJgUV8E6cruCLrYdZtOkgSzfnc7y0gqiwYC7p15nJaUlcOiCBDpGhToeplE/QMhcq4LULD+GKIV24YkgXyiqq+G5nIYuzD7J40yE+2niQkCDh/N5xTEpL4rKBiVq0T6lm0JaCCjhVVYa1ecUs3mRXVO8sKAEgvXssk9ISmZyWRO/O7RyOUinv0u4jpbAzmXYcPsGiTYdYtOkg6/OOAtArPpoJAxOYMDCRET06EhqsK6pVYNOkoFQ99hef4pPsQ3yac4jlO4soq6yifUQIl/RPYOLABC7p11n3pVYBSZOCUk04cbqCr7YdZklOPp9tyafgRBnBQcKIHh2ZMMC2Inp3jtb1ECogaFJQqgWqqgzr8opZujmfT3PyaxbM9YiLYsKARCYMTGBUaict3Kf8liYFpc7B/uJTLNmcz9KcQ3y9o5CyiipiwkO4uF9nJgxMYFz/BDrpDnPKj2hSUMpNTpZV8PX2QpbkHGLJ5nwOHz9NkMDwlI6MH5jAhAGJ9EvUDYSUb9OkoJQHVFUZNu4/ypKcfJZuzmfDPjubKbljZM04xOhenQgP0VXVyrdoUlDKCw4eLeWzLfksyTnEV9sLKC2vIjosmIv6dmb8wATGD0ggvp2W/1bO06SglJdV7w9R3Yo4cLQUETgvObamFTGwS4x2MylHaFJQykHGGLIPHGNJTj5LNuezLtfuU921Q0TNOMT5veO0eJ/yGk0KSvmQ/OOlfL75MEs2H+LLbQWcLKskMjSYsX3imejqZkpor7WZlOdoUlDKR5WWV7J8V5GdzZSTz77iUwAMTe7A+AEJTByYSFpX3URIuZcmBaX8gDGGLYeO226mnEOsyS3GGEhsH874AYlMGJDA2D7xRIZpN5M6N5oUlPJDhSdO89mWwyzdfIgvthZw4nQFwUFC19gIUjpFkdIp2nUbRY+4KLp3itI9I1Sz+ERSEJEpwBNAMPCCMebxOq/fBvwV2Od66iljzAuNnVOTgmoryiqqWLGriOW7CtlTeJK9RfanqKTsjONio0JJ6WQTRA9XwkiJs7ddOkTqVqUK8IFNdkQkGPgXcBmQB2SJSKYxJrvOofOMMfd7Kg6l/FVYSBAX9o3nwr7xZzx/vLScvUUnyXUlieqEsXHfURZtPEhF1fdf9EKDheSOZyaM7q5WRkqnKKLDdZ8tdSZPfiIygO3GmJ0AIjIXuBaomxSUUi0QExFKWtcOpHXtcNZrFZVVHDhaWtOq2Ft0kr2upLF27xGOlVaccXx8uzC6V3dH1SQM20WVEBNOkLYy2hxPJoVuQG6tx3nA6HqOu15ELga2Aj83xuTWc4xSqhlCgoPo7vrjPrae14+etK2MPUUlZySMVXuOsHDdfmo1MggPCapJGLXHMapbG7rGIjB5MinU9xWj7gDGQuBNY8xpEbkHeBkYf9aJRGYCMwFSUlLcHadSbUaHqFCGRHVgSPLZrYyyiir2F59ij6uFkVt0kj2FJewtOsXynYWUlFWecXxi+/BaYxnRpMRFktIpmsT24XSMCiMqLFin1fohjw00i8j5wB+MMZNdj38DYIz5cwPHBwNFxpizP6216ECzUt5njKGopOysLqk9ruRx4GjpWe8JCwmiU1QYHaPD6BQdSseoMDpFh31/Gx3mej205nltfXiO4wPNQBbQV0R6YmcXTQOm1z5ARLoYYw64Hl4D5HgwHqVUK4kIce3CiWsXzrCUjme9XlpeSd6RU+QWneTw8dMUnSzjSEkZRSVlHDlZxpGT5WTvP0bRyTKKT5Y3eJ2osOA6SSO0VvKok1SiQomNCtONj9zMY0nBGFMhIvcDi7BTUmcbYzaJyGPASmNMJvAzEbkGqACKgNs8FY9SynMiQoPpk9COPgntmjy2orKKo6fKOXKyjKKS8prEUVTiSiTVCeVkObsLSjhSUsbx0xUNni8mPISOzUgi1a2V2KgwnabbCF28ppTyeWUVVRSftAnDJo/ys1ojNa0SV6I5VV5Z77lEoENkKJ2iwohvF05qfBQ949vRMz6aXp3tzKtA7Mbyhe4jpZRyi7CQIBLaR7SoaGBpeWWtFkj9SST/mF1B/tbKvJr3idhNk3rGt6NXfDQ9a/10jQ38xYCaFJRSASkiNJguHSLp0iGyyWOPldquql0FJew8bG93FZTw9p4jnKjVdRUWEkRqXJQrSbSjV+fomsTRKTosIGZbaVJQSrV57SNCGZocy9Dk2DOeN8Zw+MRpdtVKFDsLSthxuISlm/MprzS1zhFCz85nty56xkf71cpx/4lUKaW8TERIiIkgISaC0b3iznitorKKfcWn2FlQckbSWLGriAVr9p1xbFL7CJsgarUsesZH071TFKHBvjV7SpOCUkq1QkhwED3ioukRF82l/c987VRZJXuKbLLYWdMtdYKPNhzgSK0pucFBQkon2x3Vy5U07P12JLYPd6Q7SpOCUkq5WWRYMAOS2jMgqf1Zrx0pKWNXYclZXVLf7CigtLyq5riosGBS485sXZzXPZbenZue9nsuNCkopZQXVa+pGF5nEWBVleHQ8dKzWheb9h3l440Hqawy3DuuN7+eMsCj8WlSUEopHxAUJDWzpS7oc2a59LKKKnKPnCTSC+snNCkopZSPCwsJ8ni3UTXfGvZWSinlKE0KSimlamhSUEopVUOTglJKqRqaFJRSStXQpKCUUqqGJgWllFI1NCkopZSqoUlBKaVUDU0KSimlamhSUEopVUOTglJKqRqaFJRSStXQpKCUUqqGJgWllFI1NCkopZSqoUlBKaVUDU0KSimlamhSUEopVcOjSUFEpojIFhHZLiIP1/N6uIjMc72+XERSPRmPUkqpxnksKYhIMPAv4HJgEHCziAyqc9idwBFjTB/g78BfPBWPUkqppnmypZABbDfG7DTGlAFzgWvrHHMt8LLr/tvABBERD8aklFKqESEePHc3ILfW4zxgdEPHGGMqROQoEAcU1D5IRGYCM10PT4jIllbGFF/33D7On+L1p1jBv+L1p1jBv+L1p1jh3OLt0ZyDPJkU6vvGb1pxDMaY54DnzjkgkZXGmJHneh5v8ad4/SlW8K94/SlW8K94/SlW8E68nuw+ygO613qcDOxv6BgRCQE6AEUejEkppVQjPJkUsoC+ItJTRMKAaUBmnWMygR+77t8ALDXGnNVSUEop5R0e6z5yjRHcDywCgoHZxphNIvIYsNIYkwm8CLwqItuxLYRpnorH5Zy7oLzMn+L1p1jBv+L1p1jBv+L1p1jBC/GKfjFXSilVTVc0K6WUqqFJQSmlVI02kxSaKrnhS0Rktojki8hGp2Npioh0F5HPRCRHRDaJyANOx9QQEYkQkRUiss4V66NOx9QcIhIsImtE5H2nY2mMiOwWkQ0islZEVjodT1NEJFZE3haRza7P7/lOx1QfEenv+jet/jkmIg967HptYUzBVXJjK3AZdhpsFnCzMSbb0cAaICIXAyeAV4wxg52OpzEi0gXoYoxZLSIxwCrgOl/8t3Wtlo82xpwQkVDgK+ABY8x3DofWKBF5CBgJtDfGXOV0PA0Rkd3ASGOMXywGE5GXgS+NMS+4ZkhGGWOKnY6rMa6/ZfuA0caYPZ64RltpKTSn5IbPMMZ8gZ+s1zDGHDDGrHbdPw7kYFeq+xxjnXA9DHX9+PS3IhFJBq4EXnA6lkAiIu2Bi7EzIDHGlPl6QnCZAOzwVEKAtpMU6iu54ZN/uPyZq8rtMGC5s5E0zNUVsxbIBz4xxvhsrC7/AH4FVDkdSDMYYLGIrHKVpvFlvYDDwBxX19wLIhLtdFDNMA1405MXaCtJoVnlNFTriUg7YD7woDHmmNPxNMQYU2mMSceusM8QEZ/tnhORq4B8Y8wqp2NpprHGmOHYysj3ubpBfVUIMBx42hgzDCgBfH2sMQy4BviPJ6/TVpJCc0puqFZy9c/PB143xrzjdDzN4eoq+ByY4nAojRkLXOPqq58LjBeR15wNqWHGmP2u23xgAbbb1lflAXm1WopvY5OEL7scWG2MOeTJi7SVpNCckhuqFVyDty8COcaY/3U6nsaISGcRiXXdjwQmApudjaphxpjfGGOSjTGp2M/sUmPMLQ6HVS8RiXZNNMDVDTMJ8NnZc8aYg0CuiPR3PTUB8LnJEXXcjIe7jsCzVVJ9RkMlNxwOq0Ei8iYwDogXkTzgEWPMi85G1aCxwK3ABldfPcBvjTEfOhhTQ7oAL7tmcAQBbxljfHqapx9JBBa4tkMJAd4wxnzsbEhN+inwuuuL4k7gdofjaZCIRGFnT87y+LXawpRUpZRSzdNWuo+UUko1gyYFpZRSNTQpKKWUqqFJQSmlVA1NCkoppWpoUlBtmohU1qlA+bDr+c9dVXXXicjX1fPZRSRMRP4hIjtEZJuIvOeqT1R9viQRmet6PVtEPhSRfiKSWrfqrYj8QUR+6bo/RkSWu2LIEZE/ePGfQakabWKdglKNOOUqe1GfGcaYla46Pn/Flhj4ExAD9DPGVIrI7cA7IjLa9Z4FwMvGmGkAIpKOncOfe/bpz/AycKMxZp1rHUX/Jo5XyiM0KSjVtC+AB10LiG4HehpjKgGMMXNE5A5gPLaeVrkx5pnqNxpj1kJNscDGJAAHXO+pxPdX16oApUlBtXWRtVZiA/zZGDOvzjFXAxuAPsDeegr+rQTSXPcbK17Xu861koC/ue7/HdgiIp8DH2NbG6XN/zWUcg9NCqqta6z76HUROQXsxpZE6ET91XXF9Xx91Xhr21H7WrXHDYwxj4nI69iaQdOxdW7GNe9XUMp9NCko1bAZxpiabSVFpAjoISIxrg2Fqg0HFrru39DaixljdgBPi8jzwGERiTPGFLb2fEq1hs4+UqqZjDEl2AHh/3UNBiMiPwKigKWun3ARubv6PSIySkQuaercInKlq+IsQF+gEvCHncBUgNGkoNq6yDpTUh9v4vjfAKXAVhHZBkwFfuDa6tMAPwAuc01J3QT8gebt3XErdkxhLfAqtpVS2dpfSqnW0iqpSimlamhLQSmlVA1NCkoppWpoUlBKKVVDk4JSSqkamhSUUkrV0KSglFKqhiYFpZRSNf4/GU6x0OCtNKYAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "### Loss graph\n",
    "plt.plot(model.history[\"loss\"])\n",
    "plt.plot(model.history[\"val_loss\"])\n",
    "plt.xlabel(\"EPOCHS\")\n",
    "plt.ylabel(\"LOSS\")\n",
    "plt.ylim(0.0,3.0)\n",
    "plt.legend([\"Train\",\"Testing\"],loc=\"upper right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. CNN3D model\n",
    "**Reducing the size of kernel in the model**<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source path =  /mnt/disks/user/project/PROJECT/Project_data/train ; batch size = 32\n",
      "Input Sample shape : (32, 30, 100, 100, 3)\n"
     ]
    }
   ],
   "source": [
    "train_data = train_doc\n",
    "val_data = val_doc\n",
    "transform_size = (100,100)\n",
    "batch_size = 32\n",
    "num_epochs = 8\n",
    "frame_selection_list = getframeselectionlist(choiceoflist=0)\n",
    "model_name = \"CNN3D_v2_complex\"\n",
    "input_sample = next(generator(train_path,\n",
    "                              train_data, \n",
    "                              batch_size=batch_size,\n",
    "                              transform_size=transform_size,\n",
    "                              frame_selection_list=frame_selection_list))\n",
    "print(\"Input Sample shape : %s\" %(str(input_sample[0].shape)))\n",
    "model = Sequential()\n",
    "model.add(Conv3D(16,(2,2,2),padding=\"same\",input_shape=(input_sample[0].shape[1],input_sample[0].shape[2],input_sample[0].shape[3],input_sample[0].shape[4])))\n",
    "model.add(Activation(\"relu\"))\n",
    "model.add(MaxPooling3D(pool_size=(2,2,2)))\n",
    "\n",
    "model.add(Conv3D(32,(2,2,2),padding=\"same\"))\n",
    "model.add(Activation(\"relu\"))\n",
    "model.add(MaxPooling3D(pool_size=(2,2,2)))\n",
    "\n",
    "model.add(Conv3D(64,(2,2,2),padding=\"same\"))\n",
    "model.add(Activation(\"relu\"))\n",
    "model.add(MaxPooling3D(pool_size=(2,2,2)))\n",
    "\n",
    "model.add(Conv3D(32,(2,2,2),padding=\"same\"))\n",
    "model.add(Activation(\"relu\"))\n",
    "\n",
    "model.add(Conv3D(32,(2,2,2),padding=\"same\"))\n",
    "model.add(Activation(\"relu\"))\n",
    "model.add(MaxPooling3D(pool_size=(2,2,2)))\n",
    "\n",
    "model.add(Flatten())\n",
    "\n",
    "model.add(Dense(512,activation='relu'))\n",
    "model.add(BatchNormalization())\n",
    "\n",
    "model.add(Dense(256,activation='relu'))\n",
    "model.add(BatchNormalization())\n",
    "\n",
    "model.add(Dense(128,activation='relu'))\n",
    "model.add(BatchNormalization())\n",
    "\n",
    "model.add(Dense(64,activation='relu'))\n",
    "model.add(BatchNormalization())\n",
    "\n",
    "model.add(Dense(input_sample[1].shape[1], activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv3d_32 (Conv3D)           (None, 30, 100, 100, 16)  400       \n",
      "_________________________________________________________________\n",
      "activation_32 (Activation)   (None, 30, 100, 100, 16)  0         \n",
      "_________________________________________________________________\n",
      "max_pooling3d_29 (MaxPooling (None, 15, 50, 50, 16)    0         \n",
      "_________________________________________________________________\n",
      "conv3d_33 (Conv3D)           (None, 15, 50, 50, 32)    4128      \n",
      "_________________________________________________________________\n",
      "activation_33 (Activation)   (None, 15, 50, 50, 32)    0         \n",
      "_________________________________________________________________\n",
      "max_pooling3d_30 (MaxPooling (None, 7, 25, 25, 32)     0         \n",
      "_________________________________________________________________\n",
      "conv3d_34 (Conv3D)           (None, 7, 25, 25, 64)     16448     \n",
      "_________________________________________________________________\n",
      "activation_34 (Activation)   (None, 7, 25, 25, 64)     0         \n",
      "_________________________________________________________________\n",
      "max_pooling3d_31 (MaxPooling (None, 3, 12, 12, 64)     0         \n",
      "_________________________________________________________________\n",
      "conv3d_35 (Conv3D)           (None, 3, 12, 12, 32)     16416     \n",
      "_________________________________________________________________\n",
      "activation_35 (Activation)   (None, 3, 12, 12, 32)     0         \n",
      "_________________________________________________________________\n",
      "conv3d_36 (Conv3D)           (None, 3, 12, 12, 32)     8224      \n",
      "_________________________________________________________________\n",
      "activation_36 (Activation)   (None, 3, 12, 12, 32)     0         \n",
      "_________________________________________________________________\n",
      "max_pooling3d_32 (MaxPooling (None, 1, 6, 6, 32)       0         \n",
      "_________________________________________________________________\n",
      "flatten_10 (Flatten)         (None, 1152)              0         \n",
      "_________________________________________________________________\n",
      "dense_34 (Dense)             (None, 512)               590336    \n",
      "_________________________________________________________________\n",
      "batch_normalization_25 (Batc (None, 512)               2048      \n",
      "_________________________________________________________________\n",
      "dense_35 (Dense)             (None, 256)               131328    \n",
      "_________________________________________________________________\n",
      "batch_normalization_26 (Batc (None, 256)               1024      \n",
      "_________________________________________________________________\n",
      "dense_36 (Dense)             (None, 128)               32896     \n",
      "_________________________________________________________________\n",
      "batch_normalization_27 (Batc (None, 128)               512       \n",
      "_________________________________________________________________\n",
      "dense_37 (Dense)             (None, 64)                8256      \n",
      "_________________________________________________________________\n",
      "batch_normalization_28 (Batc (None, 64)                256       \n",
      "_________________________________________________________________\n",
      "dense_38 (Dense)             (None, 5)                 325       \n",
      "=================================================================\n",
      "Total params: 812,597\n",
      "Trainable params: 810,677\n",
      "Non-trainable params: 1,920\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "optimiser = optimizers.Adam(lr=0.0002)#write your optimizer\n",
    "model.compile(optimizer=optimiser, loss='categorical_crossentropy', metrics=['categorical_accuracy'])\n",
    "print (model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us create the `train_generator` and the `val_generator` which will be used in `.fit_generator`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_generator = generator(train_path,\n",
    "                            train_data, \n",
    "                            batch_size=batch_size,\n",
    "                            transform_size=transform_size,\n",
    "                            frame_selection_list=frame_selection_list)\n",
    "val_generator = generator(val_path,\n",
    "                          val_data, \n",
    "                          batch_size=batch_size,\n",
    "                          transform_size=transform_size,\n",
    "                          frame_selection_list=frame_selection_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### Setting up extra parameter for Neural Network\n",
    "##### ModelCheckPoint\n",
    "We build modelcheckpoint to save models after every epoch, so that we can refer to models to best model later on for final testing.\n",
    "\n",
    "##### ReduceLROnPlateau\n",
    "If the val_loss value stops improving after patience number of epochs, we reduce the learning rate so as to take smaller steps down the gradient route for raeaching global minimum and avoid getting stuch at local minimum.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'model_init' + '_' + str(curr_dt_time).replace(' ','').replace(':','_') + '/'\n",
    "    \n",
    "if not os.path.exists(model_name):\n",
    "    os.mkdir(model_name)\n",
    "        \n",
    "filepath = model_name + 'model-{epoch:05d}-{loss:.5f}-{categorical_accuracy:.5f}-{val_loss:.5f}-{val_categorical_accuracy:.5f}.h5'\n",
    "\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='val_loss', verbose=1, save_best_only=False, save_weights_only=False, mode='auto', period=1)\n",
    "\n",
    "LR = ReduceLROnPlateau(monitor='val_loss', factor=0.2,\n",
    "                       patience=5, min_lr=0.001,verbose=1)\n",
    "callbacks_list = [checkpoint, LR]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `steps_per_epoch` and `validation_steps` are used by `fit_generator` to decide the number of next() calls it need to make."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "if (num_train_sequences%batch_size) == 0:\n",
    "    steps_per_epoch = int(num_train_sequences/batch_size)\n",
    "else:\n",
    "    steps_per_epoch = (num_train_sequences//batch_size) + 1\n",
    "\n",
    "if (num_val_sequences%batch_size) == 0:\n",
    "    validation_steps = int(num_val_sequences/batch_size)\n",
    "else:\n",
    "    validation_steps = (num_val_sequences//batch_size) + 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us now fit the model. This will start training the model and with the help of the checkpoints, you'll be able to save the model at the end of each epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source path =  /mnt/disks/user/project/PROJECT/Project_data/val ; batch size = 32\n",
      "Source path =  /mnt/disks/user/project/PROJECT/Project_data/train ; batch size = 32\n",
      "Epoch 1/8\n",
      "21/21 [==============================] - 87s 4s/step - loss: 1.6659 - categorical_accuracy: 0.3542 - val_loss: 2.3984 - val_categorical_accuracy: 0.3700\n",
      "\n",
      "Epoch 00001: saving model to model_init_2021-02-0801_33_00.731094/model-00001-1.66121-0.35596-2.39835-0.37000.h5\n",
      "Epoch 2/8\n",
      "21/21 [==============================] - 69s 3s/step - loss: 1.1204 - categorical_accuracy: 0.5589 - val_loss: 1.8715 - val_categorical_accuracy: 0.3400\n",
      "\n",
      "Epoch 00002: saving model to model_init_2021-02-0801_33_00.731094/model-00002-1.11765-0.56109-1.87154-0.34000.h5\n",
      "Epoch 3/8\n",
      "21/21 [==============================] - 73s 3s/step - loss: 0.8833 - categorical_accuracy: 0.6898 - val_loss: 1.5233 - val_categorical_accuracy: 0.4400\n",
      "\n",
      "Epoch 00003: saving model to model_init_2021-02-0801_33_00.731094/model-00003-0.87219-0.69532-1.52327-0.44000.h5\n",
      "Epoch 4/8\n",
      "21/21 [==============================] - 73s 3s/step - loss: 0.6923 - categorical_accuracy: 0.7724 - val_loss: 1.7793 - val_categorical_accuracy: 0.4300\n",
      "\n",
      "Epoch 00004: saving model to model_init_2021-02-0801_33_00.731094/model-00004-0.68289-0.77677-1.77933-0.43000.h5\n",
      "Epoch 5/8\n",
      "21/21 [==============================] - 76s 4s/step - loss: 0.5448 - categorical_accuracy: 0.8273 - val_loss: 1.5923 - val_categorical_accuracy: 0.4200\n",
      "\n",
      "Epoch 00005: saving model to model_init_2021-02-0801_33_00.731094/model-00005-0.53956-0.82956-1.59225-0.42000.h5\n",
      "Epoch 6/8\n",
      "21/21 [==============================] - 77s 4s/step - loss: 0.4470 - categorical_accuracy: 0.8951 - val_loss: 1.4924 - val_categorical_accuracy: 0.4400\n",
      "\n",
      "Epoch 00006: saving model to model_init_2021-02-0801_33_00.731094/model-00006-0.43664-0.90045-1.49242-0.44000.h5\n",
      "Epoch 7/8\n",
      "21/21 [==============================] - 74s 4s/step - loss: 0.4156 - categorical_accuracy: 0.8891 - val_loss: 1.3681 - val_categorical_accuracy: 0.4700\n",
      "\n",
      "Epoch 00007: saving model to model_init_2021-02-0801_33_00.731094/model-00007-0.40535-0.89442-1.36809-0.47000.h5\n",
      "Epoch 8/8\n",
      "21/21 [==============================] - 75s 4s/step - loss: 0.3517 - categorical_accuracy: 0.9382 - val_loss: 1.5887 - val_categorical_accuracy: 0.3900\n",
      "\n",
      "Epoch 00008: saving model to model_init_2021-02-0801_33_00.731094/model-00008-0.33727-0.94419-1.58875-0.39000.h5\n"
     ]
    }
   ],
   "source": [
    "model = model.fit_generator(train_generator, steps_per_epoch=steps_per_epoch, epochs=num_epochs, verbose=1, \n",
    "                    callbacks=callbacks_list, validation_data=val_generator, \n",
    "                    validation_steps=validation_steps, class_weight=None, workers=1, initial_epoch=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "### Accuracy graph\n",
    "plt.plot(model.history[\"categorical_accuracy\"])\n",
    "plt.plot(model.history[\"val_categorical_accuracy\"])\n",
    "plt.xlabel(\"EPOCHS\")\n",
    "plt.ylabel(\"Percentage\")\n",
    "plt.ylim(0.0,1.0)\n",
    "plt.legend([\"Train\",\"Test\"],loc=\"lower right\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "### Loss graph\n",
    "plt.plot(model.history[\"loss\"])\n",
    "plt.plot(model.history[\"val_loss\"])\n",
    "plt.xlabel(\"EPOCHS\")\n",
    "plt.ylabel(\"LOSS\")\n",
    "plt.ylim(0.0,3.0)\n",
    "plt.legend([\"Train\",\"Testing\"],loc=\"upper right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### After decreasing the parameters by 100000 the model still performs as earlier, but our training accuracy has increase, lets continue this model with dropouts.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. CNN3D model\n",
    "**Using Dropouts**<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source path =  /mnt/disks/user/project/PROJECT/Project_data/train ; batch size = 32\n",
      "Input Sample shape : (32, 30, 100, 100, 3)\n"
     ]
    }
   ],
   "source": [
    "train_data = train_doc\n",
    "val_data = val_doc\n",
    "transform_size = (100,100)\n",
    "batch_size = 32\n",
    "num_epochs = 10\n",
    "frame_selection_list = getframeselectionlist(choiceoflist=0)\n",
    "model_name = \"CNN3D_v2_complex\"\n",
    "input_sample = next(generator(train_path,\n",
    "                              train_data, \n",
    "                              batch_size=batch_size,\n",
    "                              transform_size=transform_size,\n",
    "                              frame_selection_list=frame_selection_list))\n",
    "print(\"Input Sample shape : %s\" %(str(input_sample[0].shape)))\n",
    "model = Sequential()\n",
    "model.add(Conv3D(16,(2,2,2),padding=\"same\",input_shape=(input_sample[0].shape[1],input_sample[0].shape[2],input_sample[0].shape[3],input_sample[0].shape[4])))\n",
    "model.add(Activation(\"relu\"))\n",
    "model.add(MaxPooling3D(pool_size=(2,2,2)))\n",
    "\n",
    "model.add(Conv3D(32,(2,2,2),padding=\"same\"))\n",
    "model.add(Activation(\"relu\"))\n",
    "model.add(MaxPooling3D(pool_size=(2,2,2)))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "model.add(Conv3D(64,(2,2,2),padding=\"same\"))\n",
    "model.add(Activation(\"relu\"))\n",
    "model.add(MaxPooling3D(pool_size=(2,2,2)))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "\n",
    "model.add(Conv3D(32,(2,2,2),padding=\"same\"))\n",
    "model.add(Activation(\"relu\"))\n",
    "\n",
    "model.add(Conv3D(32,(2,2,2),padding=\"same\"))\n",
    "model.add(Activation(\"relu\"))\n",
    "model.add(MaxPooling3D(pool_size=(2,2,2)))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "\n",
    "model.add(Flatten())\n",
    "\n",
    "model.add(Dense(512,activation='relu'))\n",
    "model.add(BatchNormalization())\n",
    "\n",
    "model.add(Dense(256,activation='relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "\n",
    "model.add(Dense(128,activation='relu'))\n",
    "model.add(BatchNormalization())\n",
    "\n",
    "model.add(Dense(64,activation='relu'))\n",
    "model.add(BatchNormalization())\n",
    "\n",
    "model.add(Dense(input_sample[1].shape[1], activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv3d_37 (Conv3D)           (None, 30, 100, 100, 16)  400       \n",
      "_________________________________________________________________\n",
      "activation_37 (Activation)   (None, 30, 100, 100, 16)  0         \n",
      "_________________________________________________________________\n",
      "max_pooling3d_33 (MaxPooling (None, 15, 50, 50, 16)    0         \n",
      "_________________________________________________________________\n",
      "conv3d_38 (Conv3D)           (None, 15, 50, 50, 32)    4128      \n",
      "_________________________________________________________________\n",
      "activation_38 (Activation)   (None, 15, 50, 50, 32)    0         \n",
      "_________________________________________________________________\n",
      "max_pooling3d_34 (MaxPooling (None, 7, 25, 25, 32)     0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 7, 25, 25, 32)     0         \n",
      "_________________________________________________________________\n",
      "conv3d_39 (Conv3D)           (None, 7, 25, 25, 64)     16448     \n",
      "_________________________________________________________________\n",
      "activation_39 (Activation)   (None, 7, 25, 25, 64)     0         \n",
      "_________________________________________________________________\n",
      "max_pooling3d_35 (MaxPooling (None, 3, 12, 12, 64)     0         \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 3, 12, 12, 64)     0         \n",
      "_________________________________________________________________\n",
      "conv3d_40 (Conv3D)           (None, 3, 12, 12, 32)     16416     \n",
      "_________________________________________________________________\n",
      "activation_40 (Activation)   (None, 3, 12, 12, 32)     0         \n",
      "_________________________________________________________________\n",
      "conv3d_41 (Conv3D)           (None, 3, 12, 12, 32)     8224      \n",
      "_________________________________________________________________\n",
      "activation_41 (Activation)   (None, 3, 12, 12, 32)     0         \n",
      "_________________________________________________________________\n",
      "max_pooling3d_36 (MaxPooling (None, 1, 6, 6, 32)       0         \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 1, 6, 6, 32)       0         \n",
      "_________________________________________________________________\n",
      "flatten_11 (Flatten)         (None, 1152)              0         \n",
      "_________________________________________________________________\n",
      "dense_39 (Dense)             (None, 512)               590336    \n",
      "_________________________________________________________________\n",
      "batch_normalization_29 (Batc (None, 512)               2048      \n",
      "_________________________________________________________________\n",
      "dense_40 (Dense)             (None, 256)               131328    \n",
      "_________________________________________________________________\n",
      "batch_normalization_30 (Batc (None, 256)               1024      \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_41 (Dense)             (None, 128)               32896     \n",
      "_________________________________________________________________\n",
      "batch_normalization_31 (Batc (None, 128)               512       \n",
      "_________________________________________________________________\n",
      "dense_42 (Dense)             (None, 64)                8256      \n",
      "_________________________________________________________________\n",
      "batch_normalization_32 (Batc (None, 64)                256       \n",
      "_________________________________________________________________\n",
      "dense_43 (Dense)             (None, 5)                 325       \n",
      "=================================================================\n",
      "Total params: 812,597\n",
      "Trainable params: 810,677\n",
      "Non-trainable params: 1,920\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "optimiser = optimizers.Adam(lr=0.0002)#write your optimizer\n",
    "model.compile(optimizer=optimiser, loss='categorical_crossentropy', metrics=['categorical_accuracy'])\n",
    "print (model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us create the `train_generator` and the `val_generator` which will be used in `.fit_generator`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_generator = generator(train_path,\n",
    "                            train_data, \n",
    "                            batch_size=batch_size,\n",
    "                            transform_size=transform_size,\n",
    "                            frame_selection_list=frame_selection_list)\n",
    "val_generator = generator(val_path,\n",
    "                          val_data, \n",
    "                          batch_size=batch_size,\n",
    "                          transform_size=transform_size,\n",
    "                          frame_selection_list=frame_selection_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### Setting up extra parameter for Neural Network\n",
    "##### ModelCheckPoint\n",
    "We build modelcheckpoint to save models after every epoch, so that we can refer to models to best model later on for final testing.\n",
    "\n",
    "##### ReduceLROnPlateau\n",
    "If the val_loss value stops improving after patience number of epochs, we reduce the learning rate so as to take smaller steps down the gradient route for raeaching global minimum and avoid getting stuch at local minimum.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'model_init' + '_' + str(curr_dt_time).replace(' ','').replace(':','_') + '/'\n",
    "    \n",
    "if not os.path.exists(model_name):\n",
    "    os.mkdir(model_name)\n",
    "        \n",
    "filepath = model_name + 'model-{epoch:05d}-{loss:.5f}-{categorical_accuracy:.5f}-{val_loss:.5f}-{val_categorical_accuracy:.5f}.h5'\n",
    "\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='val_loss', verbose=1, save_best_only=False, save_weights_only=False, mode='auto', period=1)\n",
    "\n",
    "LR = ReduceLROnPlateau(monitor='val_loss', factor=0.2,\n",
    "                       patience=5, min_lr=0.001,verbose=1)\n",
    "callbacks_list = [checkpoint, LR]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `steps_per_epoch` and `validation_steps` are used by `fit_generator` to decide the number of next() calls it need to make."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "if (num_train_sequences%batch_size) == 0:\n",
    "    steps_per_epoch = int(num_train_sequences/batch_size)\n",
    "else:\n",
    "    steps_per_epoch = (num_train_sequences//batch_size) + 1\n",
    "\n",
    "if (num_val_sequences%batch_size) == 0:\n",
    "    validation_steps = int(num_val_sequences/batch_size)\n",
    "else:\n",
    "    validation_steps = (num_val_sequences//batch_size) + 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us now fit the model. This will start training the model and with the help of the checkpoints, you'll be able to save the model at the end of each epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source path =  /mnt/disks/user/project/PROJECT/Project_data/val ; batch size = 32\n",
      "Source path =  /mnt/disks/user/project/PROJECT/Project_data/train ; batch size =Epoch 1/10\n",
      " 32\n",
      "21/21 [==============================] - 86s 4s/step - loss: 2.1469 - categorical_accuracy: 0.2421 - val_loss: 2.3001 - val_categorical_accuracy: 0.1700\n",
      "\n",
      "Epoch 00001: saving model to model_init_2021-02-0801_33_00.731094/model-00001-2.15261-0.24133-2.30014-0.17000.h5\n",
      "Epoch 2/10\n",
      "21/21 [==============================] - 71s 3s/step - loss: 1.9076 - categorical_accuracy: 0.2764 - val_loss: 2.4539 - val_categorical_accuracy: 0.1600\n",
      "\n",
      "Epoch 00002: saving model to model_init_2021-02-0801_33_00.731094/model-00002-1.90660-0.27602-2.45394-0.16000.h5\n",
      "Epoch 3/10\n",
      "21/21 [==============================] - 73s 3s/step - loss: 1.7965 - categorical_accuracy: 0.3052 - val_loss: 2.6350 - val_categorical_accuracy: 0.1600\n",
      "\n",
      "Epoch 00003: saving model to model_init_2021-02-0801_33_00.731094/model-00003-1.79963-0.30468-2.63500-0.16000.h5\n",
      "Epoch 4/10\n",
      "21/21 [==============================] - 74s 4s/step - loss: 1.7450 - categorical_accuracy: 0.3339 - val_loss: 2.5568 - val_categorical_accuracy: 0.1600\n",
      "\n",
      "Epoch 00004: saving model to model_init_2021-02-0801_33_00.731094/model-00004-1.74405-0.33484-2.55684-0.16000.h5\n",
      "Epoch 5/10\n",
      "21/21 [==============================] - 73s 3s/step - loss: 1.6402 - categorical_accuracy: 0.3145 - val_loss: 2.7716 - val_categorical_accuracy: 0.1600\n",
      "\n",
      "Epoch 00005: saving model to model_init_2021-02-0801_33_00.731094/model-00005-1.63741-0.31523-2.77156-0.16000.h5\n",
      "Epoch 6/10\n",
      "21/21 [==============================] - 73s 3s/step - loss: 1.6065 - categorical_accuracy: 0.3484 - val_loss: 2.9364 - val_categorical_accuracy: 0.1600\n",
      "\n",
      "Epoch 00006: saving model to model_init_2021-02-0801_33_00.731094/model-00006-1.60079-0.34842-2.93640-0.16000.h5\n",
      "Epoch 7/10\n",
      "21/21 [==============================] - 71s 3s/step - loss: 1.5805 - categorical_accuracy: 0.3501 - val_loss: 3.3256 - val_categorical_accuracy: 0.1600\n",
      "\n",
      "Epoch 00007: saving model to model_init_2021-02-0801_33_00.731094/model-00007-1.58666-0.34842-3.32562-0.16000.h5\n",
      "Epoch 8/10\n",
      "21/21 [==============================] - 73s 3s/step - loss: 1.5382 - categorical_accuracy: 0.3743 - val_loss: 3.0294 - val_categorical_accuracy: 0.1600\n",
      "\n",
      "Epoch 00008: saving model to model_init_2021-02-0801_33_00.731094/model-00008-1.53866-0.37406-3.02944-0.16000.h5\n",
      "Epoch 9/10\n",
      "21/21 [==============================] - 74s 4s/step - loss: 1.5609 - categorical_accuracy: 0.3533 - val_loss: 3.0243 - val_categorical_accuracy: 0.1600\n",
      "\n",
      "Epoch 00009: saving model to model_init_2021-02-0801_33_00.731094/model-00009-1.55406-0.35445-3.02430-0.16000.h5\n",
      "Epoch 10/10\n",
      "21/21 [==============================] - 72s 3s/step - loss: 1.4448 - categorical_accuracy: 0.3942 - val_loss: 3.0876 - val_categorical_accuracy: 0.1600\n",
      "\n",
      "Epoch 00010: saving model to model_init_2021-02-0801_33_00.731094/model-00010-1.44904-0.39367-3.08762-0.16000.h5\n"
     ]
    }
   ],
   "source": [
    "model = model.fit_generator(train_generator, steps_per_epoch=steps_per_epoch, epochs=num_epochs, verbose=1, \n",
    "                    callbacks=callbacks_list, validation_data=val_generator, \n",
    "                    validation_steps=validation_steps, class_weight=None, workers=1, initial_epoch=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "### Accuracy graph\n",
    "plt.plot(model.history[\"categorical_accuracy\"])\n",
    "plt.plot(model.history[\"val_categorical_accuracy\"])\n",
    "plt.xlabel(\"EPOCHS\")\n",
    "plt.ylabel(\"Percentage\")\n",
    "plt.ylim(0.0,1.0)\n",
    "plt.legend([\"Train\",\"Test\"],loc=\"lower right\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "### Loss graph\n",
    "plt.plot(model.history[\"loss\"])\n",
    "plt.plot(model.history[\"val_loss\"])\n",
    "plt.xlabel(\"EPOCHS\")\n",
    "plt.ylabel(\"LOSS\")\n",
    "plt.ylim(0.0,3.0)\n",
    "plt.legend([\"Train\",\"Testing\"],loc=\"upper right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using Dropouts doesnt seem to be a very good idea , so removing Dropouts. And now using CNN2d with LSTM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. CNN2D with GRU\n",
    "**simple model**<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source path =  /mnt/disks/user/project/PROJECT/Project_data/train ; batch size = 32\n",
      "Input Sample shape : (32, 30, 100, 100, 3)\n"
     ]
    }
   ],
   "source": [
    "train_data = train_doc\n",
    "val_data = val_doc\n",
    "transform_size = (100,100)\n",
    "batch_size = 32\n",
    "num_epochs = 12\n",
    "frame_selection_list = getframeselectionlist(choiceoflist=0)\n",
    "model_name = \"CNN2D_LSTM\"\n",
    "input_sample = next(generator(train_path,\n",
    "                              train_data, \n",
    "                              batch_size=batch_size,\n",
    "                              transform_size=transform_size,\n",
    "                              frame_selection_list=frame_selection_list))\n",
    "print(\"Input Sample shape : %s\" %(str(input_sample[0].shape)))\n",
    "model = Sequential()\n",
    "\n",
    "model.add(TimeDistributed(Conv2D(16, (3, 3) , padding='same', activation='relu'),\n",
    "                          input_shape=(input_sample[0].shape[1],input_sample[0].shape[2],input_sample[0].shape[3],input_sample[0].shape[4])))\n",
    "model.add(TimeDistributed(BatchNormalization()))\n",
    "model.add(TimeDistributed(MaxPooling2D((2, 2))))\n",
    "\n",
    "model.add(TimeDistributed(Conv2D(32, (3, 3) , padding='same', activation='relu')))\n",
    "model.add(TimeDistributed(BatchNormalization()))\n",
    "model.add(TimeDistributed(MaxPooling2D((2, 2))))\n",
    "\n",
    "model.add(TimeDistributed(Conv2D(64, (3, 3) , padding='same', activation='relu')))\n",
    "model.add(TimeDistributed(BatchNormalization()))\n",
    "model.add(TimeDistributed(MaxPooling2D((2, 2))))\n",
    "\n",
    "model.add(TimeDistributed(Conv2D(128, (3, 3) , padding='same', activation='relu')))\n",
    "model.add(TimeDistributed(BatchNormalization()))\n",
    "model.add(TimeDistributed(MaxPooling2D((2, 2))))\n",
    "\n",
    "\n",
    "\n",
    "model.add(TimeDistributed(Flatten()))\n",
    "\n",
    "model.add(GRU(64))\n",
    "\n",
    "model.add(Dense(64,activation='relu'))\n",
    "model.add(Dense(input_sample[1].shape[1], activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "time_distributed_1 (TimeDist (None, 30, 100, 100, 16)  448       \n",
      "_________________________________________________________________\n",
      "time_distributed_2 (TimeDist (None, 30, 100, 100, 16)  64        \n",
      "_________________________________________________________________\n",
      "time_distributed_3 (TimeDist (None, 30, 50, 50, 16)    0         \n",
      "_________________________________________________________________\n",
      "time_distributed_4 (TimeDist (None, 30, 50, 50, 32)    4640      \n",
      "_________________________________________________________________\n",
      "time_distributed_5 (TimeDist (None, 30, 50, 50, 32)    128       \n",
      "_________________________________________________________________\n",
      "time_distributed_6 (TimeDist (None, 30, 25, 25, 32)    0         \n",
      "_________________________________________________________________\n",
      "time_distributed_7 (TimeDist (None, 30, 25, 25, 64)    18496     \n",
      "_________________________________________________________________\n",
      "time_distributed_8 (TimeDist (None, 30, 25, 25, 64)    256       \n",
      "_________________________________________________________________\n",
      "time_distributed_9 (TimeDist (None, 30, 12, 12, 64)    0         \n",
      "_________________________________________________________________\n",
      "time_distributed_10 (TimeDis (None, 30, 12, 12, 128)   73856     \n",
      "_________________________________________________________________\n",
      "time_distributed_11 (TimeDis (None, 30, 12, 12, 128)   512       \n",
      "_________________________________________________________________\n",
      "time_distributed_12 (TimeDis (None, 30, 6, 6, 128)     0         \n",
      "_________________________________________________________________\n",
      "time_distributed_13 (TimeDis (None, 30, 4608)          0         \n",
      "_________________________________________________________________\n",
      "gru_1 (GRU)                  (None, 64)                897216    \n",
      "_________________________________________________________________\n",
      "dense_44 (Dense)             (None, 64)                4160      \n",
      "_________________________________________________________________\n",
      "dense_45 (Dense)             (None, 5)                 325       \n",
      "=================================================================\n",
      "Total params: 1,000,101\n",
      "Trainable params: 999,621\n",
      "Non-trainable params: 480\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "optimiser = optimizers.Adam(lr=0.0002)#write your optimizer\n",
    "model.compile(optimizer=optimiser, loss='categorical_crossentropy', metrics=['categorical_accuracy'])\n",
    "print (model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us create the `train_generator` and the `val_generator` which will be used in `.fit_generator`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_generator = generator(train_path,\n",
    "                            train_data, \n",
    "                            batch_size=batch_size,\n",
    "                            transform_size=transform_size,\n",
    "                            frame_selection_list=frame_selection_list)\n",
    "val_generator = generator(val_path,\n",
    "                          val_data, \n",
    "                          batch_size=batch_size,\n",
    "                          transform_size=transform_size,\n",
    "                          frame_selection_list=frame_selection_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### Setting up extra parameter for Neural Network\n",
    "##### ModelCheckPoint\n",
    "We build modelcheckpoint to save models after every epoch, so that we can refer to models to best model later on for final testing.\n",
    "\n",
    "##### ReduceLROnPlateau\n",
    "If the val_loss value stops improving after patience number of epochs, we reduce the learning rate so as to take smaller steps down the gradient route for raeaching global minimum and avoid getting stuch at local minimum.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'model_init' + '_' + str(curr_dt_time).replace(' ','').replace(':','_') + '/'\n",
    "    \n",
    "if not os.path.exists(model_name):\n",
    "    os.mkdir(model_name)\n",
    "        \n",
    "filepath = model_name + 'model-{epoch:05d}-{loss:.5f}-{categorical_accuracy:.5f}-{val_loss:.5f}-{val_categorical_accuracy:.5f}.h5'\n",
    "\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='val_loss', verbose=1, save_best_only=False, save_weights_only=False, mode='auto', period=1)\n",
    "\n",
    "LR = ReduceLROnPlateau(monitor='val_loss', factor=0.2,\n",
    "                       patience=5, min_lr=0.001,verbose=1)\n",
    "callbacks_list = [checkpoint, LR]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `steps_per_epoch` and `validation_steps` are used by `fit_generator` to decide the number of next() calls it need to make."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "if (num_train_sequences%batch_size) == 0:\n",
    "    steps_per_epoch = int(num_train_sequences/batch_size)\n",
    "else:\n",
    "    steps_per_epoch = (num_train_sequences//batch_size) + 1\n",
    "\n",
    "if (num_val_sequences%batch_size) == 0:\n",
    "    validation_steps = int(num_val_sequences/batch_size)\n",
    "else:\n",
    "    validation_steps = (num_val_sequences//batch_size) + 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us now fit the model. This will start training the model and with the help of the checkpoints, you'll be able to save the model at the end of each epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source path =  /mnt/disks/user/project/PROJECT/Project_data/val ; batch size = 32\n",
      "Source path =  /mnt/disks/user/project/PROJECT/Project_data/train ; batch size = 32\n",
      "Epoch 1/12\n",
      "21/21 [==============================] - 87s 4s/step - loss: 1.4069 - categorical_accuracy: 0.4119 - val_loss: 1.2427 - val_categorical_accuracy: 0.5200\n",
      "\n",
      "Epoch 00001: saving model to model_init_2021-02-0801_33_00.731094/model-00001-1.40657-0.41327-1.24275-0.52000.h5\n",
      "Epoch 2/12\n",
      "21/21 [==============================] - 70s 3s/step - loss: 0.8612 - categorical_accuracy: 0.7340 - val_loss: 1.0762 - val_categorical_accuracy: 0.6300\n",
      "\n",
      "Epoch 00002: saving model to model_init_2021-02-0801_33_00.731094/model-00002-0.85985-0.73454-1.07624-0.63000.h5\n",
      "Epoch 3/12\n",
      "21/21 [==============================] - 75s 4s/step - loss: 0.6045 - categorical_accuracy: 0.8558 - val_loss: 0.9862 - val_categorical_accuracy: 0.6600\n",
      "\n",
      "Epoch 00003: saving model to model_init_2021-02-0801_33_00.731094/model-00003-0.59560-0.86124-0.98623-0.66000.h5\n",
      "Epoch 4/12\n",
      "21/21 [==============================] - 75s 4s/step - loss: 0.4078 - categorical_accuracy: 0.9351 - val_loss: 0.9580 - val_categorical_accuracy: 0.6500\n",
      "\n",
      "Epoch 00004: saving model to model_init_2021-02-0801_33_00.731094/model-00004-0.40266-0.93816-0.95805-0.65000.h5\n",
      "Epoch 5/12\n",
      "21/21 [==============================] - 77s 4s/step - loss: 0.2899 - categorical_accuracy: 0.9708 - val_loss: 0.9947 - val_categorical_accuracy: 0.6200\n",
      "\n",
      "Epoch 00005: saving model to model_init_2021-02-0801_33_00.731094/model-00005-0.28347-0.97436-0.99471-0.62000.h5\n",
      "Epoch 6/12\n",
      "21/21 [==============================] - 75s 4s/step - loss: 0.2102 - categorical_accuracy: 0.9777 - val_loss: 0.9008 - val_categorical_accuracy: 0.6600\n",
      "\n",
      "Epoch 00006: saving model to model_init_2021-02-0801_33_00.731094/model-00006-0.20258-0.98190-0.90081-0.66000.h5\n",
      "Epoch 7/12\n",
      "21/21 [==============================] - 76s 4s/step - loss: 0.1800 - categorical_accuracy: 0.9877 - val_loss: 0.8902 - val_categorical_accuracy: 0.6500\n",
      "\n",
      "Epoch 00007: saving model to model_init_2021-02-0801_33_00.731094/model-00007-0.16967-0.99095-0.89021-0.65000.h5\n",
      "Epoch 8/12\n",
      "21/21 [==============================] - 76s 4s/step - loss: 0.1225 - categorical_accuracy: 0.9959 - val_loss: 0.9530 - val_categorical_accuracy: 0.6400\n",
      "\n",
      "Epoch 00008: saving model to model_init_2021-02-0801_33_00.731094/model-00008-0.11880-0.99698-0.95296-0.64000.h5\n",
      "Epoch 9/12\n",
      "21/21 [==============================] - 74s 4s/step - loss: 0.1086 - categorical_accuracy: 0.9877 - val_loss: 0.9727 - val_categorical_accuracy: 0.6200\n",
      "\n",
      "Epoch 00009: saving model to model_init_2021-02-0801_33_00.731094/model-00009-0.10173-0.99095-0.97271-0.62000.h5\n",
      "Epoch 10/12\n",
      "21/21 [==============================] - 76s 4s/step - loss: 0.0929 - categorical_accuracy: 0.9877 - val_loss: 0.9336 - val_categorical_accuracy: 0.6400\n",
      "\n",
      "Epoch 00010: saving model to model_init_2021-02-0801_33_00.731094/model-00010-0.08567-0.99095-0.93360-0.64000.h5\n",
      "Epoch 11/12\n",
      "21/21 [==============================] - 76s 4s/step - loss: 0.0870 - categorical_accuracy: 0.9918 - val_loss: 0.9219 - val_categorical_accuracy: 0.6800\n",
      "\n",
      "Epoch 00011: saving model to model_init_2021-02-0801_33_00.731094/model-00011-0.08043-0.99397-0.92192-0.68000.h5\n",
      "Epoch 12/12\n",
      "21/21 [==============================] - 75s 4s/step - loss: 0.0639 - categorical_accuracy: 0.9959 - val_loss: 0.9698 - val_categorical_accuracy: 0.6500\n",
      "\n",
      "Epoch 00012: saving model to model_init_2021-02-0801_33_00.731094/model-00012-0.05983-0.99698-0.96982-0.65000.h5\n"
     ]
    }
   ],
   "source": [
    "model = model.fit_generator(train_generator, steps_per_epoch=steps_per_epoch, epochs=num_epochs, verbose=1, \n",
    "                    callbacks=callbacks_list, validation_data=val_generator, \n",
    "                    validation_steps=validation_steps, class_weight=None, workers=1, initial_epoch=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAEKCAYAAAD9xUlFAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4yLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvOIA7rQAAIABJREFUeJzt3Xd8FPed//HXRx0JiSqqRDHVgG2MBa5xiYl7r2A7l3OckNylXuLLz7nLxY5zyTlxEseFFHfH5zOxncQtuNu4x4BtXCgCATYIMIheJKH2+f0xq9UiBFrEjlYrvZ+Pxz60Mzuz+1khvu+Z78x8x9wdERERgLRkFyAiIh2HQkFERKIUCiIiEqVQEBGRKIWCiIhEKRRERCQqtFAws3vNbIOZfbyP183MbjOzMjP70MwmhVWLiIjEJ8w9hfuBM/bz+pnAqMhjBvD7EGsREZE4hBYK7v4asHk/i5wP/MkD/wB6mtnAsOoREZHWZSTxswcDq2OmyyPz1jVf0MxmEOxNkJeXd9TYsWPbpUCRjqrBnYYGqHePPHcanOjzYD4tvgaQlmakmZFmez5PN2vxtWA+pJkl+Zsnnzs4HvkJ7pHnDg3R+U3z9ve8wSPrN75v7PPYZSPz+3TPoiAns011v/vuuxvdvbC15ZIZCi39dbU45oa73wncCVBSUuLz588Psy6RUNTWN7CtqpatlbVsq6pha2UtWypr2V5Vy67ddeyqqaeypo5duyM/a+qp3F3Hzt11VMa8VlVbH9fnpQFZaUZedgZ5WenkRn4C0ffeVVPPrt111EXCoj7y2J/crHRyszLIy478zEoPPiNmuvGzumVlkJHWfkHS4E5tfQO19U5NXUPkeWS6voHauqbp3Xu83kBNvce8HrNOdL1gOpEau2oy0ozM9DQy042sjLTI87To86z04PWvnngIp48f0KbPMrNP41kumaFQDhTHTBcBa5NUi0jcauqCxj22Yd9aWRNt8LdW1bClspZtkedbI8937K7b7/tmpaeRm51OXlZG0PBGGtbivNw9GtrYBrl7drBsXrOfwfwMsjJa7yF2Dxq7yt317KoJAmhXJIiCQIoJqsjPnc2mt1bVsnZrVXSd2KBJpsx0izaw0cY1o9l05HluVmND3MI66WnR9WLXycxIIzs9jcyYdbKi6wafFTudFTudEVkmLY20dgzO1iQzFJ4Evmlms4CjgW3uvlfXkUiiuQdbidura9leVceO6lp2VNdFHrVsr65tauBjGvZgC7+Wnftp3NMMeuZm0bNbJj1yMynsns3ofvn0yM2kZ7cseuZmRh7BMj1zMynIySQvO74GPAxmRnZGOtkZ6fTKy0rY+9bUNVBZU0d7ZoNBzJa2YeruOmChhYKZPQycDPQ1s3LgeiATwN3/AMwGzgLKgErg6rBqkc5ld109O6rr2F61Z2O+o7ouaOirYxv6pvmxy9bW77+lSk+zaKPdMzeLAQU5jBmQH23Ye+Vm0iOmYe/ZLYueeZl0z8roUFt9yZSVkUZWRuJCRtpHaKHg7tNbed2Bb4T1+ZL6Vm7cxZzSDby6tIJPN1VGtuLrqKlrvV+3e3YGBTkZ5Odkkp+TQWH3bA7p252Cbk3z8nMyI8tkUJCTGTM/6JbRVqZ0RcnsPhLZQ1VNPW+v2MirpRXMiQQBwCF98xg3qICCnEwKujU24EHjnZ+dSUG3mOmcTLpnZ5CurXWRNlEoSNK4Oys27mJOaQVzSjfwzsrN1NQ1kJOZxnEj+nLNCcM5eXQ/hvTJTXapIl2GQkHaVWVNHW+VbWLO0qBbaPXmKgBGFObxxWOGcvKYQiYP601OZnqSKxXpmhQKEip3p2zDTl5dWsGc0grmrtxMTX0DuVnpHDeiLzNOHMHJowsp7q29AZGOQKEgCbdrdx1vlm1kztIKXi2tYM3WYG9gVL/ufOm4oZw8ph8lw3qRnaG9AZGORqEgB83dWbp+J68u3cCc0grmfbKZ2nonLyud40f25RunjOSkMYUM7tkt2aWKSCsUCtImO6prebNsE68u3cCrpRWs3VYNwNgB+Xz5+OGcNKaQkqG9k3ZBloi0jUJBDsjWyhp+9Xwps+aupq7Byc/O4PiRffn2qYWcNKaQgT20NyCSyhQKEpeGBuexd8u56dklbKuqZfqUYs49fBCThvYiM117AyKdhUJBWrVw7TZ+/MRC3v10CyVDe3Hj+RMYN6gg2WWJSAgUCrJP26tr+c3zS/nT25/QKzeLmy85nIsnFWlsH5FOTKEge3F3Hl+whp/9fQmbdu3mqqOHcu1pY+iR27abe4hI6lAoyB6Wrt/Bfz3+Me+s3MwRxT25758nc1hRj2SXJSLtRKEgAOzcXcetLy7lvjc/oXtOBj+/8DCmTS5WV5FIF6NQ6OLcnb9/tI6fPr2I9dt3M21yMT84Yyy9E3izFRFJHQqFLmx5xU6uf2Ihb5RtZPygAn5/1VFMGtIr2WWJSBIpFLqgypo67ni5jLteX0FOZjo3nj+eK48eqnsQiIhCoStxd55ftJ4bn1rEmq1VXDRpMD8881AK87OTXZqIdBAKhS7i0027uP7JhcwprWBM/3we+dqxTBneO9lliUgHo1Do5Kpr6/n9nOX8/tXlZKYZPzr7UL503DANTSEiLVIodGIvL1nPDU8uYtXmSs49YhA/OvtQ+hfkJLssEenAFAqd0OrNldz49CJeWLSeEYV5PPSVozl+ZN9klyUiKUCh0InsrqvnrtdWcMcrZRjG/ztjLNecMFz3NBCRuCkUOok3lm3kx098zIqNuzhj/AD+69xxutOZiBwwhUIn8OA/PuW/Hv+YoX1yuf/qyZw8pl+ySxKRFKVQSHF/evsTfvzEQk4d24+ZV04iJzM92SWJSApTKKSwxkCYemgQCNkZCgQROTgKhRT1wFufcP2TC5l6aH9+d+UkHUwWkYRQKKSg+99cyQ1PLeIL4/oz8woFgogkjlqTFHNfJBBOUyCISAi0p5BC7n1jJTc+vYjTx/fn9ukKBBFJPIVCirjnjZX8NBIId1wxSWMXiUgoFAop4O7XV/Dff1/MGeMHcPsVRyoQRCQ0al06uMZAOHOCAkFEwqc9hQ7srtdW8LPZizn7sIH8dtpEBYKIhC7UVsbMzjCzUjMrM7PrWnh9iJm9Ymbvm9mHZnZWmPWkkjtfW65AEJF2F1pLY2bpwEzgTGAcMN3MxjVb7EfAI+5+JDAN+F1Y9aSSP7y6nJ/PXsLZhw/kVgWCiLSjMFubKUCZu69w9xpgFnB+s2UcKIg87wGsDbGelPD7Ocu56ZklnHP4QG69fCIZCgQRaUdhHlMYDKyOmS4Hjm62zA3A82b2LSAPmNrSG5nZDGAGwJAhQxJeaEfxuzll/PLZUs49YhC3XHaEAkFE2l2YrY61MM+bTU8H7nf3IuAs4EEz26smd7/T3UvcvaSwsDCEUpNv5itBIJynQBCRJAqz5SkHimOmi9i7e+ga4BEAd38byAG63H0jZ75Sxs3PlXL+xEH8RoEgIkkUZuszDxhlZsPNLIvgQPKTzZZZBZwKYGaHEoRCRYg1dTh3vLyMm58r5YKJg/j1pQoEEUmu0Fogd68Dvgk8BywmOMtooZndaGbnRRb7PvBVM/sAeBj4Z3dv3sXUad3+0jJ+9fxSLjxyML++TAeVRST5Qr14zd1nA7ObzftxzPNFwPFh1tBR3fbSMn7zwlIuOnIwN196BOlpLR2CERFpX7qiOQlufXEZt7y4lIsmDebmSxQIItJxKBTa2W9fXMpvX1zGxZOK+OUlhysQRKRDUSi0o1teWMqtLy3jkqOK+MXFCgQR6XgUCu3A3bnlxWXc9tIyLj2qiJsUCCLSQSkUQubu3PLCUm57uYzLSoq46aLDSVMgiEgHpVAIkbvzmxeWcvvLZVxeUsz/XHSYAkFEOjSFQkjcnV8/v5Q7Xilj2uRifn6hAkFEOj5dLRWS381Zzh2vlDF9igJBRFKHQiEE26truePlMs4YP4CfXaBAEJHUoVAIwRML1lJVW8+/nDxCgSAiKUWhEIJZc1cxbmABhxf1SHYpIiIHRKGQYB+Vb2Ph2u1Mn1KMmfYSRCS1KBQS7P/mriInM43zjxyc7FJERA6YQiGBdu2u48kFazjn8EEU5GQmuxwRkQOmUEigpz5Yy66aeqZPKW59YRGRDkihkEAPz1vN6P7dmTSkV7JLERFpE4VCgixau50PVm9l2uQhOsAsIilLoZAgs+atIisjjYsm6QCziKQuhUICVNXU87f313DWhAH0zM1KdjkiIm2mAfES4O8frWNHdR3TpgxJdikiErZdm2DpM7BkNjTUQvHRMPQ4GDQJMnOSXd1BUygkwKy5qzikbx5HD++d7FJEJAxbPoUlfw8eq94Cb4AexZCZC8ueD5ZJz4JBR8KQY4NH8RTITb02QaFwkJat38H8T7fwH2eN1QFmkc7CHdYvhCVPB4/PPgrm9xsPn7sWDj0HBhwOZlC5GVb9A1a9Hfx8eya8+dvI8uNgyDEw5LjgZ8+Of7q6QuEgPTx3NZnpxsWTipJdiogcjIZ6WP1OZI/gadjyCWBB99Bp/w1jzoI+I/ZeL7c3jD0reADUVMLa94KQ+PRt+PBRmH9v8FpBEQw9NhIUx0LhoZDWsQ7tKhQOQnVtPX99v5zTxg+gT/fs5BRRVwPrPoC170NGFuT22fPRrRekpSentlS0aTmseAXSMvS77Apqq2Hlq7D4KSh9Bio3Bt1Aw0+CE/4NRp8J+f0P7D2zcmHYCcEDgrBZ/3HT3sTK1+GjR4PXcnpA8TFBSAw9Luh+ykhSWxKhUDgIzy38jK2VtUyf3I4HmKu3Q/m8pl3V8vlQV7WfFQy69Yw0bH0jP3vv3eDFzs/pEewWdwXusG5BsHW4+GmoWLyfhWN/l609uuDvMlVUbYVlL8CSp2DZi1C7C7LyYfRpMPYcGDkVcgoS93lp6TDwiOBx9NeCv7ktn0RC4q3g57LngmXTs2HwpD2PS3Trmbha4mDu3q4feLBKSkp8/vz5yS4DgGl3vs3ardXMufbk8O6bsOOzpgD49K1gi8MbwNKCPs0hxwa7o4NLguUrN8U8Nkd+btx73q6NwZkTLdlrK7mFEMkfGGzVZHcP53uHqb4OPn2z6cDh9vLg9zn0+KBRGH16sLXW+DvbtTHmd7mP3299TcuflZYB3SK/v7y++wjk2Hl9gy1NSazt66A08u+98vXgb797/6BLaOw5MPxzyd1C37Ux5rjE28Hef0MdYNB/fFN30/AToXu/Nn2Emb3r7iWtLqdQaJsVFTv5/K9f5d9PH8M3ThmZmDd1h01lQePf+AeyZWXwWmYuFJVEtiCOgaLJkJ1/cJ9VszOm4Wve4DVv+DZB1eYgkBpZZAtoSEwfaffCg/sdhKWmEpa/HPQVL30WqrZARg6MOBXGng2jz4C8Pm177+a/y+YBsmtj67/LWBndWg7jfYVKt95B16HsaeOy4N978dOwJtJm9D4kCIFDzw02pDpYf35UzS5Y825wTGLV20HvQM1OOOtXMOWrbXrLeENB3Udt9Od5q0lPMy496iAOMNfXwroPm7YOVr0dNBgQ/GcfcixM/krwc+DhkJ7AkVfNglDJzodew+Jbp6EeqrcFNUZ3f9+GeXfDP2YGy/QZ2bTrO+SY4D9hsrpPKjfD0ueChqHspaCbLadnEACHngMjPg9ZeQf/OW36XTZA9dZIUGzcfxhv+SSY3r1t3++X3aOFwIjdQ4mZXzAoMd+7o2n8/7Tk6WCPYGNpMH/gRPj8j4IwKBybGt15WXnBXsHwE4Pp+jpY/xEUhD9igkKhDWrqGnjs3XKmHtqPfgUHcLHK7p0xxwPeDo4H1FYGr/UaBqNOb9ri7juq4/3xpqVHGpreQX2jvhDMr9sd7O427uEsfgrefzB4rXv/pu805FjoPwHSQ/yz27oaSmcHDcMnb4LXB/+RJn0x2CMYenxiw7Wt0tKafpfEuadZX7ufbqyYYNn5GWxYFDxv/PtqrqAo+DcsHBP87Ds6eHTv3/H+7pqr3h7sBWxcuudj84qgy8XSYdjxMPma4N+8Ryc4MzA9I+iubQcKhTZ4YdF6Nu2qaf0K5p0b9jwe8NlHQSNlaUHjeOQXg+MBxcdAwcD2KT4MGdnBAbHiKcF0Q0OwldZ4St6qf8CiJ4LXsroHyzXuSQwuObg+dHeoWBJ0ESx5OjhoDMEW4QnfDbYOBx3Z8Ru6eKRnBmfCHMjZMDWVQVdVbDfW1lWRRrUU3v/foFuiUXaPmJCICYvew9s3TN1h+9q9G/6Ny2DHuqbl0jKCvdG+o4N/637jYOSpKXnRWEehYwpt8MV73mFFxS5e+8EppLd0gLlyMzz25eDURgj6rosmR7aYj4GiKYk9uyEVbCvf8wKf9QsBD/5TD5zYdEpe8TGt9+03NAR7XI0XFm1eEcwvmhw0DGPPgb4JOs7T2bkHjezGpVDRvPFd27RcbOMbDYsxwe855yDuRV63O/j3i/3citLg2NoeYVXQFFCFo5ue9xrWMfb8UoAONIdk9eZKPvfLV/i3qaP5ztRRey+weQU8dGnQjXHitXDIKcHBWB0I3FPVFlg9r+mUvDXvNp3B03d0U3fT0GOh59DgtZWvB6cRLpkNuzZAWmbQ5zr27OCRPyC536mzqd4Om5bt2VVTsRQ2L4+cGRPRfUBMV1RMaBQMbtpDq9rS1OA3Nv4blwbHS7y+6b16FLewpzImOOOmM+ztJZEONIdk1rxVpBlcNrmFfsrVc+HhacHW15eeDLZ+pWXdegXnhY8+LZiurQ4uwGvck1j0OLz3QPBa/sDgeEzNjqD7aeTU4OyRUV84uK1U2b+cAhh8VPCIVV8bjAW0cWnQBdXYwH/46J4HwzPzgi35XRtgV0XT/PSs4ISEARNgwsVNAdBnZGqe4tzJxB0KZtYNGOLupSHW06HV1jfw6PxyThnTj4E9uu354sK/wV+/Bj0Gw5WPtXw5vOxbZk6wVzD02GC6oSG4kKwxJDJzg72B4Sd1ipEoU1p6ZtBt1HckcFbTfPeg8Y/dG9iyMrgYK7brp+dQXRnegcUVCmZ2LvArIAsYbmYTgRvd/bxW1jsDuBVIB+5295taWOYy4AbAgQ/c/YoD+gbt6OUlG9iwY/eeB5jd4c1b4cXrg/7waf/X9vPdpUlaWnDRTv/xwWm50vGZBd083fsFF4NJSop3T+EGYAowB8DdF5jZsP2tYGbpwEzgC0A5MM/MnnT3RTHLjAJ+CBzv7lvMrG2X6rWTWXNX0b8gm1PGRC7Qqq+D2d+Hd+8PdoPP/522YkUkpcV7OV+du+/nypkWTQHK3H2Fu9cAs4Dzmy3zVWCmu28BcPcNB/gZ7WbN1ipeXVrBZSXFZKSnBQfh/u+yIBBO+B5cdLcCQURSXrx7Ch+b2RVAemTr/tvAW62sMxhYHTNdDhzdbJnRAGb2JkEX0w3u/mzzNzKzGcAMgCFDknN3s0fmrcaBy0qKYduaIBA2LIZzb4OjvpSUmkREEi3ePYVvAeOB3cDDwHbgu62s09L5Y83Pf80ARgEnA9OBu81sryEB3f1Ody9x95LCwvYfW6e+wXlk/mo+N6qQ4t1lcPepwQVAVz2mQBCRTiWuPQV3rwT+M/KIVzkQe5uhImBtC8v8w91rgZVmVkoQEvMO4HNC9+rSDazbVs3tR1XAvd8LTqf88rPBQVARkU4k3rOPnmLvrfxtwHzgj+5e3cJq84BRZjYcWANMA5qfWfQ4wR7C/WbWl6A7aUX85bePh+eu5uu5r3DU2/fAgMNg+p9Te1gKEZF9iLf7aAWwE7gr8tgOrCdoxO9qaQV3rwO+CTwHLAYecfeFZnajmTWeyvocsMnMFgGvAP/u7pva+mXCsH5bJUcv+w3XNdyFjToN/nm2AkFEOq24hrkws9fc/cSW5pnZQndvt36Udh3moqaS5XdewYiNr7D9sKspuPDXuuhGRFJSvMNcxLunUGhm0dN+Is/7Rib3ccupFLezAn/gXIZvnMOfenyNgotuUSCISKcX7ymp3wfeMLPlBGcVDQf+1czygAfCKi5pKkrhoUtp2LGef635Lmd/foYG4xKRLiHes49mR65PGEsQCktiDi7/NqzikmLl6/DnKyE9i18N+jVz1/TjtvEHMH69iEgKO5AblI4CxgCHA5eZ2T+FU1ISfTALHrwQug9g0/RnuGt5by6eVER2hrqNRKRriPeU1OsJLjAbB8wGzgTeAP4UWmXtyR1e/QXM+Z9gfP7LHuTRuZuoa3CmTSlufX0RkU4i3j2FS4BTgc/c/WrgCCA7tKraU10NPP4vQSAccQVc+Rc8pwez5q5iyrDejOyXn+wKRUTaTbwHmqvcvcHM6sysANgAHBJiXe2jagv8+Yvwyetwyn/Cif8OZry9fCOfbKrk26e2cGc1EZFOLN5QmB8Zk+gu4F2CC9nmhlZVe9jySXDbzM0r4cI74YjLoy/NmruagpwMzjpMF6mJSNcS79lH/xp5+gczexYocPcPwysrZOXzg9tm1tfCPz0Ow06IvrRlVw3PfvwZVxw9hJxMHWAWka4lrmMKZvZS43N3/8TdP4ydl1IWPQn3nw1ZeXDNC3sEAsBf3iunpr5BB5hFpEva756CmeUAuUBfM+tF03DYBcCgkGtLLHd4eyY8/yMoKoHpsyCvb7NFnFnzVnPkkJ6MHVCQpEJFRJKnte6jrxHcN2EQwbGExlDYTnCrzdTxxi3w0k/g0PPgojshs9tei8z/dAtlG3byy4sPT0KBIiLJt99QcPdbgVvN7Fvufns71RSOCRdBfQ2c+IPgpvAteHjuKrpnZ3DOETrALCJdU7wHmm83s+OAYbHruHvqXLzWaxicfN0+X95WWcvfP1zHJUcVkZsV70lZIiKdS7xXND8IjAAWAPWR2U5nuaIZeHzBGnbXNTB9SnLuAS0i0hHEu0lcAozzeG6+kILcnYfnruKwwT2YMLhHsssREUmaeIe5+BgYEGYhybRg9VaWfLZDp6GKSJcX755CX2CRmc0FdjfOdPfz9r1K6pg1dzXdMtM574jUOstWRCTR4g2FG8IsIpl2VNfy1IdrOe+IQeTnZCa7HBGRpIr37KNXzWwoMMrdXzSzXKBTjAHx5AdrqaypV9eRiAjxD3PxVeAx4I+RWYOBx8Mqqj3NmruasQPymVjcM9mliIgkXbwHmr8BHE9wJTPuvgzoF1ZR7eXjNdv4aM02pk8ZgukezCIicYfCbnevaZwwswyC6xRS2sNzV5GdkcYFEwcnuxQRkQ4h3lB41cz+A+hmZl8AHgWeCq+s8FXW1PHEgrWcffhAeuTqALOICMQfCtcBFcBHBIPkzQZ+FFZR7eHpD9axc3edrmAWEYkR7ymp3YB73f0uADNLj8yrDKuwsD08bxUj+3WnZGivZJciItJhxLun8BJBCDTqBryY+HLax5LPtvP+qq1Mm1ysA8wiIjHiDYUcd9/ZOBF5nhtOSeGbNXc1WelpXDSpKNmliIh0KPGGwi4zm9Q4YWZHAVXhlBSu6tp6/vpeOWdMGEDvvKxklyMi0qHEe0zhO8CjZrY2Mj0QuDycksI1+6N1bK+u0xXMIiItaDUUzCwNyALGAmMIbsm5xN1rQ64tFLPmrmZYn1yOPaRPsksREelwWu0+cvcG4NfuXuvuH7v7R6kaCGUbdjL3k81M0xXMIiItiveYwvNmdrGleEs6a+4qMtKMi3WAWUSkRfEeU/gekAfUm1kVQReSu3tBaJUl2O66ev7yXjmnje9PYX52sssREemQ4h06Oz/sQsL23ML1bKmsZdpkXcEsIrIv8Q6dbWZ2lZn9V2S62MymxLHeGWZWamZlZnbdfpa7xMzczEriL/3AZGekccqYQk4Y2TesjxARSXnxHlP4HXAscEVkeicwc38rRIbCmAmcCYwDppvZuBaWywe+DbwTZy1tcvr4Adx39RTS0lL6sIiISKjiDYWj3f0bQDWAu28hOE11f6YAZe6+IjLs9izg/BaW+ynwy8b3FhGR5Ik3FGojW/4OYGaFQEMr6wwGVsdMl0fmRZnZkUCxuz+9vzcysxlmNt/M5ldUVMRZsoiIHKh4Q+E24G9APzP7GfAG8PNW1mmpnyZ6Y57IRXG3AN9v7cPd/U53L3H3ksLCwjhLFhGRAxXv2UcPmdm7wKkEjf0F7r64ldXKgdixJIqAtTHT+cAEYE7k8ocBwJNmdp67z4+zfhERSaD9hoKZ5QBfB0YS3GDnj+5eF+d7zwNGmdlwYA0wjaYD1bj7NiB6KpCZzQGuVSCIiCRPa91HDwAlBIFwJvCreN84Eh7fBJ4DFgOPuPtCM7vRzM5rY70iIhKi1rqPxrn7YQBmdg8w90De3N1nE9y6M3bej/ex7MkH8t4iIpJ4re0pRAe+O4BuIxERSVGt7SkcYWbbI88N6BaZTrmxj0REpHX7DQV3T2+vQkREJPnivU5BRES6AIWCiIhEKRRERCRKoSAiIlEKBRERiVIoiIhIlEJBRESiFAoiIhKlUBARkSiFgoiIRCkUREQkSqEgIiJRCgUREYlSKIiISJRCQUREohQKIiISpVAQEZEohYKIiEQpFEREJEqhICIiUQoFERGJUiiIiEiUQkFERKIUCiIiEqVQEBGRKIWCiIhEKRRERCRKoSAiIlEKBRERiVIoiIhIlEJBRESiQg0FMzvDzErNrMzMrmvh9e+Z2SIz+9DMXjKzoWHWIyIi+xdaKJhZOjATOBMYB0w3s3HNFnsfKHH3w4HHgF+GVY+IiLQuzD2FKUCZu69w9xpgFnB+7ALu/oq7V0Ym/wEUhViPiIi0IsxQGAysjpkuj8zbl2uAZ1p6wcxmmNl8M5tfUVGRwBJFRCRWmKFgLczzFhc0uwooAW5u6XV3v9PdS9y9pLCwMIEliohIrIwQ37scKI6ZLgLWNl/IzKYC/wmc5O67Q6xHRERaEeaewjxglJkNN7MsYBrwZOwCZnYk8EfgPHffEGItIiISh9BCwd3rgG8CzwGLgUfcfaGZ3Whm50UWuxnoDjxqZgvM7Ml9vJ2IiLSDMLuPcPfZwOxm834c83xqmJ8vIiIHJtRQEBFJptraWsrLy6murk52Ke0mJyeHoqIiMjMz27S+QkFEOq3y8nLy8/MZNmy8eCk6AAAKKUlEQVQYZi2dENm5uDubNm2ivLyc4cOHt+k9NPaRiHRa1dXV9OnTp0sEAoCZ0adPn4PaM1IoiEin1lUCodHBfl+FgoiIRCkURERCsmnTJiZOnMjEiRMZMGAAgwcPjk7X1NTE9R5XX301paWlIVfaRAeaRURC0qdPHxYsWADADTfcQPfu3bn22mv3WMbdcXfS0lreRr/vvvtCrzOWQkFEuoSfPLWQRWu3J/Q9xw0q4Ppzxx/wemVlZVxwwQWccMIJvPPOOzz99NP85Cc/4b333qOqqorLL7+cH/84uKTrhBNO4I477mDChAn07duXr3/96zzzzDPk5ubyxBNP0K9fv4R+J3UfiYgkwaJFi7jmmmt4//33GTx4MDfddBPz58/ngw8+4IUXXmDRokV7rbNt2zZOOukkPvjgA4499ljuvffehNelPQUR6RLaskUfphEjRjB58uTo9MMPP8w999xDXV0da9euZdGiRYwbt+d9ybp168aZZ54JwFFHHcXrr7+e8LoUCiIiSZCXlxd9vmzZMm699Vbmzp1Lz549ueqqq1q81iArKyv6PD09nbq6uoTXpe4jEZEk2759O/n5+RQUFLBu3Tqee+65pNWiPQURkSSbNGkS48aNY8KECRxyyCEcf/zxSavF3Fu8GVqHVVJS4vPnz092GSKSAhYvXsyhhx6a7DLaXUvf28zedfeS1tZV95GIiEQpFEREJEqhICIiUQoFERGJUiiIiEiUQkFERKIUCiIiIUnE0NkA9957L5999lmIlTbRxWsiIiGJZ+jseNx7771MmjSJAQMGJLrEvSgURKRreOY6+OyjxL7ngMPgzJvatOoDDzzAzJkzqamp4bjjjuOOO+6goaGBq6++mgULFuDuzJgxg/79+7NgwQIuv/xyunXrxty5c/cYAynRFAoiIu3s448/5m9/+xtvvfUWGRkZzJgxg1mzZjFixAg2btzIRx8F4bV161Z69uzJ7bffzh133MHEiRNDr02hICJdQxu36MPw4osvMm/ePEpKglEnqqqqKC4u5vTTT6e0tJTvfOc7nHXWWZx22mntXptCQUSknbk7X/7yl/npT3+612sffvghzzzzDLfddht/+ctfuPPOO9u1Np19JCLSzqZOncojjzzCxo0bgeAspVWrVlFRUYG7c+mll0ZvzwmQn5/Pjh072qU27SmIiLSzww47jOuvv56pU6fS0NBAZmYmf/jDH0hPT+eaa67B3TEzfvGLXwBw9dVX85WvfKVdDjRr6GwR6bQ0dHYTDZ0tIiIHTKEgIiJRCgUR6dRSrYv8YB3s91UoiEinlZOTw6ZNm7pMMLg7mzZtIicnp83vobOPRKTTKioqory8nIqKimSX0m5ycnIoKipq8/oKBRHptDIzMxk+fHiyy0gpoXYfmdkZZlZqZmVmdl0Lr2eb2Z8jr79jZsPCrEdERPYvtFAws3RgJnAmMA6Ybmbjmi12DbDF3UcCtwC/CKseERFpXZh7ClOAMndf4e41wCzg/GbLnA88EHn+GHCqmVmINYmIyH6EeUxhMLA6ZrocOHpfy7h7nZltA/oAG2MXMrMZwIzI5E4zK21jTX2bv3cn05m/n75b6urM3y+VvtvQeBYKMxRa2uJvfl5YPMvg7ncCBz1UoJnNj+cy71TVmb+fvlvq6szfrzN+tzC7j8qB4pjpImDtvpYxswygB7A5xJpERGQ/wgyFecAoMxtuZlnANODJZss8CXwp8vwS4GXvKleZiIh0QKF1H0WOEXwTeA5IB+5194VmdiMw392fBO4BHjSzMoI9hGlh1RPRvneraH+d+fvpu6Wuzvz9Ot13S7mhs0VEJDwa+0hERKIUCiIiEtVlQqG1ITdSlZkVm9krZrbYzBaa2XeSXVOimVm6mb1vZk8nu5ZEM7OeZvaYmS2J/Bsem+yaEsXM/i3yN/mxmT1sZm0furMDMLN7zWyDmX0cM6+3mb1gZssiP3sls8ZE6BKhEOeQG6mqDvi+ux8KHAN8oxN9t0bfARYnu4iQ3Ao86+5jgSPoJN/TzAYD3wZK3H0CwckmYZ9IErb7gTOazbsOeMndRwEvRaZTWpcIBeIbciMlufs6d38v8nwHQaMyOLlVJY6ZFQFnA3cnu5ZEM7MC4ESCs/Bw9xp335rcqhIqA+gWuQYpl72vU0op7v4ae19HFTtUzwPABe1aVAi6Sii0NORGp2k4G0VGmT0SeCe5lSTUb4EfAA3JLiQEhwAVwH2R7rG7zSwv2UUlgruvAX4FrALWAdvc/fnkVhWK/u6+DoINNKBfkus5aF0lFOIaTiOVmVl34C/Ad919e7LrSQQzOwfY4O7vJruWkGQAk4Dfu/uRwC46QfcDQKRv/XxgODAIyDOzq5JblcSjq4RCPENupCwzyyQIhIfc/a/JrieBjgfOM7NPCLr8Pm9m/5vckhKqHCh398Y9u8cIQqIzmAqsdPcKd68F/gocl+SawrDezAYCRH5uSHI9B62rhEI8Q26kpMhQ4/cAi939N8muJ5Hc/YfuXuTuwwj+zV52906ztenunwGrzWxMZNapwKIklpRIq4BjzCw38jd6Kp3kIHozsUP1fAl4Iom1JESXuB3nvobcSHJZiXI88EXgIzNbEJn3H+4+O4k1Sfy+BTwU2VhZAVyd5HoSwt3fMbPHgPcIzpB7nxQfEsLMHgZOBvqaWTlwPXAT8IiZXUMQhJcmr8LE0DAXIiIS1VW6j0REJA4KBRERiVIoiIhIlEJBRESiFAoiIhKlUJAuzczqzWxBzOO6yPw5kVF1PzCzNxuvJTCzLDP7rZktj4yM+URkfKbG9xtgZrMiry8ys9lmNtrMhsWOrhlZ9gYzuzby/BgzeydSw2Izu6Edfw0iUV3iOgWR/ahy94n7eO1Kd59vZjOAm4HzgJ8D+cBod683s6uBv5rZ0ZF1/gY84O7TAMxsItCfPcfeaskDwGXu/kFkVN8xrSwvEgqFgkjrXgO+a2a5BBeXDXf3egB3v8/Mvgx8nmA8rVp3/0Pjiu6+AKKDFe5PP4KB44i8d2e5sllSjEJBurpuMVeCA/yPu/+52TLnAh8BI4FVLQw4OB8YH3m+v8H7RjT7rAEEI4kC3AKUmtkc4FmCvY3q+L+GSGIoFKSr21/30UNmVgV8QjAcRW9aHl3XIvNbGo031vLYz4o9buDuN5rZQ8BpwBXAdIIhFUTalUJBZN+udPf5jRNmthkYamb5kRsaNZoEPBV5fklbP8zdlwO/N7O7gAoz6+Pum9r6fiJtobOPROLk7rsIDgj/JnIwGDP7J4K7ir0ceWSb2Vcb1zGzyWZ2UmvvbWZnR0YTBRgF1AOd6S5skiIUCtLVdWt2SupNrSz/Q6AaWGpmywhGxbzQI4ALgS9ETkldCNxAfPfu+CLBMYUFwIMEeyn1bf1SIm2lUVJFRCRKewoiIhKlUBARkSiFgoiIRCkUREQkSqEgIiJRCgUREYlSKIiISNT/B2dN5aW/0044AAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "### Accuracy graph\n",
    "plt.plot(model.history[\"categorical_accuracy\"])\n",
    "plt.plot(model.history[\"val_categorical_accuracy\"])\n",
    "plt.xlabel(\"EPOCHS\")\n",
    "plt.ylabel(\"Percentage\")\n",
    "plt.ylim(0.0,1.0)\n",
    "plt.legend([\"Train\",\"Test\"],loc=\"lower right\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAEKCAYAAAD9xUlFAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4yLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvOIA7rQAAIABJREFUeJzt3Xl8VfWd//HXJ/tKAgESIGyisgsiohStuC+1daZK1VantVpqa61obcfpdKbWzmPUx0zdW5dWrNqOy8+lVVuEWndFJFBEliJIEaIsIawhZP/8/jgnl0tISCD35mZ5Px+P+zjn3nPuyeeGcN7ne77nfK+5OyIiIgBJiS5AREQ6D4WCiIhEKBRERCRCoSAiIhEKBRERiVAoiIhIRNxCwcwyzOx9M/vAzJab2c+aWSfdzJ4yszVmtsDMhsWrHhERaV08WwrVwGnuPgGYCJxjZic2WedKYLu7HwncCdwex3pERKQVcQsFD1SET1PDR9M75S4AHg3nnwFONzOLV00iInJwKfHcuJklA4uAI4FfuvuCJqsMAjYAuHudme0ECoCtTbYzE5gJkJ2dfdyoUaPiWbaISLezaNGire7er7X14hoK7l4PTDSzfOB5Mxvn7suiVmmuVXDAuBvu/hDwEMDkyZO9pKQkLvWKiHRXZvZJW9brkKuP3H0H8DpwTpNFpcBgADNLAfKAbR1Rk4iIHCieVx/1C1sImFkmcAbw9yarvQB8PZy/CHjVNUKfiEjCxPP00QDg0bBfIQl42t1fMrNbgBJ3fwF4GHjczNYQtBAuiWM9IiLSiriFgrsvBY5t5vX/jJqvAmbEqwYR6X5qa2spLS2lqqoq0aV0ShkZGRQXF5OamnpY749rR7OISKyVlpaSm5vLsGHD0BXs+3N3ysvLKS0tZfjw4Ye1DQ1zISJdSlVVFQUFBQqEZpgZBQUF7WpFKRREpMtRILSsvb8bhYKIiEQoFEREDkF5eTkTJ05k4sSJFBUVMWjQoMjzmpqaNm3jiiuuYNWqVXGu9PCoo1lE5BAUFBSwZMkSAG6++WZycnK48cYb91vH3XF3kpKaP+5+5JFH4l7n4VJLQUQkBtasWcO4ceO4+uqrmTRpEhs3bmTmzJlMnjyZsWPHcsstt0TWPemkk1iyZAl1dXXk5+dz0003MWHCBKZOncqWLVsS+CnUUhCRLuxnLy5nxWe7YrrNMQN78dMvjj2s965YsYJHHnmEBx54AIDbbruNPn36UFdXx6mnnspFF13EmDFj9nvPzp07OeWUU7jtttu44YYbmD17NjfddFO7P8fhUktBRCRGRowYwfHHHx95/sQTTzBp0iQmTZrEypUrWbFixQHvyczM5NxzzwXguOOOY926dR1VbrPUUhCRLutwj+jjJTs7OzK/evVq7r77bt5//33y8/O57LLLmr1/IC0tLTKfnJxMXV1dh9TaErUURETiYNeuXeTm5tKrVy82btzI3LlzE11Sm6ilICISB5MmTWLMmDGMGzeOI444gmnTpiW6pDaxrjZStb5kR6RnW7lyJaNHj050GZ1ac78jM1vk7pNbe69OH4mISIRCQUREIhQKIiISoVAQEZEIhYKIiEQoFEREJEKhICJyCGIxdDbA7Nmz2bRpU+R5ZxlOWzeviYgcgrYMnd0Ws2fPZtKkSRQVFQGdZzhttRRERGLk0UcfZcqUKUycOJHvfve7NDQ0UFdXx+WXX8748eMZN24c99xzD0899RRLlizh4osvjrQw2jKc9urVqznhhBOYMmUK//Ef/0F+fn7MP4NaCiLSdc25CTZ9GNttFo2Hc2875LctW7aM559/nnfffZeUlBRmzpzJk08+yYgRI9i6dSsffhjUuWPHDvLz87n33nu57777mDhx4gHbamk47WuvvZYbb7yRGTNmcN9997X7ozZHLQURkRh45ZVXWLhwIZMnT2bixIm88cYbfPzxxxx55JGsWrWK6667jrlz55KXl9fqtloaTnvBggVceOGFAHz1q1+Ny+dQS0FEuq7DOKKPF3fnm9/8Jj//+c8PWLZ06VLmzJnDPffcw7PPPstDDz100G0lcjhttRRERGLgjDPO4Omnn2br1q1AcJXS+vXrKSsrw92ZMWMGP/vZz1i8eDEAubm57N69+5B+xpQpU3j++ecBePLJJ2P7AUJqKYiIxMD48eP56U9/yhlnnEFDQwOpqak88MADJCcnc+WVV+LumBm33347EFyCetVVV5GZmcn777/fpp9xzz33cPnll3P77bdz3nnntelU1KGK29DZZjYYeAwoAhqAh9z97ibrTAf+CPwjfOk5d7+Fg9DQ2SI9W08eOnvPnj1kZWVhZvzud7/j+eef59lnnz1gvfYMnR3PlkId8AN3X2xmucAiM/uLuzf9ktK33P38ONYhItItLFy4kFmzZtHQ0EDv3r3jcm9D3ELB3TcCG8P53Wa2EhgEHPjN1SIi0qrp06dHbpyLlw7paDazYcCxwIJmFk81sw/MbI6Zda5v4RaRTqmrfWNkR2rv7ybuoWBmOcCzwCx339Vk8WJgqLtPAO4F/tDCNmaaWYmZlZSVlcW3YBHp1DIyMigvL1cwNMPdKS8vJyMj47C3EdfvaDazVOAlYK6739GG9dcBk919a0vrqKNZpGerra2ltLSUqqqqRJfSKWVkZFBcXExqaup+rye8o9nMDHgYWNlSIJhZEbDZ3d3MphC0XMrjVZOIdH2pqakMHz480WV0W/G8+mgacDnwoZk19oz8GBgC4O4PABcB3zGzOmAvcImrTSgikjDxvProbcBaWec+ID6jOomIyCHTMBciIhKhUBARkQiFgoiIRCgUREQkQqEgIiIRCgUREYlQKIiISIRCQUREIhQKIiISoVAQEZEIhYKIiEQoFEREJEKhICIiEQoFERGJUCiIiEiEQkFERCIUCiIiEqFQEBGRCIWCiIhEKBRERCRCoSAiIhEKBRERiVAoiIhIhEJBREQiFAoiIhKhUBARkQiFgoiIRCgUREQkIm6hYGaDzew1M1tpZsvN7Lpm1jEzu8fM1pjZUjObFK96RESkdSlx3HYd8AN3X2xmucAiM/uLu6+IWudc4KjwcQJwfzgVEZEEiFtLwd03uvvicH43sBIY1GS1C4DHPPAekG9mA+JVk4iIHFyH9CmY2TDgWGBBk0WDgA1Rz0s5MDgws5lmVmJmJWVlZfEqU0Skx4t7KJhZDvAsMMvddzVd3Mxb/IAX3B9y98nuPrlfv37xKFNERIhzKJhZKkEg/N7dn2tmlVJgcNTzYuCzeNYkIiIti+fVRwY8DKx09ztaWO0F4F/Cq5BOBHa6+8Z41SQiIgcXz6uPpgGXAx+a2ZLwtR8DQwDc/QHgz8B5wBqgErgijvWIiEgr4hYK7v42zfcZRK/jwDXxqkFERA6N7mgWEZEIhYKIiEQoFEREJEKhICIiEQoFERGJUCiIiEiEQkFERCIUCiIiEqFQEBGRCIWCiIhEKBRERCRCoSAiIhEKBRERiVAoiIhIhEJBREQiFAoiIhKhUBARkQiFgoiIRCgUREQkQqEgIiIRhxQKZpZqZseaWf94FSQiIolz0FAwswfMbGw4nwd8ADwG/M3MLu2A+kREpAO11lI42d2Xh/NXAB+5+3jgOOBHca1MREQ6XGuhUBM1fybwBwB33xS3ikREJGFaC4UdZna+mR0LTANeBjCzFCAz3sWJiEjHSmll+beBe4AiYFZUC+F04E/xLExERDreQUPB3T8Czmnm9bnA3HgVJSIiidHa1UffMrOjwnkzs0fMbJeZLQ1PKR3svbPNbIuZLWth+XQz22lmS8LHfx7+xxARkVhorU/hOmBdOH8pcAwwHLiB4LTSwfyWZloZTbzl7hPDxy2trCsiInHWWijUuXttOH8+8Ji7l7v7K0D2wd7o7m8C22JQo4iIdJDWQqHBzAaYWQZB5/IrUcticfXRVDP7wMzmNN4k1xwzm2lmJWZWUlZWFoMfKyIizWktFP4TKCE4hfRC441sZnYKsLadP3sxMNTdJwD3Et4D0Rx3f8jdJ7v75H79+rXzx4qISEsOGgru/hIwFBjt7t+KWlQCXNyeH+zuu9y9Ipz/M5BqZn3bs00REWmf1u5TAOgDXBOe3nFgBfArd9/cnh9sZkXAZnd3M5tCEFDl7dmmiIi0T2uXpE4DFoZPHwN+F84vCJcd7L1PAPOBkWZWamZXmtnVZnZ1uMpFwDIz+4DgSqZL3N0P94OIiEj72cH2w2b2HvAdd/9bk9cnAg+6+wlxru8AkydP9pKSksN6b2VNHVlpbWkciYh0L2a2yN0nt7Zeax3NvZoGAoC7LwFyD7e4RJi3fBMn3/4an5TvSXQpIiKdVmuhYGbWu5kX+7ThvZ3KuEF51NY3cP1TS6irb0h0OSIinVJrO/Y7gXlmdoqZ5YaP6cAc4K64VxdDA/Mz+fk/jWPx+h3c//rHiS5HRKRTam1AvIfM7DPg50D01Uf/5e4vdkB9MXXBxEG8snILd/91NaeM7McxxfmJLklEpFNp9RSQu7/k7p939wJ37xvOv2hmszqiwFj7rwvG0S83nVlPLWFvTX2iyxER6VTa0y9wQ8yq6EB5Wan874wJrC3bw61zVia6HBGRTqU9oWAxq6KDTTuyL1eeNJzH5n/C66u2JLocEZFOoz2h0KVvNPvh2SM5ujCHHz6zlG17alp/g4hID9DaHc27wy/VafrYDQzsoBrjIiM1mbsuPpYdlTX8+LkP0c3UIiKtD4iX6+69mnnkunuXvzV4zMBe/OCskby8fBPPLv400eWIiCRcl7oBLR6+dfIRTBneh5tfWM6GbZWJLkdEJKF6Tig01EPFgV/Qk5xk/GLGBABueHoJ9Q06jSQiPVfPCYWP5sKdY+AP34WNS/dbNLhPFj/70lgWrtvOQ2+297uDRES6rp4TCv1Hw6Svw/Ln4cGT4ZEvwMqXghYE8OVJgzhvfBF3/GUVyz7dmeBiRUQS46BDZ3dG7Rk6G4C922Hx4/D+r2HnesgfCid8G469jO31mZx915vkZaby4rUnkZGaHLvCRUQSKFZDZ3c/mb1h2vfh+3+DrzwGvQbC3B/DHWPo/cZPuO/sPFZvqeD2l/+e6EpFRDpczwuFRskpMOYC+ObLMPN1GHU+lMxmyktnMrf/L1k1/yXe/ujAjmkRke6s550+Opjdm6BkNr7wYaxyK2tsCAPPmkXW5K9CamZ8fqaISAfQ6aPDkVsEp/4Yu345paf8gpp6I2vuDfgdY+CvP4ddGxNdoYhIXCkUmpOaQfGpV/Ha9Ge5pOYnbMw7Ft76Bdw1Dp69CkoXJbpCEZG4UCgcxLdPGUHt4GmcvenbbLriPZgyE1a9DL85DX5zJix7DurrEl2miEjMKBQOIiU5iTu/MpGGBuf6uTtoOOu/4YYVcM7tsKcMnrkC7j4G3r4TKrclulwRkXZTKLRiSEEWP/3iWOavLefht/8BGb3gxKvh2kVw6ZNQMAJeuRnuGAMvzoKyVYkuWUTksCkU2mDG5GLOGlPI/8xdxcqNu4IXk5Jh5Lnw9Rfh6ndg/EWw5P/gl1Pg8S/D6r9AQ0NiCxcROUS6JLWNyiuqOfuut+ibk8YfvzeN9JRm7nbesxVKHoGFv4GKTZA7EIZ+DoZOhSFTod9oSFIOi0jHa+slqQqFQ/Dq3zfzzd+WMPPzR/Dj80a3vGJdDaz4A6z6M3wyPwgIgIw8GHwiDDkxCIuBx0JKescULyI9WltDoct/UU5HOm1UIV87YQi/fmstp47sz9QRBc2vmJIGx3wleLjD9nWw/j1Y/24wXT03WC85HQZNCloRQ6bC4CmQmd9hn0dEpKm4tRTMbDZwPrDF3cc1s9yAu4HzgErgG+6+uLXtJrKlAFBZU8cX7nmb6tp65sz6PHmZqYe+kT1bw5CYH0w3LoGGOsCgcGzQkmgMirxBMf8MItLzJPz0kZl9HqgAHmshFM4DriUIhROAu939hNa2m+hQAFiyYQcX3v8uX5owkDsvntj+DdbsgU8XBQHxybtQuhBqKoJleUPCPokwKPqOVL+EiByyhJ8+cvc3zWzYQVa5gCAwHHjPzPLNbIC7d/qxJCYOzufa047krldWc/ro/px/zMD2bTAtG4Z/PnhAcEPc5mVhS2I+fPwaLH0qWJbZO+iXaOy8HjAxOF3VnYXfeUGShjIXibdE9ikMAjZEPS8NXzsgFMxsJjATYMiQIR1SXGu+d+qRvL6qjH9/fhmTh/ahKC8jdhtPToGBE4PHid8J+iW2rY065TQfPpoTrJuSAYMmBy2J3sMgqw9k9tk3zewdbK8zqt0bDEJYsSXojK/YEj6Pnt8S3CiYkhH0uQybBkNPCvpi1EkvEnNxvfoobCm81MLpoz8Bt7r72+HzvwI/cveDDizUGU4fNfrH1j2cd/dbHDe0N499cwpJSdZxP7yiDDa8F1zdtH4+bPwAvL75ddN7BeHQNDD2m/beN83sA+m5YIfxedyDLzLavQkqNu977N584M6+uplvuLNkyOkfPoqCaW4RVO2Ede/AluXBeikZUHw8DDsJhk4L5lNjGMzSOneoqw5OdVbv3jetroCaxmlF8LeUOzD4d8wdEPydHc7fVk/lHhwY7Vgf/P8sGHFYm0n46aM2KAUGRz0vBj5LUC2HZXjfbH5y/mj+/fllPDp/HVdMG95xPzynH4z+YvCA4Ki7Ygvs3RYMubF3ezhtMl+5DbZ9DJXbm98pN0pKbSZIop6nZUNl+f47/93htKH2wO2lZkNuIeQUBp3pR54eteMvDJcVBds/2Gmiym1Bv8sn78C6t+H12wCH5LSgxTRsWhASg6cENcr+3IM+rJqKqJ131A68enfUDr5xZ990edROv7l/69Ykp+0LiMh0QJPnRcHoAT2Be3DxyY71sOOTcNrkUbc3WHfadXDmLXEtJ5EthS8A32NfR/M97j6ltW12ppYCgLtz1aMlvL1mKy9dexJHFeYmuqS2q68LAiMSJM0EyN5tQYBEP6+vCTdgkFUQ/AfOKYzauUc9Gpel58TnM+zdEXbQvx20JBpbTEkpMHDSvtNNQ04Ijlh7CnfY9SmU/T0YeiV6WtWW7yC34PeVlhP826XlBM+jX4vMR03TcyAtd9/y1KwgSHZvgt2fhdONwXRX4/NNQcg0lZbTQniE870GBAcSnb2F6B4cQDXd4W//5MCdfqPM3pA/JHwM3TdfODaYHobOcPXRE8B0oC+wGfgpkArg7g+El6TeB5xDcEnqFe7e6t6+s4UCQNnuas65600Ke2Xwh2umkZbSja8OajzSrK0M+ysO45LceKreDesX7AuJzxYHl/taMgyYEBUSJ3aPe0IaGmDnhgN3/GWr9t/RZhUEd9T3Gwn5g4NTigfs4HP3BUBadsee4okEx8YWgiN8Xl994Hsze+8Li5zC4AuxUjKCPqfGaXL6/s9TMsJH2oHrpmQErZnG561d4OAeHDDt+KTlI/3ayv3fk5G/b0ffe1hUAAyBvMFxaSUlPBTipTOGAsC85ZuY+fgivjN9BP96zqhElyONavbAhvfD003vwKclYUvHoGhcEBCNp5yy+iS62pY11Ac3QTbd+W/9aP8dTk5hsOPvNypqOgqy+yas9JiJ9Fdt3D88dkXNV2yBuqqgr6Ou6vBObzWVlBoVGE3CpbYq3Onv2f890Tv96CP9/CFBKGfktb+uQ6RQSIB/fWYpTy/awNPfnsrxwzrxDqYnq90LpSX7+iRKFwY7D4D+Y4JwGDYNeg0KjxbTo6bpwZFl41FnPI6k6+tg+z/CHX/0zn/1vjoh6LjtNxL6j9638+97dOcOtkRoqA8Cor56X1DsN41+PWpZfU0z61YFQ9hEr5uS3mSHHx7pd8JWqEIhASqq6zjv7rdocGfOdSeTm9HJTq3Igeqq4dPF+043bXj/wKO+liSnNQmKqGlKRsvLktP3nbpITgdvgPLVQQCUr4nqsyG4ebHfyP2P+vsdnZAjTenaFAoJsuiT7cx44F2+PKmY/50xIdHlyKGqr4VNHwYd6o1HhfU14dFmTdRRZ03U0WfT15pOq1pe1ngqq/fQqFM+4dF/36Pj10EvPU5XuCS1WzpuaG+uOfVI7n11DaeP6s+54wckuiQ5FMmpwY1xHaWhIWgpdNYbDKXH6caXySTO908/imOK87juySU8+MbH1Dd0rdaYdKCkJAWCdCoKhThITU7it1dM4bRR/bl1zt+59KH32LCtsvU3iogkmEIhTvpkp3H/ZZP4xYwJrNy4i3PuepOnF26gq/XhiEjPolCIIzPjwuOKmTPrZMYX5/GjZ5cy8/FFbK1o5gYcEZFOQKHQAYp7Z/F/V53IT74wmjc+KuPsO99k3vJNiS5LROQACoUOkpRkXHXyEbz4vZMo7JXBzMcX8aNnPmB3VQzuuBQRiRGFQgcbWZTLH66ZxjWnjuCZRaWce/dbvP+PbYkuS0QEUCgkRFpKEj88exT/7+qpJJlx8UPzuXXOSqrrWvg+BBGRDqJQSKDjhvZhznUnc8nxQ3jwjbVccN87rNy4K9FliUgPplBIsOz0FG798nhmf2MyWytquOC+d3hAN7yJSIIoFDqJ00YVMu/6z3P66P7cphveRCRBFAqdSJ/sNH71tUnc8RXd8CYiiaFQ6GTMjC9P2v+Gt289phveRKRjKBQ6qegb3t5crRveRKRjKBQ6scYb3l66Vje8iUjHUCh0AUcX6oY3EekYCoUuIvqGt+Sk8Ia3P+uGNxGJLYVCF3Pc0D78+fsnc+mUITz4pm54E5HYUih0QdnpKfz3P4/nkW8cT/ke3fAmIrGjUOjCTh3Vn7mz9t3wdu7db/KnpRtpUDiIyGFSKHRxjTe8/eprk6hvcK75v8Wcd89bvLxM4SAih06h0A2YGeeNH8C860/h7ksmUlPXwNW/W8wX7n2becs36Y5oEWkz62o7jMmTJ3tJSUmiy+jU6uobeOGDz7jnr6tZV17JuEG9mHX60Zw+uj9mlujyRCQBzGyRu09ubb24thTM7BwzW2Vma8zspmaWf8PMysxsSfi4Kp719BQpyUl8eVIxr9xwCv9z0THs2lvHVY+VcMEv3+G1v29Ry0FEWhS3loKZJQMfAWcCpcBC4FJ3XxG1zjeAye7+vbZuVy2FQ1db38Bzi0u599U1lG7fy8TB+Vx/5tF8/qi+ajmI9BCdoaUwBVjj7mvdvQZ4Erggjj9PWpCanMTFxw/h1R9M59Yvj6dsdzVfn/0+F97/Lm+v3qqWg4hExDMUBgEbop6Xhq81daGZLTWzZ8xscBzr6fHSUpK4dMoQXrtxOv/1T+PYuLOKyx5ewFcenM+7H29NdHki0gnEMxSaOy/R9JD0RWCYux8DvAI82uyGzGaaWYmZlZSVlcW4zJ4nLSWJy04cyus/nM4tF4xl/bZKvvrrBVz84HzeW1ue6PJEJIHi2acwFbjZ3c8On/8bgLvf2sL6ycA2d8872HbVpxB7VbX1PPH+en71+seU7a7mcyMKuP7Mozl+WJ9ElyYiMdIZ+hQWAkeZ2XAzSwMuAV6IXsHMBkQ9/RKwMo71SAsyUpO5Ytpw3vrRqfzH+WP4aHMFMx6Yz+UPL2DRJ9sTXZ6IdKC4hYK71wHfA+YS7OyfdvflZnaLmX0pXO37ZrbczD4Avg98I171SOsyUpO58qQgHP79vNGs+GwXF97/Lv8y+33+tl7hINIT6OY1aVFlTR2Pzf+EB9/4mO2VtZw6sh/Xn3k0xxTnJ7o0ETlEbT19pFCQVlVU1/Hou+v49Vtr2VFZyxmj+3Pd6Uczvvig3T8i0okoFCTmdlfV8tt3gnDYVVXHkf1zOGtMIWeOKWRCcT5JSboRTqSzUihI3OyqquX5xZ8yb8Um3lu7jfoGp39uOmeOKeSssUVMPaKAtBSNtSjSmSgUpEPsqKzhtVVbmLd8M6+vKmNvbT256SlMH9Wfs8YUMn1kP3IzUhNdpkiPp1CQDldVW887a7Yyb/lmXlm5mfI9NaQmG58b0ZezxhZy5uhC+vfKSHSZIj2SQkESqr7B+dv67cxbsZm5yzfxSXklAMcOyeesMUWcNbaQEf1yElylSM+hUJBOw91ZvaWCecs3MW/FZpaW7gRgRL9szhpbxFnqqBaJO4WCdFqf7djLKys3M2/5Zt5bW05d2FF9xphCzhpTyNQRBaSnJCe6TJFuRaEgXcLOytqgo3rFJl5fVUZlTT056SlMH9mPs8YWMX1kP3qpo1qk3RQK0uVU1dYz/+Ny5q3YxF9WbGZrRdBRPXVEX6Yf3Y+JQ/IZM6AXGalqRYgcKoWCdGn1Dc6SDduZtzzoqF4XdlSnJhujB/RiQnE+EwbnM3FwPkf0zVZ/hEgrFArSrWzaWcWSDTv4oHQHS9bvYGnpDvbU1AOQm57CMYPzmDg4nwnFQVDo0leR/bU1FFI6ohiR9irKy+CcvCLOGVcEBC2JtWUVLNmwIxIWD76xlrqG4CBnQF5GEBJhUIwvziMnXX/uIq3R/xLpkpKTjKMKczmqMJcZk4Nvca2qrWf5Z7uCkAiDYs6yTQCYwdH9c5kwOC8SFCOLcklN1nAcItEUCtJtZKQmc9zQ3hw3tHfktW17avigNAyJDTv4y4rNPF1SGq6fxLiBYUgMzufYwfkU987ETP0T0nOpT0F6FHdnw7a9LAmDYsmGHSz7dCfVdQ0A9MlOY0JxHkP6ZFGQk05BThoF2en0yw2mBTlp5KSnKDiky1GfgkgzzIwhBVkMKcjiSxMGAlBb38CqTbsjLYqlpTtZvH4HO/fWNruNtJQk+man7RcafXPSIvMFOWn0jVqmEWOlK1EoSI+XmpzEuEF5jBuUx9dOGBp5vaauge2VNWytqKa8Imq6J5iWV1RTvqeG1ZsrKKuopiZsbTTVKyNlv5AoyAkCpW9UoAzuk0VRrwxdWisJp1AQaUFaShKFvTIobMPlre7Onpp6yiuq2RoVGJHne2rYuruatVsrWLiuhm2VNTQ9c5uWnERx70wG98liSJ8shhZkReYH98nS1VPSIfRXJhIDZkZOego56SkMLchudf36Bmd7ZQ3lFTVs2V3Fhm17Wb+tkg3bKlm/rZIlGw48fVWQnRYJiSFRYTG0IIvCXhkkq5UhMaBQEEmA5CSjb066wYIyAAAIgklEQVQ6fXPSGVmU2+w6OytrWR+GRONjQxgYf/pwI/UN+5oaTVsZjYExpE/Qf6JWhrSV/lJEOqm8rFTGZ+UxvjjvgGV19Q18tqPqgMBoqZXRJ2xlDO2TRWGvdDLTUshOSyYrLZmstJRgmh5MM1OTyQ7nG5erFdJzKBREuqCU5KTIVVTNOVgrY8vuKqpqm+8Ub0l6StL+AdI0TFKTyUpPbnGdzNRkMqOeZ6QmR15X53rnolAQ6YYO1soAaGhw9tbWU1lTT2VN3X7TPdX17K2tC6Y1+6+zp6aOvTX17KmpZ29NHZt21e5bXl1PZW39fqe12iIjNYmstJSo4EhuMr8vZBrDJCstmczwPcH8vuUpSUZy04cZKUlJJCWx/9TQPSdNKBREeqCkJCM7PYXs9BQgPWbbdXeq6xrC4KiLCpUgaPbWNFBZUxcJpL019eF8EDpVtfvWL6+oYUNNHVW1DZHl1S1c9tsejaFxQJA0eb25sElLTiInI7jAIDec5qSnkp2eHD5PjSzPSU/Zb76znpJTKIhIzJgZGanBEXvv7LSYb7++wSPBsbemnsraIHj2RoInaKlEHu7UNTgNDU2m7tTVB8vrGxqob2D/qe/bRnPvawi3W13bwLY9Nawvr2R3dR0VVUHgtUVWWnKzYZGTkUJuGNiN8zlhwIzol80Rcf5uc4WCiHQZyfu1cDqnuvoG9tTUUxGGREV1Lbur6qiormNPdV1kviKc7g5fr6iqY/2eyn3Lq+sOOBV39SkjuOncUXGtv/P+ZkVEuqCU5CTyMpPIy2zf18g2noqLDpHe2fH/atq4DspiZueY2SozW2NmNzWzPN3MngqXLzCzYfGsR0Skq2g8FdcvN53hfbMZX5xHce/mrzaLpbiFgpklA78EzgXGAJea2Zgmq10JbHf3I4E7gdvjVY+IiLQuni2FKcAad1/r7jXAk8AFTda5AHg0nH8GON10fZiISMLEs09hELAh6nkpcEJL67h7nZntBAqArdErmdlMYGb4tMLMVh1mTX2bbrub6c6fT5+t6+rOn68rfbahra8S31Bo7oi/6V0tbVkHd38IeKjdBZmVtOVLJrqq7vz59Nm6ru78+brjZ4vn6aNSYHDU82Lgs5bWMbMUIA/YFseaRETkIOIZCguBo8xsuJmlAZcALzRZ5wXg6+H8RcCr3tW+H1REpBuJ2+mjsI/ge8BcIBmY7e7LzewWoMTdXwAeBh43szUELYRL4lVPqN2noDq57vz59Nm6ru78+brdZzMdmIuISCN9o7iIiEQoFEREJKLHhEJrQ250VWY22MxeM7OVZrbczK5LdE2xZmbJZvY3M3sp0bXEmpnlm9kzZvb38N9waqJrihUzuz78m1xmZk+YWUaia2oPM5ttZlvMbFnUa33M7C9mtjqc9k5kjbHQI0KhjUNudFV1wA/cfTRwInBNN/psja4DVia6iDi5G3jZ3UcBE+gmn9PMBgHfBya7+ziCi03ifSFJvP0WOKfJazcBf3X3o4C/hs+7tB4RCrRtyI0uyd03uvvicH43wU5lUGKrih0zKwa+APwm0bXEmpn1Aj5PcBUe7l7j7jsSW1VMpQCZ4T1IWRx4n1KX4u5vcuB9VNFD9TwK/FOHFhUHPSUUmhtyo9vsOBuFo8weCyxIbCUxdRfwIyD2X7mVeEcAZcAj4emx35hZdqKLigV3/xT4X2A9sBHY6e7zEltVXBS6+0YIDtCA/gmup916Sii0aTiNrszMcoBngVnuvivR9cSCmZ0PbHH3RYmuJU5SgEnA/e5+LLCHbnD6ASA8t34BMBwYCGSb2WWJrUraoqeEQluG3OiyzCyVIBB+7+7PJbqeGJoGfMnM1hGc8jvNzH6X2JJiqhQodffGlt0zBCHRHZwB/MPdy9y9FngO+FyCa4qHzWY2ACCcbklwPe3WU0KhLUNudEnhUOMPAyvd/Y5E1xNL7v5v7l7s7sMI/s1edfduc7Tp7puADWY2MnzpdGBFAkuKpfXAiWaWFf6Nnk436URvInqonq8Df0xgLTHRI76Os6UhNxJcVqxMAy4HPjSzJeFrP3b3PyewJmm7a4Hfhwcra4ErElxPTLj7AjN7BlhMcIXc3+jiQ0KY2RPAdKCvmZUCPwVuA542sysJgnBG4iqMDQ1zISIiET3l9JGIiLSBQkFERCIUCiIiEqFQEBGRCIWCiIhEKBSkRzOzejNbEvW4KXz99XBU3Q/M7J3GewnMLM3M7jKzj8ORMf8Yjs/UuL0iM3syXL7CzP5sZkeb2bDo0TXDdW82sxvD+RPNbEFYw0ozu7kDfw0iET3iPgWRg9jr7hNbWPY1dy8xs5nA/wBfAv4byAWOdvd6M7sCeM7MTgjf8zzwqLtfAmBmE4FC9h97qzmPAl9x9w/CUX1HtrK+SFwoFERa9yYwy8yyCG4uG+7u9QDu/oiZfRM4jWA8rVp3f6Dxje6+BCKDFR5Mf4KB4wi33V3ubJYuRqEgPV1m1J3gALe6+1NN1vki8CFwJLC+mQEHS4Cx4fzBBu8b0eRnFRGMJApwJ7DKzF4HXiZobVS1/WOIxIZCQXq6g50++r2Z7QXWEQxH0YfmR9e18PXmRuON9nH0z4ruN3D3W8zs98BZwFeBSwmGVBDpUAoFkZZ9zd1LGp+Y2TZgqJnlhl9o1GgS8GI4f9Hh/jB3/xi438x+DZSZWYG7lx/u9kQOh64+Emkjd99D0CF8R9gZjJn9C8G3ir0aPtLN7FuN7zGz483slNa2bWZfCEcTBTgKqAe607ewSRehUJCeLrPJJam3tbL+vwFVwEdmtppgVMx/9hDwz8CZ4SWpy4Gbadt3d1xO0KewBHicoJVSf7gfSuRwaZRUERGJUEtBREQiFAoiIhKhUBARkQiFgoiIRCgUREQkQqEgIiIRCgUREYn4/9edUK/EGZhNAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "### Loss graph\n",
    "plt.plot(model.history[\"loss\"])\n",
    "plt.plot(model.history[\"val_loss\"])\n",
    "plt.xlabel(\"EPOCHS\")\n",
    "plt.ylabel(\"LOSS\")\n",
    "plt.ylim(0.0,3.0)\n",
    "plt.legend([\"Train\",\"Testing\"],loc=\"upper right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### As we can see our data is till overfitting, what we can do is add augmentation in the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelBuilderMoreAugmentation(metaclass= abc.ABCMeta):\n",
    "    \n",
    "    def initialize_path(self,project_folder):\n",
    "        self.train_doc = np.random.permutation(open(project_folder + '/' + 'train.csv').readlines())\n",
    "        self.val_doc = np.random.permutation(open(project_folder + '/' + 'val.csv').readlines())\n",
    "        self.train_path = project_folder + '/' + 'train'\n",
    "        self.val_path =  project_folder + '/' + 'val'\n",
    "        self.num_train_sequences = len(self.train_doc)\n",
    "        self.num_val_sequences = len(self.val_doc)\n",
    "        \n",
    "    def initialize_image_properties(self,image_height=100,image_width=100):\n",
    "        self.image_height=image_height\n",
    "        self.image_width=image_width\n",
    "        self.channels=3\n",
    "        self.num_classes=5\n",
    "        self.total_frames=30\n",
    "          \n",
    "    def initialize_hyperparams(self,frames_to_sample=30,batch_size=20,num_epochs=20):\n",
    "        self.frames_to_sample=frames_to_sample\n",
    "        self.batch_size=batch_size\n",
    "        self.num_epochs=num_epochs\n",
    "        \n",
    "        \n",
    "    def generator(self,source_path, folder_list, augment=False):\n",
    "        img_idx = np.round(np.linspace(0,self.total_frames-1,self.frames_to_sample)).astype(int)\n",
    "        batch_size=self.batch_size\n",
    "        while True:\n",
    "            t = np.random.permutation(folder_list)\n",
    "            num_batches = len(t)//batch_size\n",
    "        \n",
    "            for batch in range(num_batches): \n",
    "                batch_data, batch_labels= self.one_batch_data(source_path,t,batch,batch_size,img_idx,augment)\n",
    "                yield batch_data, batch_labels \n",
    "\n",
    "            remaining_seq=len(t)%batch_size\n",
    "        \n",
    "            if (remaining_seq != 0):\n",
    "                batch_data, batch_labels= self.one_batch_data(source_path,t,num_batches,batch_size,img_idx,augment,remaining_seq)\n",
    "                yield batch_data, batch_labels \n",
    "    \n",
    "    \n",
    "    def one_batch_data(self,source_path,t,batch,batch_size,img_idx,augment,remaining_seq=0):\n",
    "    \n",
    "        seq_len = remaining_seq if remaining_seq else batch_size\n",
    "    \n",
    "        batch_data = np.zeros((seq_len,len(img_idx),self.image_height,self.image_width,self.channels)) \n",
    "        batch_labels = np.zeros((seq_len,self.num_classes)) \n",
    "    \n",
    "        if (augment): batch_data_aug = np.zeros((seq_len,len(img_idx),self.image_height,self.image_width,self.channels))\n",
    "\n",
    "        \n",
    "        for folder in range(seq_len): \n",
    "            imgs = os.listdir(source_path+'/'+ t[folder + (batch*batch_size)].split(';')[0]) \n",
    "            for idx,item in enumerate(img_idx): \n",
    "                image = imread(source_path+'/'+ t[folder + (batch*batch_size)].strip().split(';')[0]+'/'+imgs[item]).astype(np.float32)\n",
    "                image_resized=imresize(image,(self.image_height,self.image_width,3))\n",
    "            \n",
    "\n",
    "                batch_data[folder,idx,:,:,0] = (image_resized[:,:,0])/255\n",
    "                batch_data[folder,idx,:,:,1] = (image_resized[:,:,1])/255\n",
    "                batch_data[folder,idx,:,:,2] = (image_resized[:,:,2])/255\n",
    "            \n",
    "                if (augment):\n",
    "                    shifted = cv2.warpAffine(image, \n",
    "                                             np.float32([[1, 0, np.random.randint(-30,30)],[0, 1, np.random.randint(-30,30)]]), \n",
    "                                            (image.shape[1], image.shape[0]))\n",
    "                    \n",
    "                    gray = cv2.cvtColor(shifted,cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "                    x0, y0 = np.argwhere(gray > 0).min(axis=0)\n",
    "                    x1, y1 = np.argwhere(gray > 0).max(axis=0) \n",
    "                    \n",
    "                    cropped=shifted[x0:x1,y0:y1,:]\n",
    "                    \n",
    "                    image_resized=imresize(cropped,(self.image_height,self.image_width,3))\n",
    "                    \n",
    "                    M = cv2.getRotationMatrix2D((self.image_width//2,self.image_height//2),\n",
    "                                                np.random.randint(-10,10), 1.0)\n",
    "                    rotated = cv2.warpAffine(image_resized, M, (self.image_width, self.image_height))\n",
    "                    \n",
    "            \n",
    "                    batch_data_aug[folder,idx,:,:,0] = (rotated[:,:,0])/255\n",
    "                    batch_data_aug[folder,idx,:,:,1] = (rotated[:,:,1])/255\n",
    "                    batch_data_aug[folder,idx,:,:,2] = (rotated[:,:,2])/255\n",
    "                \n",
    "            \n",
    "            batch_labels[folder, int(t[folder + (batch*batch_size)].strip().split(';')[2])] = 1\n",
    "            \n",
    "    \n",
    "        if (augment):\n",
    "            batch_data=np.concatenate([batch_data,batch_data_aug])\n",
    "            batch_labels=np.concatenate([batch_labels,batch_labels])\n",
    "\n",
    "        \n",
    "        return(batch_data,batch_labels)\n",
    "    \n",
    "    \n",
    "    def train_model(self, model, augment_data=False):\n",
    "        train_generator = self.generator(self.train_path, self.train_doc,augment=augment_data)\n",
    "        val_generator = self.generator(self.val_path, self.val_doc)\n",
    "\n",
    "        model_name = 'model_init' + '_' + str(datetime.datetime.now()).replace(' ','').replace(':','_') + '/'\n",
    "    \n",
    "        if not os.path.exists(model_name):\n",
    "            os.mkdir(model_name)\n",
    "        \n",
    "        filepath = model_name + 'model-{epoch:05d}-{loss:.5f}-{categorical_accuracy:.5f}-{val_loss:.5f}-{val_categorical_accuracy:.5f}.h5'\n",
    "\n",
    "        checkpoint = ModelCheckpoint(filepath, monitor='val_loss', verbose=1, save_best_only=False, save_weights_only=False, mode='auto', period=1)\n",
    "        LR = ReduceLROnPlateau(monitor='val_loss', factor=0.2, verbose=1, patience=4)\n",
    "        callbacks_list = [checkpoint, LR]\n",
    "\n",
    "        if (self.num_train_sequences%self.batch_size) == 0:\n",
    "            steps_per_epoch = int(self.num_train_sequences/self.batch_size)\n",
    "        else:\n",
    "            steps_per_epoch = (self.num_train_sequences//self.batch_size) + 1\n",
    "\n",
    "        if (self.num_val_sequences%self.batch_size) == 0:\n",
    "            validation_steps = int(self.num_val_sequences/self.batch_size)\n",
    "        else:\n",
    "            validation_steps = (self.num_val_sequences//self.batch_size) + 1\n",
    "    \n",
    "        history=model.fit_generator(train_generator, steps_per_epoch=steps_per_epoch, epochs=self.num_epochs, verbose=1, \n",
    "                            callbacks=callbacks_list, validation_data=val_generator, \n",
    "                            validation_steps=validation_steps, class_weight=None, workers=1, initial_epoch=0)\n",
    "        return history\n",
    "\n",
    "        \n",
    "    @abc.abstractmethod\n",
    "    def define_model(self):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. CNN3d with Aug"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class ModelConv3D7(ModelBuilderMoreAugmentation):\n",
    "    \n",
    "    def define_model(self,filtersize=(3,3,3),dense_neurons=64,dropout=0.25):\n",
    "\n",
    "        model = Sequential()\n",
    "        model.add(Conv3D(16, filtersize, padding='same',\n",
    "                 input_shape=(self.frames_to_sample,self.image_height,self.image_width,self.channels)))\n",
    "        model.add(Activation('relu'))\n",
    "        model.add(BatchNormalization())\n",
    "        model.add(MaxPooling3D(pool_size=(2, 2, 2)))\n",
    "\n",
    "        model.add(Conv3D(32, filtersize, padding='same'))\n",
    "        model.add(Activation('relu'))\n",
    "        model.add(BatchNormalization())\n",
    "        model.add(MaxPooling3D(pool_size=(2, 2, 2)))\n",
    "\n",
    "        model.add(Conv3D(64, filtersize, padding='same'))\n",
    "        model.add(Activation('relu'))\n",
    "        model.add(BatchNormalization())\n",
    "        model.add(MaxPooling3D(pool_size=(2, 2, 2)))\n",
    "\n",
    "        model.add(Conv3D(128, filtersize, padding='same'))\n",
    "        model.add(Activation('relu'))\n",
    "        model.add(BatchNormalization())\n",
    "        model.add(MaxPooling3D(pool_size=(2, 2, 2)))\n",
    "\n",
    "        model.add(Flatten())\n",
    "        model.add(Dense(dense_neurons,activation='relu'))\n",
    "        model.add(BatchNormalization())\n",
    "        model.add(Dropout(dropout))\n",
    "\n",
    "        model.add(Dense(dense_neurons,activation='relu'))\n",
    "        model.add(BatchNormalization())\n",
    "        model.add(Dropout(dropout))\n",
    "\n",
    "\n",
    "        model.add(Dense(self.num_classes,activation='softmax'))\n",
    "\n",
    "        optimiser = optimizers.Adam(lr=0.0002)\n",
    "        model.compile(optimizer=optimiser, loss='categorical_crossentropy', metrics=['categorical_accuracy'])\n",
    "        return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv3d_42 (Conv3D)           (None, 20, 160, 160, 16)  1312      \n",
      "_________________________________________________________________\n",
      "activation_42 (Activation)   (None, 20, 160, 160, 16)  0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_37 (Batc (None, 20, 160, 160, 16)  64        \n",
      "_________________________________________________________________\n",
      "max_pooling3d_37 (MaxPooling (None, 10, 80, 80, 16)    0         \n",
      "_________________________________________________________________\n",
      "conv3d_43 (Conv3D)           (None, 10, 80, 80, 32)    13856     \n",
      "_________________________________________________________________\n",
      "activation_43 (Activation)   (None, 10, 80, 80, 32)    0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_38 (Batc (None, 10, 80, 80, 32)    128       \n",
      "_________________________________________________________________\n",
      "max_pooling3d_38 (MaxPooling (None, 5, 40, 40, 32)     0         \n",
      "_________________________________________________________________\n",
      "conv3d_44 (Conv3D)           (None, 5, 40, 40, 64)     55360     \n",
      "_________________________________________________________________\n",
      "activation_44 (Activation)   (None, 5, 40, 40, 64)     0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_39 (Batc (None, 5, 40, 40, 64)     256       \n",
      "_________________________________________________________________\n",
      "max_pooling3d_39 (MaxPooling (None, 2, 20, 20, 64)     0         \n",
      "_________________________________________________________________\n",
      "conv3d_45 (Conv3D)           (None, 2, 20, 20, 128)    221312    \n",
      "_________________________________________________________________\n",
      "activation_45 (Activation)   (None, 2, 20, 20, 128)    0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_40 (Batc (None, 2, 20, 20, 128)    512       \n",
      "_________________________________________________________________\n",
      "max_pooling3d_40 (MaxPooling (None, 1, 10, 10, 128)    0         \n",
      "_________________________________________________________________\n",
      "flatten_13 (Flatten)         (None, 12800)             0         \n",
      "_________________________________________________________________\n",
      "dense_46 (Dense)             (None, 256)               3277056   \n",
      "_________________________________________________________________\n",
      "batch_normalization_41 (Batc (None, 256)               1024      \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_47 (Dense)             (None, 256)               65792     \n",
      "_________________________________________________________________\n",
      "batch_normalization_42 (Batc (None, 256)               1024      \n",
      "_________________________________________________________________\n",
      "dropout_6 (Dropout)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_48 (Dense)             (None, 5)                 1285      \n",
      "=================================================================\n",
      "Total params: 3,638,981\n",
      "Trainable params: 3,637,477\n",
      "Non-trainable params: 1,504\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "\n",
    "conv_3d7=ModelConv3D7()\n",
    "conv_3d7.initialize_path(project_folder)\n",
    "conv_3d7.initialize_image_properties(image_height=160,image_width=160)\n",
    "conv_3d7.initialize_hyperparams(frames_to_sample=20,batch_size=20,num_epochs=20)\n",
    "conv_3d7_model=conv_3d7.define_model(dense_neurons=256,dropout=0.5)\n",
    "conv_3d7_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Params: 3638981\n",
      "Epoch 1/20\n",
      "34/34 [==============================] - 144s 4s/step - loss: 1.8721 - categorical_accuracy: 0.3826 - val_loss: 1.0147 - val_categorical_accuracy: 0.6000\n",
      "\n",
      "Epoch 00001: saving model to model_init_2021-02-0803_01_45.405982/model-00001-1.89307-0.37557-1.01474-0.60000.h5\n",
      "Epoch 2/20\n",
      "34/34 [==============================] - 128s 4s/step - loss: 1.4393 - categorical_accuracy: 0.5000 - val_loss: 0.8639 - val_categorical_accuracy: 0.6500\n",
      "\n",
      "Epoch 00002: saving model to model_init_2021-02-0803_01_45.405982/model-00002-1.43108-0.50830-0.86388-0.65000.h5\n",
      "Epoch 3/20\n",
      "34/34 [==============================] - 132s 4s/step - loss: 1.2465 - categorical_accuracy: 0.5618 - val_loss: 1.3313 - val_categorical_accuracy: 0.5600\n",
      "\n",
      "Epoch 00003: saving model to model_init_2021-02-0803_01_45.405982/model-00003-1.22579-0.56335-1.33133-0.56000.h5\n",
      "Epoch 4/20\n",
      "34/34 [==============================] - 132s 4s/step - loss: 1.1398 - categorical_accuracy: 0.6056 - val_loss: 2.1268 - val_categorical_accuracy: 0.4800\n",
      "\n",
      "Epoch 00004: saving model to model_init_2021-02-0803_01_45.405982/model-00004-1.15180-0.60407-2.12685-0.48000.h5\n",
      "Epoch 5/20\n",
      "34/34 [==============================] - 133s 4s/step - loss: 0.9399 - categorical_accuracy: 0.6586 - val_loss: 1.3391 - val_categorical_accuracy: 0.5700\n",
      "\n",
      "Epoch 00005: saving model to model_init_2021-02-0803_01_45.405982/model-00005-0.93891-0.65837-1.33914-0.57000.h5\n",
      "Epoch 6/20\n",
      "34/34 [==============================] - 132s 4s/step - loss: 0.8802 - categorical_accuracy: 0.6825 - val_loss: 0.9228 - val_categorical_accuracy: 0.7100\n",
      "\n",
      "Epoch 00006: saving model to model_init_2021-02-0803_01_45.405982/model-00006-0.86251-0.68703-0.92283-0.71000.h5\n",
      "\n",
      "Epoch 00006: ReduceLROnPlateau reducing learning rate to 3.9999998989515007e-05.\n",
      "Epoch 7/20\n",
      "34/34 [==============================] - 131s 4s/step - loss: 0.7989 - categorical_accuracy: 0.7152 - val_loss: 0.9766 - val_categorical_accuracy: 0.6900\n",
      "\n",
      "Epoch 00007: saving model to model_init_2021-02-0803_01_45.405982/model-00007-0.77762-0.72474-0.97655-0.69000.h5\n",
      "Epoch 8/20\n",
      "34/34 [==============================] - 131s 4s/step - loss: 0.7278 - categorical_accuracy: 0.7270 - val_loss: 0.8058 - val_categorical_accuracy: 0.7200\n",
      "\n",
      "Epoch 00008: saving model to model_init_2021-02-0803_01_45.405982/model-00008-0.74258-0.72021-0.80584-0.72000.h5\n",
      "Epoch 9/20\n",
      "34/34 [==============================] - 131s 4s/step - loss: 0.7477 - categorical_accuracy: 0.7336 - val_loss: 0.7474 - val_categorical_accuracy: 0.7300\n",
      "\n",
      "Epoch 00009: saving model to model_init_2021-02-0803_01_45.405982/model-00009-0.76468-0.72700-0.74743-0.73000.h5\n",
      "Epoch 10/20\n",
      "34/34 [==============================] - 132s 4s/step - loss: 0.7015 - categorical_accuracy: 0.7425 - val_loss: 0.7362 - val_categorical_accuracy: 0.7500\n",
      "\n",
      "Epoch 00010: saving model to model_init_2021-02-0803_01_45.405982/model-00010-0.69362-0.74434-0.73619-0.75000.h5\n",
      "Epoch 11/20\n",
      "34/34 [==============================] - 131s 4s/step - loss: 0.6047 - categorical_accuracy: 0.7730 - val_loss: 0.7809 - val_categorical_accuracy: 0.7200\n",
      "\n",
      "Epoch 00011: saving model to model_init_2021-02-0803_01_45.405982/model-00011-0.60768-0.77149-0.78093-0.72000.h5\n",
      "Epoch 12/20\n",
      "34/34 [==============================] - 132s 4s/step - loss: 0.6670 - categorical_accuracy: 0.7506 - val_loss: 0.7565 - val_categorical_accuracy: 0.7200\n",
      "\n",
      "Epoch 00012: saving model to model_init_2021-02-0803_01_45.405982/model-00012-0.62466-0.76094-0.75648-0.72000.h5\n",
      "Epoch 13/20\n",
      "34/34 [==============================] - 132s 4s/step - loss: 0.6207 - categorical_accuracy: 0.7745 - val_loss: 0.7106 - val_categorical_accuracy: 0.7300\n",
      "\n",
      "Epoch 00013: saving model to model_init_2021-02-0803_01_45.405982/model-00013-0.60966-0.77300-0.71062-0.73000.h5\n",
      "Epoch 14/20\n",
      "34/34 [==============================] - 133s 4s/step - loss: 0.5834 - categorical_accuracy: 0.7859 - val_loss: 0.6265 - val_categorical_accuracy: 0.7900\n",
      "\n",
      "Epoch 00014: saving model to model_init_2021-02-0803_01_45.405982/model-00014-0.59072-0.78054-0.62646-0.79000.h5\n",
      "Epoch 15/20\n",
      "34/34 [==============================] - 131s 4s/step - loss: 0.5533 - categorical_accuracy: 0.8076 - val_loss: 0.7137 - val_categorical_accuracy: 0.7500\n",
      "\n",
      "Epoch 00015: saving model to model_init_2021-02-0803_01_45.405982/model-00015-0.53371-0.81523-0.71367-0.75000.h5\n",
      "Epoch 16/20\n",
      "34/34 [==============================] - 132s 4s/step - loss: 0.5611 - categorical_accuracy: 0.8017 - val_loss: 0.6598 - val_categorical_accuracy: 0.7800\n",
      "\n",
      "Epoch 00016: saving model to model_init_2021-02-0803_01_45.405982/model-00016-0.54217-0.80920-0.65983-0.78000.h5\n",
      "Epoch 17/20\n",
      "34/34 [==============================] - 132s 4s/step - loss: 0.4900 - categorical_accuracy: 0.8190 - val_loss: 0.6791 - val_categorical_accuracy: 0.7800\n",
      "\n",
      "Epoch 00017: saving model to model_init_2021-02-0803_01_45.405982/model-00017-0.47992-0.82278-0.67913-0.78000.h5\n",
      "Epoch 18/20\n",
      "34/34 [==============================] - 132s 4s/step - loss: 0.5924 - categorical_accuracy: 0.7892 - val_loss: 0.6379 - val_categorical_accuracy: 0.7600\n",
      "\n",
      "Epoch 00018: saving model to model_init_2021-02-0803_01_45.405982/model-00018-0.56903-0.79638-0.63785-0.76000.h5\n",
      "\n",
      "Epoch 00018: ReduceLROnPlateau reducing learning rate to 7.999999797903002e-06.\n",
      "Epoch 19/20\n",
      "34/34 [==============================] - 131s 4s/step - loss: 0.5731 - categorical_accuracy: 0.8010 - val_loss: 0.6181 - val_categorical_accuracy: 0.8000\n",
      "\n",
      "Epoch 00019: saving model to model_init_2021-02-0803_01_45.405982/model-00019-0.53596-0.80845-0.61814-0.80000.h5\n",
      "Epoch 20/20\n",
      "34/34 [==============================] - 133s 4s/step - loss: 0.4737 - categorical_accuracy: 0.8263 - val_loss: 0.5951 - val_categorical_accuracy: 0.8000\n",
      "\n",
      "Epoch 00020: saving model to model_init_2021-02-0803_01_45.405982/model-00020-0.48244-0.82202-0.59513-0.80000.h5\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(\"Total Params:\", conv_3d7_model.count_params())\n",
    "history_model7=conv_3d7.train_model(conv_3d7_model,augment_data=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### This model gives very good results, lets try to achieve with lesser parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 8 -CNN3d Adding more layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelConv3D8(ModelBuilderMoreAugmentation):\n",
    "    \n",
    "    def define_model(self,filtersize=(3,3,3),dense_neurons=64,dropout=0.25):\n",
    "\n",
    "        model = Sequential()\n",
    "        model.add(Conv3D(16, filtersize, padding='same',\n",
    "                 input_shape=(self.frames_to_sample,self.image_height,self.image_width,self.channels)))\n",
    "        model.add(Activation('relu'))\n",
    "        model.add(BatchNormalization())\n",
    "        \n",
    "        model.add(MaxPooling3D(pool_size=(2, 2, 2)))\n",
    "\n",
    "        model.add(Conv3D(32, filtersize, padding='same'))\n",
    "        model.add(Activation('relu'))\n",
    "        model.add(BatchNormalization())\n",
    "        \n",
    "        model.add(Conv3D(32, filtersize, padding='same'))\n",
    "        model.add(Activation('relu'))\n",
    "        model.add(BatchNormalization())\n",
    "        \n",
    "        model.add(MaxPooling3D(pool_size=(2, 2, 2)))\n",
    "\n",
    "        model.add(Conv3D(64, filtersize, padding='same'))\n",
    "        model.add(Activation('relu'))\n",
    "        model.add(BatchNormalization())\n",
    "        \n",
    "        model.add(Conv3D(64, filtersize, padding='same'))\n",
    "        model.add(Activation('relu'))\n",
    "        model.add(BatchNormalization())\n",
    "        \n",
    "        model.add(MaxPooling3D(pool_size=(2, 2, 2)))\n",
    "\n",
    "        model.add(Conv3D(128, filtersize, padding='same'))\n",
    "        model.add(Activation('relu'))\n",
    "        model.add(BatchNormalization())\n",
    "        \n",
    "        model.add(Conv3D(128, filtersize, padding='same'))\n",
    "        model.add(Activation('relu'))\n",
    "        model.add(BatchNormalization())\n",
    "        \n",
    "        model.add(MaxPooling3D(pool_size=(2, 2, 2)))\n",
    "        \n",
    "\n",
    "        model.add(Flatten())\n",
    "        model.add(Dense(dense_neurons,activation='relu'))\n",
    "        model.add(BatchNormalization())\n",
    "        model.add(Dropout(dropout))\n",
    "\n",
    "        model.add(Dense(dense_neurons,activation='relu'))\n",
    "        model.add(BatchNormalization())\n",
    "        model.add(Dropout(dropout))\n",
    "\n",
    "\n",
    "        model.add(Dense(self.num_classes,activation='softmax'))\n",
    "\n",
    "        optimiser = optimizers.Adam(lr=0.0002)\n",
    "        model.compile(optimizer=optimiser, loss='categorical_crossentropy', metrics=['categorical_accuracy'])\n",
    "        return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv3d_53 (Conv3D)           (None, 16, 120, 120, 16)  1312      \n",
      "_________________________________________________________________\n",
      "activation_53 (Activation)   (None, 16, 120, 120, 16)  0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_52 (Batc (None, 16, 120, 120, 16)  64        \n",
      "_________________________________________________________________\n",
      "max_pooling3d_45 (MaxPooling (None, 8, 60, 60, 16)     0         \n",
      "_________________________________________________________________\n",
      "conv3d_54 (Conv3D)           (None, 8, 60, 60, 32)     13856     \n",
      "_________________________________________________________________\n",
      "activation_54 (Activation)   (None, 8, 60, 60, 32)     0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_53 (Batc (None, 8, 60, 60, 32)     128       \n",
      "_________________________________________________________________\n",
      "conv3d_55 (Conv3D)           (None, 8, 60, 60, 32)     27680     \n",
      "_________________________________________________________________\n",
      "activation_55 (Activation)   (None, 8, 60, 60, 32)     0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_54 (Batc (None, 8, 60, 60, 32)     128       \n",
      "_________________________________________________________________\n",
      "max_pooling3d_46 (MaxPooling (None, 4, 30, 30, 32)     0         \n",
      "_________________________________________________________________\n",
      "conv3d_56 (Conv3D)           (None, 4, 30, 30, 64)     55360     \n",
      "_________________________________________________________________\n",
      "activation_56 (Activation)   (None, 4, 30, 30, 64)     0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_55 (Batc (None, 4, 30, 30, 64)     256       \n",
      "_________________________________________________________________\n",
      "conv3d_57 (Conv3D)           (None, 4, 30, 30, 64)     110656    \n",
      "_________________________________________________________________\n",
      "activation_57 (Activation)   (None, 4, 30, 30, 64)     0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_56 (Batc (None, 4, 30, 30, 64)     256       \n",
      "_________________________________________________________________\n",
      "max_pooling3d_47 (MaxPooling (None, 2, 15, 15, 64)     0         \n",
      "_________________________________________________________________\n",
      "conv3d_58 (Conv3D)           (None, 2, 15, 15, 128)    221312    \n",
      "_________________________________________________________________\n",
      "activation_58 (Activation)   (None, 2, 15, 15, 128)    0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_57 (Batc (None, 2, 15, 15, 128)    512       \n",
      "_________________________________________________________________\n",
      "conv3d_59 (Conv3D)           (None, 2, 15, 15, 128)    442496    \n",
      "_________________________________________________________________\n",
      "activation_59 (Activation)   (None, 2, 15, 15, 128)    0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_58 (Batc (None, 2, 15, 15, 128)    512       \n",
      "_________________________________________________________________\n",
      "max_pooling3d_48 (MaxPooling (None, 1, 7, 7, 128)      0         \n",
      "_________________________________________________________________\n",
      "flatten_15 (Flatten)         (None, 6272)              0         \n",
      "_________________________________________________________________\n",
      "dense_52 (Dense)             (None, 256)               1605888   \n",
      "_________________________________________________________________\n",
      "batch_normalization_59 (Batc (None, 256)               1024      \n",
      "_________________________________________________________________\n",
      "dropout_9 (Dropout)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_53 (Dense)             (None, 256)               65792     \n",
      "_________________________________________________________________\n",
      "batch_normalization_60 (Batc (None, 256)               1024      \n",
      "_________________________________________________________________\n",
      "dropout_10 (Dropout)         (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_54 (Dense)             (None, 5)                 1285      \n",
      "=================================================================\n",
      "Total params: 2,549,541\n",
      "Trainable params: 2,547,589\n",
      "Non-trainable params: 1,952\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "conv_3d8=ModelConv3D8()\n",
    "conv_3d8.initialize_path(project_folder)\n",
    "conv_3d8.initialize_image_properties(image_height=120,image_width=120)\n",
    "conv_3d8.initialize_hyperparams(frames_to_sample=16,batch_size=32,num_epochs=15)\n",
    "conv_3d8_model=conv_3d8.define_model(filtersize=(3,3,3),dense_neurons=256,dropout=0.5)\n",
    "conv_3d8_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Params: 2549541\n",
      "Epoch 1/15\n",
      "21/21 [==============================] - 103s 5s/step - loss: 2.2220 - categorical_accuracy: 0.3036 - val_loss: 2.1349 - val_categorical_accuracy: 0.4400\n",
      "\n",
      "Epoch 00001: saving model to model_init_2021-02-0803_49_30.733147/model-00001-2.22094-0.30468-2.13492-0.44000.h5\n",
      "Epoch 2/15\n",
      "21/21 [==============================] - 78s 4s/step - loss: 1.6416 - categorical_accuracy: 0.4406 - val_loss: 1.3721 - val_categorical_accuracy: 0.5100\n",
      "\n",
      "Epoch 00002: saving model to model_init_2021-02-0803_49_30.733147/model-00002-1.64304-0.44042-1.37208-0.51000.h5\n",
      "Epoch 3/15\n",
      "21/21 [==============================] - 89s 4s/step - loss: 1.4547 - categorical_accuracy: 0.4925 - val_loss: 1.0027 - val_categorical_accuracy: 0.6500\n",
      "\n",
      "Epoch 00003: saving model to model_init_2021-02-0803_49_30.733147/model-00003-1.46054-0.49095-1.00272-0.65000.h5\n",
      "Epoch 4/15\n",
      "21/21 [==============================] - 88s 4s/step - loss: 1.2183 - categorical_accuracy: 0.5646 - val_loss: 1.1338 - val_categorical_accuracy: 0.5900\n",
      "\n",
      "Epoch 00004: saving model to model_init_2021-02-0803_49_30.733147/model-00004-1.22355-0.56259-1.13382-0.59000.h5\n",
      "Epoch 5/15\n",
      "21/21 [==============================] - 89s 4s/step - loss: 1.1140 - categorical_accuracy: 0.6068 - val_loss: 1.0071 - val_categorical_accuracy: 0.6500\n",
      "\n",
      "Epoch 00005: saving model to model_init_2021-02-0803_49_30.733147/model-00005-1.11090-0.60709-1.00713-0.65000.h5\n",
      "Epoch 6/15\n",
      "21/21 [==============================] - 89s 4s/step - loss: 0.9327 - categorical_accuracy: 0.6558 - val_loss: 1.1123 - val_categorical_accuracy: 0.6500\n",
      "\n",
      "Epoch 00006: saving model to model_init_2021-02-0803_49_30.733147/model-00006-0.93033-0.65611-1.11230-0.65000.h5\n",
      "Epoch 7/15\n",
      "21/21 [==============================] - 88s 4s/step - loss: 0.8637 - categorical_accuracy: 0.6918 - val_loss: 0.8466 - val_categorical_accuracy: 0.6900\n",
      "\n",
      "Epoch 00007: saving model to model_init_2021-02-0803_49_30.733147/model-00007-0.86790-0.69005-0.84663-0.69000.h5\n",
      "Epoch 8/15\n",
      "21/21 [==============================] - 88s 4s/step - loss: 0.7613 - categorical_accuracy: 0.7249 - val_loss: 0.7485 - val_categorical_accuracy: 0.7100\n",
      "\n",
      "Epoch 00008: saving model to model_init_2021-02-0803_49_30.733147/model-00008-0.76185-0.72474-0.74853-0.71000.h5\n",
      "Epoch 9/15\n",
      "21/21 [==============================] - 88s 4s/step - loss: 0.6432 - categorical_accuracy: 0.7647 - val_loss: 0.8471 - val_categorical_accuracy: 0.7100\n",
      "\n",
      "Epoch 00009: saving model to model_init_2021-02-0803_49_30.733147/model-00009-0.64476-0.76471-0.84707-0.71000.h5\n",
      "Epoch 10/15\n",
      "21/21 [==============================] - 88s 4s/step - loss: 0.6517 - categorical_accuracy: 0.7664 - val_loss: 0.9004 - val_categorical_accuracy: 0.7100\n",
      "\n",
      "Epoch 00010: saving model to model_init_2021-02-0803_49_30.733147/model-00010-0.65213-0.76621-0.90036-0.71000.h5\n",
      "Epoch 11/15\n",
      "21/21 [==============================] - 88s 4s/step - loss: 0.5719 - categorical_accuracy: 0.7995 - val_loss: 1.1250 - val_categorical_accuracy: 0.5500\n",
      "\n",
      "Epoch 00011: saving model to model_init_2021-02-0803_49_30.733147/model-00011-0.57361-0.79940-1.12495-0.55000.h5\n",
      "Epoch 12/15\n",
      "21/21 [==============================] - 89s 4s/step - loss: 0.5036 - categorical_accuracy: 0.8310 - val_loss: 1.0140 - val_categorical_accuracy: 0.6900\n",
      "\n",
      "Epoch 00012: saving model to model_init_2021-02-0803_49_30.733147/model-00012-0.49984-0.83333-1.01402-0.69000.h5\n",
      "\n",
      "Epoch 00012: ReduceLROnPlateau reducing learning rate to 3.9999998989515007e-05.\n",
      "Epoch 13/15\n",
      "21/21 [==============================] - 88s 4s/step - loss: 0.4470 - categorical_accuracy: 0.8407 - val_loss: 0.9642 - val_categorical_accuracy: 0.7100\n",
      "\n",
      "Epoch 00013: saving model to model_init_2021-02-0803_49_30.733147/model-00013-0.44771-0.84087-0.96420-0.71000.h5\n",
      "Epoch 14/15\n",
      "21/21 [==============================] - 87s 4s/step - loss: 0.4304 - categorical_accuracy: 0.8381 - val_loss: 0.8864 - val_categorical_accuracy: 0.7000\n",
      "\n",
      "Epoch 00014: saving model to model_init_2021-02-0803_49_30.733147/model-00014-0.43182-0.83710-0.88635-0.70000.h5\n",
      "Epoch 15/15\n",
      "21/21 [==============================] - 88s 4s/step - loss: 0.3838 - categorical_accuracy: 0.8567 - val_loss: 0.8870 - val_categorical_accuracy: 0.7000\n",
      "\n",
      "Epoch 00015: saving model to model_init_2021-02-0803_49_30.733147/model-00015-0.38523-0.85596-0.88699-0.70000.h5\n"
     ]
    }
   ],
   "source": [
    "print(\"Total Params:\", conv_3d8_model.count_params())\n",
    "history_model8=conv_3d8.train_model(conv_3d8_model,augment_data=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lets Try Augmented with simple GRU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 10 CNN2d and GRU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNNCNN10(ModelBuilderMoreAugmentation):\n",
    "    \n",
    "    def define_model(self,lstm_cells=64,dense_neurons=64,dropout=0.25):\n",
    "\n",
    "        model = Sequential()\n",
    "\n",
    "        model.add(TimeDistributed(Conv2D(16, (3, 3) , padding='same', activation='relu'),\n",
    "                                  input_shape=(self.frames_to_sample,self.image_height,self.image_width,self.channels)))\n",
    "        model.add(TimeDistributed(BatchNormalization()))\n",
    "        model.add(TimeDistributed(MaxPooling2D((2, 2))))\n",
    "        \n",
    "        model.add(TimeDistributed(Conv2D(32, (3, 3) , padding='same', activation='relu')))\n",
    "        model.add(TimeDistributed(BatchNormalization()))\n",
    "        model.add(TimeDistributed(MaxPooling2D((2, 2))))\n",
    "        \n",
    "\n",
    "        model.add(TimeDistributed(Flatten()))\n",
    "\n",
    "\n",
    "        model.add(GRU(lstm_cells))\n",
    "        model.add(Dropout(dropout))\n",
    "        \n",
    "        model.add(Dense(dense_neurons,activation='relu'))\n",
    "        model.add(Dropout(dropout))\n",
    "        \n",
    "        model.add(Dense(self.num_classes, activation='softmax'))\n",
    "        optimiser = optimizers.Adam(lr=0.0002)\n",
    "        model.compile(optimizer=optimiser, loss='categorical_crossentropy', metrics=['categorical_accuracy'])\n",
    "        return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "time_distributed_14 (TimeDis (None, 18, 120, 120, 16)  448       \n",
      "_________________________________________________________________\n",
      "time_distributed_15 (TimeDis (None, 18, 120, 120, 16)  64        \n",
      "_________________________________________________________________\n",
      "time_distributed_16 (TimeDis (None, 18, 60, 60, 16)    0         \n",
      "_________________________________________________________________\n",
      "time_distributed_17 (TimeDis (None, 18, 60, 60, 32)    4640      \n",
      "_________________________________________________________________\n",
      "time_distributed_18 (TimeDis (None, 18, 60, 60, 32)    128       \n",
      "_________________________________________________________________\n",
      "time_distributed_19 (TimeDis (None, 18, 30, 30, 32)    0         \n",
      "_________________________________________________________________\n",
      "time_distributed_20 (TimeDis (None, 18, 28800)         0         \n",
      "_________________________________________________________________\n",
      "gru_2 (GRU)                  (None, 128)               11108736  \n",
      "_________________________________________________________________\n",
      "dropout_11 (Dropout)         (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_55 (Dense)             (None, 128)               16512     \n",
      "_________________________________________________________________\n",
      "dropout_12 (Dropout)         (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_56 (Dense)             (None, 5)                 645       \n",
      "=================================================================\n",
      "Total params: 11,131,173\n",
      "Trainable params: 11,131,077\n",
      "Non-trainable params: 96\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "rnn_cnn10=RNNCNN10()\n",
    "rnn_cnn10.initialize_path(project_folder)\n",
    "rnn_cnn10.initialize_image_properties(image_height=120,image_width=120)\n",
    "rnn_cnn10.initialize_hyperparams(frames_to_sample=18,batch_size=32,num_epochs=12)\n",
    "rnn_cnn10_model=rnn_cnn10.define_model(lstm_cells=128,dense_neurons=128,dropout=0.25)\n",
    "rnn_cnn10_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Params: 11131173\n",
      "Epoch 1/12\n",
      "21/21 [==============================] - 107s 5s/step - loss: 1.4829 - categorical_accuracy: 0.3655 - val_loss: 1.1793 - val_categorical_accuracy: 0.5500\n",
      "\n",
      "Epoch 00001: saving model to model_init_2021-02-0804_14_47.851883/model-00001-1.48628-0.36350-1.17926-0.55000.h5\n",
      "Epoch 2/12\n",
      "21/21 [==============================] - 92s 4s/step - loss: 1.1495 - categorical_accuracy: 0.5332 - val_loss: 1.0337 - val_categorical_accuracy: 0.6300\n",
      "\n",
      "Epoch 00002: saving model to model_init_2021-02-0804_14_47.851883/model-00002-1.14916-0.53394-1.03371-0.63000.h5\n",
      "Epoch 3/12\n",
      "21/21 [==============================] - 97s 5s/step - loss: 0.9770 - categorical_accuracy: 0.6223 - val_loss: 1.0030 - val_categorical_accuracy: 0.6300\n",
      "\n",
      "Epoch 00003: saving model to model_init_2021-02-0804_14_47.851883/model-00003-0.97716-0.62217-1.00297-0.63000.h5\n",
      "Epoch 4/12\n",
      "21/21 [==============================] - 97s 5s/step - loss: 0.8199 - categorical_accuracy: 0.6958 - val_loss: 0.9868 - val_categorical_accuracy: 0.6100\n",
      "\n",
      "Epoch 00004: saving model to model_init_2021-02-0804_14_47.851883/model-00004-0.81946-0.69608-0.98683-0.61000.h5\n",
      "Epoch 5/12\n",
      "21/21 [==============================] - 98s 5s/step - loss: 0.7533 - categorical_accuracy: 0.7232 - val_loss: 0.9078 - val_categorical_accuracy: 0.6700\n",
      "\n",
      "Epoch 00005: saving model to model_init_2021-02-0804_14_47.851883/model-00005-0.75364-0.72247-0.90777-0.67000.h5\n",
      "Epoch 6/12\n",
      "21/21 [==============================] - 98s 5s/step - loss: 0.6553 - categorical_accuracy: 0.7753 - val_loss: 0.8561 - val_categorical_accuracy: 0.6600\n",
      "\n",
      "Epoch 00006: saving model to model_init_2021-02-0804_14_47.851883/model-00006-0.65443-0.77602-0.85607-0.66000.h5\n",
      "Epoch 7/12\n",
      "21/21 [==============================] - 97s 5s/step - loss: 0.5740 - categorical_accuracy: 0.8016 - val_loss: 0.8539 - val_categorical_accuracy: 0.6700\n",
      "\n",
      "Epoch 00007: saving model to model_init_2021-02-0804_14_47.851883/model-00007-0.57654-0.80015-0.85392-0.67000.h5\n",
      "Epoch 8/12\n",
      "21/21 [==============================] - 97s 5s/step - loss: 0.5471 - categorical_accuracy: 0.8060 - val_loss: 0.8059 - val_categorical_accuracy: 0.7300\n",
      "\n",
      "Epoch 00008: saving model to model_init_2021-02-0804_14_47.851883/model-00008-0.54913-0.80543-0.80595-0.73000.h5\n",
      "Epoch 9/12\n",
      "21/21 [==============================] - 97s 5s/step - loss: 0.4587 - categorical_accuracy: 0.8508 - val_loss: 0.8042 - val_categorical_accuracy: 0.7000\n",
      "\n",
      "Epoch 00009: saving model to model_init_2021-02-0804_14_47.851883/model-00009-0.45686-0.85143-0.80421-0.70000.h5\n",
      "Epoch 10/12\n",
      "21/21 [==============================] - 98s 5s/step - loss: 0.4434 - categorical_accuracy: 0.8454 - val_loss: 0.9408 - val_categorical_accuracy: 0.6500\n",
      "\n",
      "Epoch 00010: saving model to model_init_2021-02-0804_14_47.851883/model-00010-0.44328-0.84540-0.94078-0.65000.h5\n",
      "Epoch 11/12\n",
      "21/21 [==============================] - 98s 5s/step - loss: 0.3961 - categorical_accuracy: 0.8756 - val_loss: 0.8367 - val_categorical_accuracy: 0.6700\n",
      "\n",
      "Epoch 00011: saving model to model_init_2021-02-0804_14_47.851883/model-00011-0.39634-0.87481-0.83668-0.67000.h5\n",
      "Epoch 12/12\n",
      "21/21 [==============================] - 97s 5s/step - loss: 0.3425 - categorical_accuracy: 0.8999 - val_loss: 0.7774 - val_categorical_accuracy: 0.6800\n",
      "\n",
      "Epoch 00012: saving model to model_init_2021-02-0804_14_47.851883/model-00012-0.34328-0.89970-0.77740-0.68000.h5\n"
     ]
    }
   ],
   "source": [
    "print(\"Total Params:\", rnn_cnn10_model.count_params())\n",
    "history_model10=rnn_cnn10.train_model(rnn_cnn10_model,augment_data=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 11 CNN2d and GRU with simpler architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNNCNN11(ModelBuilderMoreAugmentation):\n",
    "    \n",
    "    def define_model(self,lstm_cells=64,dense_neurons=64,dropout=0.25):\n",
    "\n",
    "        model = Sequential()\n",
    "\n",
    "        model.add(TimeDistributed(Conv2D(16, (3, 3) , padding='same', activation='relu'),\n",
    "                                  input_shape=(self.frames_to_sample,self.image_height,self.image_width,self.channels)))\n",
    "        model.add(TimeDistributed(BatchNormalization()))\n",
    "        model.add(TimeDistributed(MaxPooling2D((2, 2))))\n",
    "        \n",
    "        model.add(TimeDistributed(Conv2D(32, (3, 3) , padding='same', activation='relu')))\n",
    "        model.add(TimeDistributed(BatchNormalization()))\n",
    "        model.add(TimeDistributed(MaxPooling2D((2, 2))))\n",
    "        \n",
    "        model.add(TimeDistributed(Conv2D(64, (3, 3) , padding='same', activation='relu')))\n",
    "        model.add(TimeDistributed(BatchNormalization()))\n",
    "        model.add(TimeDistributed(MaxPooling2D((2, 2))))\n",
    "\n",
    "        model.add(TimeDistributed(Conv2D(128, (3, 3) , padding='same', activation='relu')))\n",
    "        model.add(TimeDistributed(BatchNormalization()))\n",
    "        model.add(TimeDistributed(MaxPooling2D((2, 2))))\n",
    "\n",
    "        model.add(TimeDistributed(Flatten()))\n",
    "\n",
    "\n",
    "        model.add(GRU(128))\n",
    "        model.add(Dropout(dropout))\n",
    "        #model.add(GRU(128))\n",
    "\n",
    "        model.add(Dense(dense_neurons,activation='relu'))\n",
    "        model.add(Dropout(dropout))\n",
    "        model.add(Dense(dense_neurons,activation='relu'))\n",
    "        model.add(Dense(dense_neurons,activation='relu'))\n",
    "\n",
    "        model.add(Dense(self.num_classes, activation='softmax'))\n",
    "        optimiser = optimizers.Adam(lr=0.0002)\n",
    "        model.compile(optimizer=optimiser, loss='categorical_crossentropy', metrics=['categorical_accuracy'])\n",
    "        return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "time_distributed_47 (TimeDis (None, 18, 120, 120, 16)  448       \n",
      "_________________________________________________________________\n",
      "time_distributed_48 (TimeDis (None, 18, 120, 120, 16)  64        \n",
      "_________________________________________________________________\n",
      "time_distributed_49 (TimeDis (None, 18, 60, 60, 16)    0         \n",
      "_________________________________________________________________\n",
      "time_distributed_50 (TimeDis (None, 18, 60, 60, 32)    4640      \n",
      "_________________________________________________________________\n",
      "time_distributed_51 (TimeDis (None, 18, 60, 60, 32)    128       \n",
      "_________________________________________________________________\n",
      "time_distributed_52 (TimeDis (None, 18, 30, 30, 32)    0         \n",
      "_________________________________________________________________\n",
      "time_distributed_53 (TimeDis (None, 18, 30, 30, 64)    18496     \n",
      "_________________________________________________________________\n",
      "time_distributed_54 (TimeDis (None, 18, 30, 30, 64)    256       \n",
      "_________________________________________________________________\n",
      "time_distributed_55 (TimeDis (None, 18, 15, 15, 64)    0         \n",
      "_________________________________________________________________\n",
      "time_distributed_56 (TimeDis (None, 18, 15, 15, 128)   73856     \n",
      "_________________________________________________________________\n",
      "time_distributed_57 (TimeDis (None, 18, 15, 15, 128)   512       \n",
      "_________________________________________________________________\n",
      "time_distributed_58 (TimeDis (None, 18, 7, 7, 128)     0         \n",
      "_________________________________________________________________\n",
      "time_distributed_59 (TimeDis (None, 18, 6272)          0         \n",
      "_________________________________________________________________\n",
      "gru_5 (GRU)                  (None, 128)               2457984   \n",
      "_________________________________________________________________\n",
      "dropout_17 (Dropout)         (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_65 (Dense)             (None, 128)               16512     \n",
      "_________________________________________________________________\n",
      "dropout_18 (Dropout)         (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_66 (Dense)             (None, 128)               16512     \n",
      "_________________________________________________________________\n",
      "dense_67 (Dense)             (None, 128)               16512     \n",
      "_________________________________________________________________\n",
      "dense_68 (Dense)             (None, 5)                 645       \n",
      "=================================================================\n",
      "Total params: 2,606,565\n",
      "Trainable params: 2,606,085\n",
      "Non-trainable params: 480\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "rnn_cnn11=RNNCNN11()\n",
    "rnn_cnn11.initialize_path(project_folder)\n",
    "rnn_cnn11.initialize_image_properties(image_height=120,image_width=120)\n",
    "rnn_cnn11.initialize_hyperparams(frames_to_sample=18,batch_size=32,num_epochs=22)\n",
    "rnn_cnn11_model=rnn_cnn11.define_model(lstm_cells=128,dense_neurons=128,dropout=0.25)\n",
    "rnn_cnn11_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Params: 2606565\n",
      "Epoch 1/22\n",
      "21/21 [==============================] - 108s 5s/step - loss: 1.5535 - categorical_accuracy: 0.3073 - val_loss: 1.4348 - val_categorical_accuracy: 0.4100\n",
      "\n",
      "Epoch 00001: saving model to model_init_2021-02-0805_42_12.453972/model-00001-1.55581-0.30618-1.43479-0.41000.h5\n",
      "Epoch 2/22\n",
      "21/21 [==============================] - 91s 4s/step - loss: 1.3254 - categorical_accuracy: 0.4687 - val_loss: 1.2110 - val_categorical_accuracy: 0.5600\n",
      "\n",
      "Epoch 00002: saving model to model_init_2021-02-0805_42_12.453972/model-00002-1.32485-0.46833-1.21101-0.56000.h5\n",
      "Epoch 3/22\n",
      "21/21 [==============================] - 100s 5s/step - loss: 1.1572 - categorical_accuracy: 0.5137 - val_loss: 1.1103 - val_categorical_accuracy: 0.5900\n",
      "\n",
      "Epoch 00003: saving model to model_init_2021-02-0805_42_12.453972/model-00003-1.15744-0.51357-1.11030-0.59000.h5\n",
      "Epoch 4/22\n",
      "21/21 [==============================] - 99s 5s/step - loss: 0.9692 - categorical_accuracy: 0.6335 - val_loss: 0.9655 - val_categorical_accuracy: 0.6000\n",
      "\n",
      "Epoch 00004: saving model to model_init_2021-02-0805_42_12.453972/model-00004-0.96685-0.63499-0.96547-0.60000.h5\n",
      "Epoch 5/22\n",
      "21/21 [==============================] - 98s 5s/step - loss: 0.8039 - categorical_accuracy: 0.6948 - val_loss: 0.9622 - val_categorical_accuracy: 0.5900\n",
      "\n",
      "Epoch 00005: saving model to model_init_2021-02-0805_42_12.453972/model-00005-0.80253-0.69532-0.96216-0.59000.h5\n",
      "Epoch 6/22\n",
      "21/21 [==============================] - 99s 5s/step - loss: 0.6945 - categorical_accuracy: 0.7379 - val_loss: 0.8351 - val_categorical_accuracy: 0.6700\n",
      "\n",
      "Epoch 00006: saving model to model_init_2021-02-0805_42_12.453972/model-00006-0.69729-0.73680-0.83514-0.67000.h5\n",
      "Epoch 7/22\n",
      "21/21 [==============================] - 98s 5s/step - loss: 0.5793 - categorical_accuracy: 0.7849 - val_loss: 0.7391 - val_categorical_accuracy: 0.7000\n",
      "\n",
      "Epoch 00007: saving model to model_init_2021-02-0805_42_12.453972/model-00007-0.57857-0.78582-0.73908-0.70000.h5\n",
      "Epoch 8/22\n",
      "21/21 [==============================] - 98s 5s/step - loss: 0.4876 - categorical_accuracy: 0.8311 - val_loss: 0.9264 - val_categorical_accuracy: 0.6700\n",
      "\n",
      "Epoch 00008: saving model to model_init_2021-02-0805_42_12.453972/model-00008-0.48174-0.83258-0.92638-0.67000.h5\n",
      "Epoch 9/22\n",
      "21/21 [==============================] - 98s 5s/step - loss: 0.3902 - categorical_accuracy: 0.8667 - val_loss: 0.7010 - val_categorical_accuracy: 0.7200\n",
      "\n",
      "Epoch 00009: saving model to model_init_2021-02-0805_42_12.453972/model-00009-0.39083-0.86727-0.70100-0.72000.h5\n",
      "Epoch 10/22\n",
      "21/21 [==============================] - 98s 5s/step - loss: 0.3056 - categorical_accuracy: 0.8997 - val_loss: 0.7446 - val_categorical_accuracy: 0.7200\n",
      "\n",
      "Epoch 00010: saving model to model_init_2021-02-0805_42_12.453972/model-00010-0.30636-0.89894-0.74456-0.72000.h5\n",
      "Epoch 11/22\n",
      "21/21 [==============================] - 98s 5s/step - loss: 0.2247 - categorical_accuracy: 0.9291 - val_loss: 0.8168 - val_categorical_accuracy: 0.7000\n",
      "\n",
      "Epoch 00011: saving model to model_init_2021-02-0805_42_12.453972/model-00011-0.22276-0.92986-0.81683-0.70000.h5\n",
      "Epoch 12/22\n",
      "21/21 [==============================] - 98s 5s/step - loss: 0.1837 - categorical_accuracy: 0.9393 - val_loss: 0.6961 - val_categorical_accuracy: 0.7500\n",
      "\n",
      "Epoch 00012: saving model to model_init_2021-02-0805_42_12.453972/model-00012-0.18390-0.93967-0.69609-0.75000.h5\n",
      "Epoch 13/22\n",
      "21/21 [==============================] - 98s 5s/step - loss: 0.1683 - categorical_accuracy: 0.9493 - val_loss: 0.6675 - val_categorical_accuracy: 0.7700\n",
      "\n",
      "Epoch 00013: saving model to model_init_2021-02-0805_42_12.453972/model-00013-0.16749-0.94947-0.66752-0.77000.h5\n",
      "Epoch 14/22\n",
      "21/21 [==============================] - 98s 5s/step - loss: 0.1541 - categorical_accuracy: 0.9498 - val_loss: 0.7668 - val_categorical_accuracy: 0.7600\n",
      "\n",
      "Epoch 00014: saving model to model_init_2021-02-0805_42_12.453972/model-00014-0.15475-0.94947-0.76679-0.76000.h5\n",
      "Epoch 15/22\n",
      "21/21 [==============================] - 97s 5s/step - loss: 0.1301 - categorical_accuracy: 0.9647 - val_loss: 0.7761 - val_categorical_accuracy: 0.7200\n",
      "\n",
      "Epoch 00015: saving model to model_init_2021-02-0805_42_12.453972/model-00015-0.13055-0.96456-0.77606-0.72000.h5\n",
      "Epoch 16/22\n",
      "21/21 [==============================] - 97s 5s/step - loss: 0.1028 - categorical_accuracy: 0.9685 - val_loss: 0.9094 - val_categorical_accuracy: 0.7400\n",
      "\n",
      "Epoch 00016: saving model to model_init_2021-02-0805_42_12.453972/model-00016-0.10301-0.96833-0.90942-0.74000.h5\n",
      "Epoch 17/22\n",
      "21/21 [==============================] - 97s 5s/step - loss: 0.0877 - categorical_accuracy: 0.9759 - val_loss: 0.8626 - val_categorical_accuracy: 0.7600\n",
      "\n",
      "Epoch 00017: saving model to model_init_2021-02-0805_42_12.453972/model-00017-0.08823-0.97587-0.86261-0.76000.h5\n",
      "\n",
      "Epoch 00017: ReduceLROnPlateau reducing learning rate to 3.9999998989515007e-05.\n",
      "Epoch 18/22\n",
      "21/21 [==============================] - 95s 5s/step - loss: 0.0666 - categorical_accuracy: 0.9814 - val_loss: 0.7267 - val_categorical_accuracy: 0.7800\n",
      "\n",
      "Epoch 00018: saving model to model_init_2021-02-0805_42_12.453972/model-00018-0.06705-0.98115-0.72672-0.78000.h5\n",
      "Epoch 19/22\n",
      "21/21 [==============================] - 98s 5s/step - loss: 0.0585 - categorical_accuracy: 0.9868 - val_loss: 0.8011 - val_categorical_accuracy: 0.7600\n",
      "\n",
      "Epoch 00019: saving model to model_init_2021-02-0805_42_12.453972/model-00019-0.05765-0.98718-0.80109-0.76000.h5\n",
      "Epoch 20/22\n",
      "21/21 [==============================] - 97s 5s/step - loss: 0.0580 - categorical_accuracy: 0.9836 - val_loss: 0.8306 - val_categorical_accuracy: 0.7500\n",
      "\n",
      "Epoch 00020: saving model to model_init_2021-02-0805_42_12.453972/model-00020-0.05812-0.98341-0.83061-0.75000.h5\n",
      "Epoch 21/22\n",
      "21/21 [==============================] - 97s 5s/step - loss: 0.0588 - categorical_accuracy: 0.9804 - val_loss: 0.8412 - val_categorical_accuracy: 0.7500\n",
      "\n",
      "Epoch 00021: saving model to model_init_2021-02-0805_42_12.453972/model-00021-0.05856-0.98039-0.84122-0.75000.h5\n",
      "\n",
      "Epoch 00021: ReduceLROnPlateau reducing learning rate to 7.999999797903002e-06.\n",
      "Epoch 22/22\n",
      "21/21 [==============================] - 98s 5s/step - loss: 0.0433 - categorical_accuracy: 0.9888 - val_loss: 0.8200 - val_categorical_accuracy: 0.7500\n",
      "\n",
      "Epoch 00022: saving model to model_init_2021-02-0805_42_12.453972/model-00022-0.04356-0.98869-0.82002-0.75000.h5\n"
     ]
    }
   ],
   "source": [
    "print(\"Total Params:\", rnn_cnn11_model.count_params())\n",
    "history_model11=rnn_cnn11.train_model(rnn_cnn11_model,augment_data=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Till now this is one of the best training and validation accuracy achieved"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lets use Transfer Learning "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 12 with mobilenet v2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.applications import MobileNetV2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "mobilenetV2_transfer = MobileNetV2(weights='imagenet', include_top=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNNCNN_mobile_net(ModelBuilderMoreAugmentation):\n",
    "    \n",
    "    def define_model(self,lstm_cells=64,dense_neurons=64,dropout=0.25):\n",
    "        \n",
    "        model = Sequential()\n",
    "        model.add(TimeDistributed(mobilenetV2_transfer,input_shape=(self.frames_to_sample,self.image_height,self.image_width,self.channels)))\n",
    "        \n",
    "        \n",
    "        for layer in model.layers:\n",
    "            layer.trainable = False\n",
    "        \n",
    "        \n",
    "        model.add(TimeDistributed(BatchNormalization()))\n",
    "        model.add(TimeDistributed(MaxPooling2D((2, 2))))\n",
    "        model.add(TimeDistributed(Flatten()))\n",
    "\n",
    "        model.add(LSTM(lstm_cells))\n",
    "        model.add(Dropout(dropout))\n",
    "        \n",
    "        model.add(Dense(dense_neurons,activation='relu'))\n",
    "        model.add(Dropout(dropout))\n",
    "        \n",
    "        model.add(Dense(self.num_classes, activation='softmax'))\n",
    "        \n",
    "        \n",
    "        optimiser = optimizers.Adam()\n",
    "        model.compile(optimizer=optimiser, loss='categorical_crossentropy', metrics=['categorical_accuracy'])\n",
    "        return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "time_distributed_60 (TimeDis (None, 16, 4, 4, 1280)    2257984   \n",
      "_________________________________________________________________\n",
      "time_distributed_61 (TimeDis (None, 16, 4, 4, 1280)    5120      \n",
      "_________________________________________________________________\n",
      "time_distributed_62 (TimeDis (None, 16, 2, 2, 1280)    0         \n",
      "_________________________________________________________________\n",
      "time_distributed_63 (TimeDis (None, 16, 5120)          0         \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 128)               2687488   \n",
      "_________________________________________________________________\n",
      "dropout_19 (Dropout)         (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_69 (Dense)             (None, 128)               16512     \n",
      "_________________________________________________________________\n",
      "dropout_20 (Dropout)         (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_70 (Dense)             (None, 5)                 645       \n",
      "=================================================================\n",
      "Total params: 4,967,749\n",
      "Trainable params: 2,707,205\n",
      "Non-trainable params: 2,260,544\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "rnn_cnn_mn_v2=RNNCNN_mobile_net()\n",
    "rnn_cnn_mn_v2.initialize_path(project_folder)\n",
    "rnn_cnn_mn_v2.initialize_image_properties(image_height=120,image_width=120)\n",
    "rnn_cnn_mn_v2.initialize_hyperparams(frames_to_sample=16,batch_size=5,num_epochs=10)\n",
    "rnn_cnn_mn_v2_model=rnn_cnn_mn_v2.define_model(lstm_cells=128,dense_neurons=128,dropout=0.25)\n",
    "rnn_cnn_mn_v2_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Params: 4967749\n",
      "Epoch 1/10\n",
      "133/133 [==============================] - 104s 781ms/step - loss: 1.4712 - categorical_accuracy: 0.3727 - val_loss: 1.4045 - val_categorical_accuracy: 0.3700\n",
      "\n",
      "Epoch 00001: saving model to model_init_2021-02-0806_24_25.928698/model-00001-1.47020-0.37330-1.40447-0.37000.h5\n",
      "Epoch 2/10\n",
      "133/133 [==============================] - 80s 604ms/step - loss: 1.2128 - categorical_accuracy: 0.4830 - val_loss: 1.2493 - val_categorical_accuracy: 0.4700\n",
      "\n",
      "Epoch 00002: saving model to model_init_2021-02-0806_24_25.928698/model-00002-1.21258-0.48341-1.24933-0.47000.h5\n",
      "Epoch 3/10\n",
      "133/133 [==============================] - 87s 653ms/step - loss: 1.1243 - categorical_accuracy: 0.5353 - val_loss: 1.4023 - val_categorical_accuracy: 0.4000\n",
      "\n",
      "Epoch 00003: saving model to model_init_2021-02-0806_24_25.928698/model-00003-1.12344-0.53544-1.40227-0.40000.h5\n",
      "Epoch 4/10\n",
      "133/133 [==============================] - 83s 621ms/step - loss: 1.0233 - categorical_accuracy: 0.5787 - val_loss: 1.4600 - val_categorical_accuracy: 0.4300\n",
      "\n",
      "Epoch 00004: saving model to model_init_2021-02-0806_24_25.928698/model-00004-1.02458-0.57843-1.45999-0.43000.h5\n",
      "Epoch 5/10\n",
      "133/133 [==============================] - 82s 616ms/step - loss: 0.9238 - categorical_accuracy: 0.6391 - val_loss: 1.5700 - val_categorical_accuracy: 0.3300\n",
      "\n",
      "Epoch 00005: saving model to model_init_2021-02-0806_24_25.928698/model-00005-0.92292-0.63952-1.57003-0.33000.h5\n",
      "Epoch 6/10\n",
      "133/133 [==============================] - 82s 617ms/step - loss: 0.8907 - categorical_accuracy: 0.6622 - val_loss: 1.2062 - val_categorical_accuracy: 0.5000\n",
      "\n",
      "Epoch 00006: saving model to model_init_2021-02-0806_24_25.928698/model-00006-0.88997-0.66214-1.20622-0.50000.h5\n",
      "Epoch 7/10\n",
      "133/133 [==============================] - 82s 616ms/step - loss: 0.8893 - categorical_accuracy: 0.6509 - val_loss: 1.4431 - val_categorical_accuracy: 0.4000\n",
      "\n",
      "Epoch 00007: saving model to model_init_2021-02-0806_24_25.928698/model-00007-0.88905-0.65083-1.44310-0.40000.h5\n",
      "Epoch 8/10\n",
      "133/133 [==============================] - 83s 625ms/step - loss: 0.8075 - categorical_accuracy: 0.6897 - val_loss: 1.4349 - val_categorical_accuracy: 0.3500\n",
      "\n",
      "Epoch 00008: saving model to model_init_2021-02-0806_24_25.928698/model-00008-0.80704-0.68929-1.43493-0.35000.h5\n",
      "Epoch 9/10\n",
      "133/133 [==============================] - 85s 642ms/step - loss: 0.8506 - categorical_accuracy: 0.6762 - val_loss: 1.8743 - val_categorical_accuracy: 0.2800\n",
      "\n",
      "Epoch 00009: saving model to model_init_2021-02-0806_24_25.928698/model-00009-0.84926-0.67722-1.87430-0.28000.h5\n",
      "Epoch 10/10\n",
      "133/133 [==============================] - 83s 622ms/step - loss: 0.8146 - categorical_accuracy: 0.6702 - val_loss: 1.3911 - val_categorical_accuracy: 0.4200\n",
      "\n",
      "Epoch 00010: saving model to model_init_2021-02-0806_24_25.928698/model-00010-0.81510-0.66968-1.39114-0.42000.h5\n",
      "\n",
      "Epoch 00010: ReduceLROnPlateau reducing learning rate to 0.00020000000949949026.\n"
     ]
    }
   ],
   "source": [
    "print(\"Total Params:\", rnn_cnn_mn_v2_model.count_params())\n",
    "history_model12=rnn_cnn_mn_v2.train_model(rnn_cnn_mn_v2_model,augment_data=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### PRe trained weights doesnt give good results, lets try and train all the weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 13 with mobilenet v2 training all the weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "mobilenetV2_transfer = MobileNetV2(weights='imagenet', include_top=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "          \n",
    "class RNNCNN_tain_weights(ModelBuilderMoreAugmentation):\n",
    "    \n",
    "    def define_model(self,gru_cells=64,dense_neurons=64,dropout=0.25):\n",
    "        \n",
    "        model = Sequential()\n",
    "        model.add(TimeDistributed(mobilenetV2_transfer,input_shape=(self.frames_to_sample,self.image_height,self.image_width,self.channels)))\n",
    " \n",
    "        \n",
    "        model.add(TimeDistributed(BatchNormalization()))\n",
    "        model.add(TimeDistributed(MaxPooling2D((2, 2))))\n",
    "        model.add(TimeDistributed(Flatten()))\n",
    "\n",
    "        model.add(GRU(gru_cells))\n",
    "        model.add(Dropout(dropout))\n",
    "        \n",
    "        model.add(Dense(dense_neurons,activation='relu'))\n",
    "        model.add(Dropout(dropout))\n",
    "        \n",
    "        model.add(Dense(self.num_classes, activation='softmax'))\n",
    "        \n",
    "        \n",
    "        optimiser = optimizers.Adam()\n",
    "        model.compile(optimizer=optimiser, loss='categorical_crossentropy', metrics=['categorical_accuracy'])\n",
    "        return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "time_distributed_1 (TimeDist (None, 16, 4, 4, 1280)    2257984   \n",
      "_________________________________________________________________\n",
      "time_distributed_2 (TimeDist (None, 16, 4, 4, 1280)    5120      \n",
      "_________________________________________________________________\n",
      "time_distributed_3 (TimeDist (None, 16, 2, 2, 1280)    0         \n",
      "_________________________________________________________________\n",
      "time_distributed_4 (TimeDist (None, 16, 5120)          0         \n",
      "_________________________________________________________________\n",
      "gru_1 (GRU)                  (None, 128)               2015616   \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 128)               16512     \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 5)                 645       \n",
      "=================================================================\n",
      "Total params: 4,295,877\n",
      "Trainable params: 4,259,205\n",
      "Non-trainable params: 36,672\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "rnn_cnn_13=RNNCNN_tain_weights()\n",
    "rnn_cnn_13.initialize_path(project_folder)\n",
    "rnn_cnn_13.initialize_image_properties(image_height=120,image_width=120)\n",
    "rnn_cnn_13.initialize_hyperparams(frames_to_sample=16,batch_size=5,num_epochs=10)\n",
    "rnn_cnn_13_model=rnn_cnn_13.define_model(gru_cells=128,dense_neurons=128,dropout=0.25)\n",
    "rnn_cnn_13_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Params: 4295877\n",
      "Epoch 1/10\n",
      "133/133 [==============================] - 115s 864ms/step - loss: 1.3298 - categorical_accuracy: 0.4639 - val_loss: 0.8398 - val_categorical_accuracy: 0.6900\n",
      "\n",
      "Epoch 00001: saving model to model_init_2021-02-0806_58_46.674261/model-00001-1.32957-0.46380-0.83982-0.69000.h5\n",
      "Epoch 2/10\n",
      "133/133 [==============================] - 97s 730ms/step - loss: 1.0957 - categorical_accuracy: 0.5582 - val_loss: 1.1752 - val_categorical_accuracy: 0.4600\n",
      "\n",
      "Epoch 00002: saving model to model_init_2021-02-0806_58_46.674261/model-00002-1.09208-0.55882-1.17523-0.46000.h5\n",
      "Epoch 3/10\n",
      "133/133 [==============================] - 95s 712ms/step - loss: 0.9375 - categorical_accuracy: 0.6190 - val_loss: 1.1691 - val_categorical_accuracy: 0.5200\n",
      "\n",
      "Epoch 00003: saving model to model_init_2021-02-0806_58_46.674261/model-00003-0.93885-0.61840-1.16911-0.52000.h5\n",
      "Epoch 4/10\n",
      "133/133 [==============================] - 97s 727ms/step - loss: 0.7708 - categorical_accuracy: 0.6840 - val_loss: 0.7720 - val_categorical_accuracy: 0.6900\n",
      "\n",
      "Epoch 00004: saving model to model_init_2021-02-0806_58_46.674261/model-00004-0.77115-0.68401-0.77195-0.69000.h5\n",
      "Epoch 5/10\n",
      "133/133 [==============================] - 94s 708ms/step - loss: 0.7790 - categorical_accuracy: 0.6699 - val_loss: 1.6509 - val_categorical_accuracy: 0.4000\n",
      "\n",
      "Epoch 00005: saving model to model_init_2021-02-0806_58_46.674261/model-00005-0.78045-0.66893-1.65086-0.40000.h5\n",
      "Epoch 6/10\n",
      "133/133 [==============================] - 97s 728ms/step - loss: 0.7716 - categorical_accuracy: 0.6789 - val_loss: 1.4029 - val_categorical_accuracy: 0.5000\n",
      "\n",
      "Epoch 00006: saving model to model_init_2021-02-0806_58_46.674261/model-00006-0.77319-0.67798-1.40286-0.50000.h5\n",
      "Epoch 7/10\n",
      "133/133 [==============================] - 97s 727ms/step - loss: 0.7426 - categorical_accuracy: 0.6985 - val_loss: 1.2553 - val_categorical_accuracy: 0.5200\n",
      "\n",
      "Epoch 00007: saving model to model_init_2021-02-0806_58_46.674261/model-00007-0.74124-0.69910-1.25530-0.52000.h5\n",
      "Epoch 8/10\n",
      "133/133 [==============================] - 97s 728ms/step - loss: 0.6337 - categorical_accuracy: 0.7379 - val_loss: 1.7181 - val_categorical_accuracy: 0.5400\n",
      "\n",
      "Epoch 00008: saving model to model_init_2021-02-0806_58_46.674261/model-00008-0.63248-0.73906-1.71808-0.54000.h5\n",
      "\n",
      "Epoch 00008: ReduceLROnPlateau reducing learning rate to 0.00020000000949949026.\n",
      "Epoch 9/10\n",
      "133/133 [==============================] - 96s 719ms/step - loss: 0.4947 - categorical_accuracy: 0.7822 - val_loss: 0.9817 - val_categorical_accuracy: 0.6400\n",
      "\n",
      "Epoch 00009: saving model to model_init_2021-02-0806_58_46.674261/model-00009-0.49389-0.78205-0.98170-0.64000.h5\n",
      "Epoch 10/10\n",
      "133/133 [==============================] - 94s 703ms/step - loss: 0.4245 - categorical_accuracy: 0.8055 - val_loss: 0.6906 - val_categorical_accuracy: 0.7300\n",
      "\n",
      "Epoch 00010: saving model to model_init_2021-02-0806_58_46.674261/model-00010-0.42465-0.80543-0.69060-0.73000.h5\n"
     ]
    }
   ],
   "source": [
    "print(\"Total Params:\", rnn_cnn_13_model.count_params())\n",
    "history_model13=rnn_cnn_13.train_model(rnn_cnn_13_model,augment_data=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lets Try Mobile net V1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 14 with mobilenet v1 training all the weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://github.com/fchollet/deep-learning-models/releases/download/v0.6/mobilenet_1_0_224_tf_no_top.h5\n",
      "17227776/17225924 [==============================] - 1s 0us/step\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from keras.applications import mobilenet\n",
    "mobilenet_transfer =  mobilenet.MobileNet(weights='imagenet', include_top=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNNCNN_tain_weights_14(ModelBuilderMoreAugmentation):\n",
    "    \n",
    "    def define_model(self,gru_cells=64,dense_neurons=64,dropout=0.25):\n",
    "        \n",
    "        model = Sequential()\n",
    "        model.add(TimeDistributed(mobilenet_transfer,input_shape=(self.frames_to_sample,self.image_height,self.image_width,self.channels)))\n",
    " \n",
    "        \n",
    "        model.add(TimeDistributed(BatchNormalization()))\n",
    "        model.add(TimeDistributed(MaxPooling2D((2, 2))))\n",
    "        model.add(TimeDistributed(Flatten()))\n",
    "\n",
    "        model.add(GRU(gru_cells))\n",
    "        model.add(Dropout(dropout))\n",
    "        \n",
    "        model.add(Dense(dense_neurons,activation='relu'))\n",
    "        model.add(Dropout(dropout))\n",
    "        \n",
    "        model.add(Dense(self.num_classes, activation='softmax'))\n",
    "        \n",
    "        \n",
    "        optimiser = optimizers.Adam()\n",
    "        model.compile(optimizer=optimiser, loss='categorical_crossentropy', metrics=['categorical_accuracy'])\n",
    "        return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "time_distributed_5 (TimeDist (None, 16, 4, 4, 1280)    2257984   \n",
      "_________________________________________________________________\n",
      "time_distributed_6 (TimeDist (None, 16, 4, 4, 1280)    5120      \n",
      "_________________________________________________________________\n",
      "time_distributed_7 (TimeDist (None, 16, 2, 2, 1280)    0         \n",
      "_________________________________________________________________\n",
      "time_distributed_8 (TimeDist (None, 16, 5120)          0         \n",
      "_________________________________________________________________\n",
      "gru_2 (GRU)                  (None, 128)               2015616   \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 128)               16512     \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 5)                 645       \n",
      "=================================================================\n",
      "Total params: 4,295,877\n",
      "Trainable params: 4,259,205\n",
      "Non-trainable params: 36,672\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "rnn_cnn_14=RNNCNN_tain_weights()\n",
    "rnn_cnn_14.initialize_path(project_folder)\n",
    "rnn_cnn_14.initialize_image_properties(image_height=120,image_width=120)\n",
    "rnn_cnn_14.initialize_hyperparams(frames_to_sample=16,batch_size=5,num_epochs=15)\n",
    "rnn_cnn_14_model=rnn_cnn_14.define_model(gru_cells=128,dense_neurons=128,dropout=0.25)\n",
    "rnn_cnn_14_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Params: 4295877\n",
      "Epoch 1/15\n",
      "133/133 [==============================] - 112s 844ms/step - loss: 0.9503 - categorical_accuracy: 0.6461 - val_loss: 1.0527 - val_categorical_accuracy: 0.5800\n",
      "\n",
      "Epoch 00001: saving model to model_init_2021-02-0807_27_51.966986/model-00001-0.95046-0.64555-1.05265-0.58000.h5\n",
      "Epoch 2/15\n",
      "133/133 [==============================] - 96s 725ms/step - loss: 0.9476 - categorical_accuracy: 0.6196 - val_loss: 1.5255 - val_categorical_accuracy: 0.4300\n",
      "\n",
      "Epoch 00002: saving model to model_init_2021-02-0807_27_51.966986/model-00002-0.94260-0.62142-1.52546-0.43000.h5\n",
      "Epoch 3/15\n",
      "133/133 [==============================] - 94s 707ms/step - loss: 0.8016 - categorical_accuracy: 0.6531 - val_loss: 1.7509 - val_categorical_accuracy: 0.3200\n",
      "\n",
      "Epoch 00003: saving model to model_init_2021-02-0807_27_51.966986/model-00003-0.80260-0.65309-1.75094-0.32000.h5\n",
      "Epoch 4/15\n",
      "133/133 [==============================] - 94s 709ms/step - loss: 0.9027 - categorical_accuracy: 0.6190 - val_loss: 1.4398 - val_categorical_accuracy: 0.4000\n",
      "\n",
      "Epoch 00004: saving model to model_init_2021-02-0807_27_51.966986/model-00004-0.90298-0.61840-1.43978-0.40000.h5\n",
      "Epoch 5/15\n",
      "133/133 [==============================] - 94s 705ms/step - loss: 0.9790 - categorical_accuracy: 0.5950 - val_loss: 1.5928 - val_categorical_accuracy: 0.3100\n",
      "\n",
      "Epoch 00005: saving model to model_init_2021-02-0807_27_51.966986/model-00005-0.97850-0.59578-1.59282-0.31000.h5\n",
      "\n",
      "Epoch 00005: ReduceLROnPlateau reducing learning rate to 0.00020000000949949026.\n",
      "Epoch 6/15\n",
      "133/133 [==============================] - 96s 725ms/step - loss: 0.8294 - categorical_accuracy: 0.6396 - val_loss: 0.9917 - val_categorical_accuracy: 0.6000\n",
      "\n",
      "Epoch 00006: saving model to model_init_2021-02-0807_27_51.966986/model-00006-0.82995-0.63952-0.99172-0.60000.h5\n",
      "Epoch 7/15\n",
      "133/133 [==============================] - 94s 704ms/step - loss: 0.6885 - categorical_accuracy: 0.6837 - val_loss: 0.7308 - val_categorical_accuracy: 0.7000\n",
      "\n",
      "Epoch 00007: saving model to model_init_2021-02-0807_27_51.966986/model-00007-0.68700-0.68477-0.73083-0.70000.h5\n",
      "Epoch 8/15\n",
      "133/133 [==============================] - 93s 701ms/step - loss: 0.6093 - categorical_accuracy: 0.7201 - val_loss: 0.5431 - val_categorical_accuracy: 0.7500\n",
      "\n",
      "Epoch 00008: saving model to model_init_2021-02-0807_27_51.966986/model-00008-0.60921-0.72021-0.54310-0.75000.h5\n",
      "Epoch 9/15\n",
      "133/133 [==============================] - 94s 705ms/step - loss: 0.5754 - categorical_accuracy: 0.7281 - val_loss: 0.4882 - val_categorical_accuracy: 0.7900\n",
      "\n",
      "Epoch 00009: saving model to model_init_2021-02-0807_27_51.966986/model-00009-0.57412-0.72926-0.48820-0.79000.h5\n",
      "Epoch 10/15\n",
      "133/133 [==============================] - 94s 709ms/step - loss: 0.5193 - categorical_accuracy: 0.7461 - val_loss: 0.4283 - val_categorical_accuracy: 0.8300\n",
      "\n",
      "Epoch 00010: saving model to model_init_2021-02-0807_27_51.966986/model-00010-0.52012-0.74585-0.42832-0.83000.h5\n",
      "Epoch 11/15\n",
      "133/133 [==============================] - 93s 700ms/step - loss: 0.4516 - categorical_accuracy: 0.7672 - val_loss: 0.4065 - val_categorical_accuracy: 0.8000\n",
      "\n",
      "Epoch 00011: saving model to model_init_2021-02-0807_27_51.966986/model-00011-0.45210-0.76697-0.40649-0.80000.h5\n",
      "Epoch 12/15\n",
      "133/133 [==============================] - 94s 704ms/step - loss: 0.4234 - categorical_accuracy: 0.7744 - val_loss: 0.4411 - val_categorical_accuracy: 0.8000\n",
      "\n",
      "Epoch 00012: saving model to model_init_2021-02-0807_27_51.966986/model-00012-0.42273-0.77526-0.44110-0.80000.h5\n",
      "Epoch 13/15\n",
      "133/133 [==============================] - 94s 707ms/step - loss: 0.3773 - categorical_accuracy: 0.8098 - val_loss: 0.3368 - val_categorical_accuracy: 0.8600\n",
      "\n",
      "Epoch 00013: saving model to model_init_2021-02-0807_27_51.966986/model-00013-0.37744-0.80920-0.33684-0.86000.h5\n",
      "Epoch 14/15\n",
      "133/133 [==============================] - 93s 703ms/step - loss: 0.3990 - categorical_accuracy: 0.7830 - val_loss: 0.3561 - val_categorical_accuracy: 0.8600\n",
      "\n",
      "Epoch 00014: saving model to model_init_2021-02-0807_27_51.966986/model-00014-0.39799-0.78281-0.35613-0.86000.h5\n",
      "Epoch 15/15\n",
      "133/133 [==============================] - 94s 705ms/step - loss: 0.3459 - categorical_accuracy: 0.8213 - val_loss: 0.3894 - val_categorical_accuracy: 0.8300\n",
      "\n",
      "Epoch 00015: saving model to model_init_2021-02-0807_27_51.966986/model-00015-0.34617-0.82127-0.38940-0.83000.h5\n"
     ]
    }
   ],
   "source": [
    "print(\"Total Params:\", rnn_cnn_14_model.count_params())\n",
    "history_model14=rnn_cnn_14.train_model(rnn_cnn_14_model,augment_data=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 15 The above model a good accuracy but it the model size is 50 MB , so lets retrain model 11 with more epochs and lesser layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNNCNN15(ModelBuilderMoreAugmentation):\n",
    "    \n",
    "    def define_model(self,lstm_cells=64,dense_neurons=64,dropout=0.25):\n",
    "\n",
    "        model = Sequential()\n",
    "\n",
    "        model.add(TimeDistributed(Conv2D(16, (3, 3) , padding='same', activation='relu'),\n",
    "                                  input_shape=(self.frames_to_sample,self.image_height,self.image_width,self.channels)))\n",
    "        model.add(TimeDistributed(BatchNormalization()))\n",
    "        model.add(TimeDistributed(MaxPooling2D((2, 2))))\n",
    "        \n",
    "        model.add(TimeDistributed(Conv2D(32, (3, 3) , padding='same', activation='relu')))\n",
    "        model.add(TimeDistributed(BatchNormalization()))\n",
    "        model.add(TimeDistributed(MaxPooling2D((2, 2))))\n",
    "        \n",
    "        model.add(TimeDistributed(Conv2D(64, (3, 3) , padding='same', activation='relu')))\n",
    "        model.add(TimeDistributed(BatchNormalization()))\n",
    "        model.add(TimeDistributed(MaxPooling2D((2, 2))))\n",
    "\n",
    "        model.add(TimeDistributed(Conv2D(128, (3, 3) , padding='same', activation='relu')))\n",
    "        model.add(TimeDistributed(BatchNormalization()))\n",
    "        model.add(TimeDistributed(MaxPooling2D((2, 2))))\n",
    "\n",
    "        model.add(TimeDistributed(Flatten()))\n",
    "\n",
    "\n",
    "        model.add(GRU(128))\n",
    "        model.add(Dropout(dropout))\n",
    "        #model.add(GRU(128))\n",
    "\n",
    "        model.add(Dense(dense_neurons,activation='relu'))\n",
    "\n",
    "        model.add(Dense(self.num_classes, activation='softmax'))\n",
    "        optimiser = optimizers.Adam(lr=0.0002)\n",
    "        model.compile(optimizer=optimiser, loss='categorical_crossentropy', metrics=['categorical_accuracy'])\n",
    "        return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "time_distributed_9 (TimeDist (None, 18, 120, 120, 16)  448       \n",
      "_________________________________________________________________\n",
      "time_distributed_10 (TimeDis (None, 18, 120, 120, 16)  64        \n",
      "_________________________________________________________________\n",
      "time_distributed_11 (TimeDis (None, 18, 60, 60, 16)    0         \n",
      "_________________________________________________________________\n",
      "time_distributed_12 (TimeDis (None, 18, 60, 60, 32)    4640      \n",
      "_________________________________________________________________\n",
      "time_distributed_13 (TimeDis (None, 18, 60, 60, 32)    128       \n",
      "_________________________________________________________________\n",
      "time_distributed_14 (TimeDis (None, 18, 30, 30, 32)    0         \n",
      "_________________________________________________________________\n",
      "time_distributed_15 (TimeDis (None, 18, 30, 30, 64)    18496     \n",
      "_________________________________________________________________\n",
      "time_distributed_16 (TimeDis (None, 18, 30, 30, 64)    256       \n",
      "_________________________________________________________________\n",
      "time_distributed_17 (TimeDis (None, 18, 15, 15, 64)    0         \n",
      "_________________________________________________________________\n",
      "time_distributed_18 (TimeDis (None, 18, 15, 15, 128)   73856     \n",
      "_________________________________________________________________\n",
      "time_distributed_19 (TimeDis (None, 18, 15, 15, 128)   512       \n",
      "_________________________________________________________________\n",
      "time_distributed_20 (TimeDis (None, 18, 7, 7, 128)     0         \n",
      "_________________________________________________________________\n",
      "time_distributed_21 (TimeDis (None, 18, 6272)          0         \n",
      "_________________________________________________________________\n",
      "gru_3 (GRU)                  (None, 128)               2457984   \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 128)               16512     \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 5)                 645       \n",
      "=================================================================\n",
      "Total params: 2,573,541\n",
      "Trainable params: 2,573,061\n",
      "Non-trainable params: 480\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "rnn_cnn15=RNNCNN15()\n",
    "rnn_cnn15.initialize_path(project_folder)\n",
    "rnn_cnn15.initialize_image_properties(image_height=120,image_width=120)\n",
    "rnn_cnn15.initialize_hyperparams(frames_to_sample=18,batch_size=32,num_epochs=18)\n",
    "rnn_cnn15_model=rnn_cnn15.define_model(lstm_cells=128,dense_neurons=128,dropout=0.25)\n",
    "rnn_cnn15_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Params: 2573541\n",
      "Epoch 1/18\n",
      "21/21 [==============================] - 112s 5s/step - loss: 1.4131 - categorical_accuracy: 0.3744 - val_loss: 1.1419 - val_categorical_accuracy: 0.5500\n",
      "\n",
      "Epoch 00001: saving model to model_init_2021-02-0808_22_48.607015/model-00001-1.41558-0.37330-1.14195-0.55000.h5\n",
      "Epoch 2/18\n",
      "21/21 [==============================] - 89s 4s/step - loss: 0.9820 - categorical_accuracy: 0.6219 - val_loss: 0.9366 - val_categorical_accuracy: 0.7000\n",
      "\n",
      "Epoch 00002: saving model to model_init_2021-02-0808_22_48.607015/model-00002-0.97911-0.62293-0.93657-0.70000.h5\n",
      "Epoch 3/18\n",
      "21/21 [==============================] - 99s 5s/step - loss: 0.7838 - categorical_accuracy: 0.7124 - val_loss: 0.9905 - val_categorical_accuracy: 0.6000\n",
      "\n",
      "Epoch 00003: saving model to model_init_2021-02-0808_22_48.607015/model-00003-0.78410-0.71116-0.99053-0.60000.h5\n",
      "Epoch 4/18\n",
      "21/21 [==============================] - 99s 5s/step - loss: 0.6161 - categorical_accuracy: 0.7915 - val_loss: 0.8702 - val_categorical_accuracy: 0.7000\n",
      "\n",
      "Epoch 00004: saving model to model_init_2021-02-0808_22_48.607015/model-00004-0.61655-0.79110-0.87018-0.70000.h5\n",
      "Epoch 5/18\n",
      "21/21 [==============================] - 99s 5s/step - loss: 0.4510 - categorical_accuracy: 0.8679 - val_loss: 0.8046 - val_categorical_accuracy: 0.6600\n",
      "\n",
      "Epoch 00005: saving model to model_init_2021-02-0808_22_48.607015/model-00005-0.45036-0.86727-0.80465-0.66000.h5\n",
      "Epoch 6/18\n",
      "21/21 [==============================] - 99s 5s/step - loss: 0.3663 - categorical_accuracy: 0.9043 - val_loss: 0.8728 - val_categorical_accuracy: 0.6600\n",
      "\n",
      "Epoch 00006: saving model to model_init_2021-02-0808_22_48.607015/model-00006-0.36569-0.90422-0.87283-0.66000.h5\n",
      "Epoch 7/18\n",
      "21/21 [==============================] - 99s 5s/step - loss: 0.3149 - categorical_accuracy: 0.9166 - val_loss: 0.7987 - val_categorical_accuracy: 0.7200\n",
      "\n",
      "Epoch 00007: saving model to model_init_2021-02-0808_22_48.607015/model-00007-0.31299-0.91780-0.79868-0.72000.h5\n",
      "Epoch 8/18\n",
      "21/21 [==============================] - 98s 5s/step - loss: 0.2411 - categorical_accuracy: 0.9409 - val_loss: 0.7746 - val_categorical_accuracy: 0.6900\n",
      "\n",
      "Epoch 00008: saving model to model_init_2021-02-0808_22_48.607015/model-00008-0.24187-0.94042-0.77462-0.69000.h5\n",
      "Epoch 9/18\n",
      "21/21 [==============================] - 98s 5s/step - loss: 0.2038 - categorical_accuracy: 0.9525 - val_loss: 0.7530 - val_categorical_accuracy: 0.6800\n",
      "\n",
      "Epoch 00009: saving model to model_init_2021-02-0808_22_48.607015/model-00009-0.20338-0.95249-0.75304-0.68000.h5\n",
      "Epoch 10/18\n",
      "21/21 [==============================] - 98s 5s/step - loss: 0.1654 - categorical_accuracy: 0.9640 - val_loss: 0.7274 - val_categorical_accuracy: 0.7400\n",
      "\n",
      "Epoch 00010: saving model to model_init_2021-02-0808_22_48.607015/model-00010-0.16552-0.96380-0.72743-0.74000.h5\n",
      "Epoch 11/18\n",
      "21/21 [==============================] - 97s 5s/step - loss: 0.1313 - categorical_accuracy: 0.9725 - val_loss: 0.7223 - val_categorical_accuracy: 0.7500\n",
      "\n",
      "Epoch 00011: saving model to model_init_2021-02-0808_22_48.607015/model-00011-0.13211-0.97210-0.72227-0.75000.h5\n",
      "Epoch 12/18\n",
      "21/21 [==============================] - 98s 5s/step - loss: 0.1123 - categorical_accuracy: 0.9784 - val_loss: 0.7240 - val_categorical_accuracy: 0.7200\n",
      "\n",
      "Epoch 00012: saving model to model_init_2021-02-0808_22_48.607015/model-00012-0.11273-0.97813-0.72403-0.72000.h5\n",
      "Epoch 13/18\n",
      "21/21 [==============================] - 97s 5s/step - loss: 0.0936 - categorical_accuracy: 0.9814 - val_loss: 0.7422 - val_categorical_accuracy: 0.7500\n",
      "\n",
      "Epoch 00013: saving model to model_init_2021-02-0808_22_48.607015/model-00013-0.09331-0.98115-0.74222-0.75000.h5\n",
      "Epoch 14/18\n",
      "21/21 [==============================] - 97s 5s/step - loss: 0.0709 - categorical_accuracy: 0.9873 - val_loss: 0.7151 - val_categorical_accuracy: 0.7600\n",
      "\n",
      "Epoch 00014: saving model to model_init_2021-02-0808_22_48.607015/model-00014-0.07114-0.98718-0.71512-0.76000.h5\n",
      "Epoch 15/18\n",
      "21/21 [==============================] - 96s 5s/step - loss: 0.0707 - categorical_accuracy: 0.9886 - val_loss: 0.6845 - val_categorical_accuracy: 0.7400\n",
      "\n",
      "Epoch 00015: saving model to model_init_2021-02-0808_22_48.607015/model-00015-0.07042-0.98869-0.68451-0.74000.h5\n",
      "Epoch 16/18\n",
      "21/21 [==============================] - 97s 5s/step - loss: 0.0599 - categorical_accuracy: 0.9893 - val_loss: 0.7376 - val_categorical_accuracy: 0.7500\n",
      "\n",
      "Epoch 00016: saving model to model_init_2021-02-0808_22_48.607015/model-00016-0.05975-0.98944-0.73759-0.75000.h5\n",
      "Epoch 17/18\n",
      "21/21 [==============================] - 98s 5s/step - loss: 0.0646 - categorical_accuracy: 0.9846 - val_loss: 0.6099 - val_categorical_accuracy: 0.8000\n",
      "\n",
      "Epoch 00017: saving model to model_init_2021-02-0808_22_48.607015/model-00017-0.06362-0.98492-0.60992-0.80000.h5\n",
      "Epoch 18/18\n",
      "21/21 [==============================] - 98s 5s/step - loss: 0.0591 - categorical_accuracy: 0.9856 - val_loss: 0.7705 - val_categorical_accuracy: 0.7600\n",
      "\n",
      "Epoch 00018: saving model to model_init_2021-02-0808_22_48.607015/model-00018-0.05915-0.98567-0.77048-0.76000.h5\n"
     ]
    }
   ],
   "source": [
    "print(\"Total Params:\", rnn_cnn15_model.count_params())\n",
    "history_model15=rnn_cnn15.train_model(rnn_cnn15_model,augment_data=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 16, with less layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNNCNN16(ModelBuilderMoreAugmentation):\n",
    "    \n",
    "    def define_model(self,lstm_cells=64,dense_neurons=64,dropout=0.25):\n",
    "\n",
    "        model = Sequential()\n",
    "\n",
    "        model.add(TimeDistributed(Conv2D(16, (3, 3) , padding='same', activation='relu'),\n",
    "                                  input_shape=(self.frames_to_sample,self.image_height,self.image_width,self.channels)))\n",
    "        model.add(TimeDistributed(BatchNormalization()))\n",
    "        model.add(TimeDistributed(MaxPooling2D((2, 2))))\n",
    "        \n",
    "        model.add(TimeDistributed(Conv2D(32, (3, 3) , padding='same', activation='relu')))\n",
    "        model.add(TimeDistributed(BatchNormalization()))\n",
    "        model.add(TimeDistributed(MaxPooling2D((2, 2))))\n",
    "        \n",
    "        model.add(TimeDistributed(Conv2D(64, (3, 3) , padding='same', activation='relu')))\n",
    "        model.add(TimeDistributed(BatchNormalization()))\n",
    "        model.add(TimeDistributed(MaxPooling2D((2, 2))))\n",
    "\n",
    "        model.add(TimeDistributed(Conv2D(128, (3, 3) , activation='relu')))\n",
    "        model.add(TimeDistributed(BatchNormalization()))\n",
    "        #model.add(TimeDistributed(MaxPooling2D((2, 2))))\n",
    "\n",
    "        model.add(TimeDistributed(Flatten()))\n",
    "\n",
    "\n",
    "        model.add(GRU(128))\n",
    "        model.add(Dropout(dropout))\n",
    "        #model.add(GRU(128))\n",
    "\n",
    "        model.add(Dense(self.num_classes, activation='softmax'))\n",
    "        optimiser = optimizers.Adam(lr=0.0002)\n",
    "        model.compile(optimizer=optimiser, loss='categorical_crossentropy', metrics=['categorical_accuracy'])\n",
    "        return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "time_distributed_58 (TimeDis (None, 18, 120, 120, 16)  448       \n",
      "_________________________________________________________________\n",
      "time_distributed_59 (TimeDis (None, 18, 120, 120, 16)  64        \n",
      "_________________________________________________________________\n",
      "time_distributed_60 (TimeDis (None, 18, 60, 60, 16)    0         \n",
      "_________________________________________________________________\n",
      "time_distributed_61 (TimeDis (None, 18, 60, 60, 32)    4640      \n",
      "_________________________________________________________________\n",
      "time_distributed_62 (TimeDis (None, 18, 60, 60, 32)    128       \n",
      "_________________________________________________________________\n",
      "time_distributed_63 (TimeDis (None, 18, 30, 30, 32)    0         \n",
      "_________________________________________________________________\n",
      "time_distributed_64 (TimeDis (None, 18, 30, 30, 64)    18496     \n",
      "_________________________________________________________________\n",
      "time_distributed_65 (TimeDis (None, 18, 30, 30, 64)    256       \n",
      "_________________________________________________________________\n",
      "time_distributed_66 (TimeDis (None, 18, 15, 15, 64)    0         \n",
      "_________________________________________________________________\n",
      "time_distributed_67 (TimeDis (None, 18, 13, 13, 128)   73856     \n",
      "_________________________________________________________________\n",
      "time_distributed_68 (TimeDis (None, 18, 13, 13, 128)   512       \n",
      "_________________________________________________________________\n",
      "time_distributed_69 (TimeDis (None, 18, 21632)         0         \n",
      "_________________________________________________________________\n",
      "gru_7 (GRU)                  (None, 128)               8356224   \n",
      "_________________________________________________________________\n",
      "dropout_9 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_12 (Dense)             (None, 5)                 645       \n",
      "=================================================================\n",
      "Total params: 8,455,269\n",
      "Trainable params: 8,454,789\n",
      "Non-trainable params: 480\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "rnn_cnn16=RNNCNN16()\n",
    "rnn_cnn16.initialize_path(project_folder)\n",
    "rnn_cnn16.initialize_image_properties(image_height=120,image_width=120)\n",
    "rnn_cnn16.initialize_hyperparams(frames_to_sample=18,batch_size=32,num_epochs=18)\n",
    "rnn_cnn16_model=rnn_cnn16.define_model(lstm_cells=128,dense_neurons=128,dropout=0.25)\n",
    "rnn_cnn16_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Params: 8455269\n",
      "Epoch 1/18\n",
      "21/21 [==============================] - 108s 5s/step - loss: 1.2558 - categorical_accuracy: 0.5314 - val_loss: 1.0492 - val_categorical_accuracy: 0.6600\n",
      "\n",
      "Epoch 00001: saving model to model_init_2021-02-0808_59_00.024434/model-00001-1.25996-0.53017-1.04916-0.66000.h5\n",
      "Epoch 2/18\n",
      "21/21 [==============================] - 90s 4s/step - loss: 0.4482 - categorical_accuracy: 0.8345 - val_loss: 0.8395 - val_categorical_accuracy: 0.6800\n",
      "\n",
      "Epoch 00002: saving model to model_init_2021-02-0808_59_00.024434/model-00002-0.44879-0.83484-0.83951-0.68000.h5\n",
      "Epoch 3/18\n",
      "21/21 [==============================] - 99s 5s/step - loss: 0.2211 - categorical_accuracy: 0.9326 - val_loss: 0.8281 - val_categorical_accuracy: 0.6800\n",
      "\n",
      "Epoch 00003: saving model to model_init_2021-02-0808_59_00.024434/model-00003-0.22047-0.93288-0.82813-0.68000.h5\n",
      "Epoch 4/18\n",
      "21/21 [==============================] - 99s 5s/step - loss: 0.1790 - categorical_accuracy: 0.9429 - val_loss: 0.8357 - val_categorical_accuracy: 0.7500\n",
      "\n",
      "Epoch 00004: saving model to model_init_2021-02-0808_59_00.024434/model-00004-0.17963-0.94268-0.83571-0.75000.h5\n",
      "Epoch 5/18\n",
      "21/21 [==============================] - 98s 5s/step - loss: 0.1166 - categorical_accuracy: 0.9726 - val_loss: 0.9025 - val_categorical_accuracy: 0.7100\n",
      "\n",
      "Epoch 00005: saving model to model_init_2021-02-0808_59_00.024434/model-00005-0.11619-0.97285-0.90248-0.71000.h5\n",
      "Epoch 6/18\n",
      "21/21 [==============================] - 99s 5s/step - loss: 0.0970 - categorical_accuracy: 0.9747 - val_loss: 0.8894 - val_categorical_accuracy: 0.7300\n",
      "\n",
      "Epoch 00006: saving model to model_init_2021-02-0808_59_00.024434/model-00006-0.09727-0.97436-0.88936-0.73000.h5\n",
      "Epoch 7/18\n",
      "21/21 [==============================] - 98s 5s/step - loss: 0.0661 - categorical_accuracy: 0.9911 - val_loss: 0.7823 - val_categorical_accuracy: 0.7700\n",
      "\n",
      "Epoch 00007: saving model to model_init_2021-02-0808_59_00.024434/model-00007-0.06607-0.99095-0.78229-0.77000.h5\n",
      "Epoch 8/18\n",
      "21/21 [==============================] - 98s 5s/step - loss: 0.0578 - categorical_accuracy: 0.9918 - val_loss: 0.8430 - val_categorical_accuracy: 0.7300\n",
      "\n",
      "Epoch 00008: saving model to model_init_2021-02-0808_59_00.024434/model-00008-0.05778-0.99170-0.84302-0.73000.h5\n",
      "Epoch 9/18\n",
      "21/21 [==============================] - 98s 5s/step - loss: 0.0421 - categorical_accuracy: 0.9970 - val_loss: 0.8540 - val_categorical_accuracy: 0.7500\n",
      "\n",
      "Epoch 00009: saving model to model_init_2021-02-0808_59_00.024434/model-00009-0.04217-0.99698-0.85401-0.75000.h5\n",
      "Epoch 10/18\n",
      "21/21 [==============================] - 99s 5s/step - loss: 0.0419 - categorical_accuracy: 0.9953 - val_loss: 0.8873 - val_categorical_accuracy: 0.7000\n",
      "\n",
      "Epoch 00010: saving model to model_init_2021-02-0808_59_00.024434/model-00010-0.04195-0.99548-0.88732-0.70000.h5\n",
      "Epoch 11/18\n",
      "21/21 [==============================] - 99s 5s/step - loss: 0.0337 - categorical_accuracy: 0.9963 - val_loss: 0.8201 - val_categorical_accuracy: 0.7300\n",
      "\n",
      "Epoch 00011: saving model to model_init_2021-02-0808_59_00.024434/model-00011-0.03384-0.99623-0.82009-0.73000.h5\n",
      "\n",
      "Epoch 00011: ReduceLROnPlateau reducing learning rate to 3.9999998989515007e-05.\n",
      "Epoch 12/18\n",
      "21/21 [==============================] - 96s 5s/step - loss: 0.0268 - categorical_accuracy: 1.0000 - val_loss: 0.8212 - val_categorical_accuracy: 0.7200\n",
      "\n",
      "Epoch 00012: saving model to model_init_2021-02-0808_59_00.024434/model-00012-0.02690-1.00000-0.82116-0.72000.h5\n",
      "Epoch 13/18\n",
      "21/21 [==============================] - 98s 5s/step - loss: 0.0247 - categorical_accuracy: 0.9993 - val_loss: 0.8168 - val_categorical_accuracy: 0.7400\n",
      "\n",
      "Epoch 00013: saving model to model_init_2021-02-0808_59_00.024434/model-00013-0.02454-0.99925-0.81682-0.74000.h5\n",
      "Epoch 14/18\n",
      "21/21 [==============================] - 98s 5s/step - loss: 0.0274 - categorical_accuracy: 0.9978 - val_loss: 0.8155 - val_categorical_accuracy: 0.7500\n",
      "\n",
      "Epoch 00014: saving model to model_init_2021-02-0808_59_00.024434/model-00014-0.02718-0.99774-0.81552-0.75000.h5\n",
      "Epoch 15/18\n",
      "21/21 [==============================] - 98s 5s/step - loss: 0.0234 - categorical_accuracy: 0.9985 - val_loss: 0.8189 - val_categorical_accuracy: 0.7400\n",
      "\n",
      "Epoch 00015: saving model to model_init_2021-02-0808_59_00.024434/model-00015-0.02338-0.99849-0.81891-0.74000.h5\n",
      "\n",
      "Epoch 00015: ReduceLROnPlateau reducing learning rate to 7.999999797903002e-06.\n",
      "Epoch 16/18\n",
      "21/21 [==============================] - 97s 5s/step - loss: 0.0237 - categorical_accuracy: 0.9985 - val_loss: 0.8138 - val_categorical_accuracy: 0.7400\n",
      "\n",
      "Epoch 00016: saving model to model_init_2021-02-0808_59_00.024434/model-00016-0.02361-0.99849-0.81381-0.74000.h5\n",
      "Epoch 17/18\n",
      "21/21 [==============================] - 99s 5s/step - loss: 0.0234 - categorical_accuracy: 0.9993 - val_loss: 0.8097 - val_categorical_accuracy: 0.7400\n",
      "\n",
      "Epoch 00017: saving model to model_init_2021-02-0808_59_00.024434/model-00017-0.02340-0.99925-0.80972-0.74000.h5\n",
      "Epoch 18/18\n",
      "21/21 [==============================] - 99s 5s/step - loss: 0.0198 - categorical_accuracy: 0.9993 - val_loss: 0.8125 - val_categorical_accuracy: 0.7400\n",
      "\n",
      "Epoch 00018: saving model to model_init_2021-02-0808_59_00.024434/model-00018-0.01972-0.99925-0.81246-0.74000.h5\n"
     ]
    }
   ],
   "source": [
    "print(\"Total Params:\", rnn_cnn16_model.count_params())\n",
    "history_model16=rnn_cnn16.train_model(rnn_cnn16_model,augment_data=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### REtraining model 7 with higher epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class ModelConv3D17(ModelBuilderMoreAugmentation):\n",
    "    \n",
    "    def define_model(self,filtersize=(3,3,3),dense_neurons=64,dropout=0.25):\n",
    "\n",
    "        model = Sequential()\n",
    "        model.add(Conv3D(16, filtersize, padding='same',\n",
    "                 input_shape=(self.frames_to_sample,self.image_height,self.image_width,self.channels)))\n",
    "        model.add(Activation('relu'))\n",
    "        model.add(BatchNormalization())\n",
    "        model.add(MaxPooling3D(pool_size=(2, 2, 2)))\n",
    "\n",
    "        model.add(Conv3D(32, filtersize, padding='same'))\n",
    "        model.add(Activation('relu'))\n",
    "        model.add(BatchNormalization())\n",
    "        model.add(MaxPooling3D(pool_size=(2, 2, 2)))\n",
    "\n",
    "        model.add(Conv3D(64, filtersize, padding='same'))\n",
    "        model.add(Activation('relu'))\n",
    "        model.add(BatchNormalization())\n",
    "        model.add(MaxPooling3D(pool_size=(2, 2, 2)))\n",
    "\n",
    "        model.add(Conv3D(128, filtersize, padding='same'))\n",
    "        model.add(Activation('relu'))\n",
    "        model.add(BatchNormalization())\n",
    "        model.add(MaxPooling3D(pool_size=(2, 2, 2)))\n",
    "\n",
    "        model.add(Flatten())\n",
    "        model.add(Dense(dense_neurons,activation='relu'))\n",
    "        model.add(BatchNormalization())\n",
    "        model.add(Dropout(dropout))\n",
    "\n",
    "        model.add(Dense(dense_neurons,activation='relu'))\n",
    "        model.add(BatchNormalization())\n",
    "        model.add(Dropout(dropout))\n",
    "\n",
    "\n",
    "        model.add(Dense(self.num_classes,activation='softmax'))\n",
    "\n",
    "        optimiser = optimizers.Adam(lr=0.0002)\n",
    "        model.compile(optimizer=optimiser, loss='categorical_crossentropy', metrics=['categorical_accuracy'])\n",
    "        return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv3d_1 (Conv3D)            (None, 20, 160, 160, 16)  1312      \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 20, 160, 160, 16)  0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_22 (Batc (None, 20, 160, 160, 16)  64        \n",
      "_________________________________________________________________\n",
      "max_pooling3d_1 (MaxPooling3 (None, 10, 80, 80, 16)    0         \n",
      "_________________________________________________________________\n",
      "conv3d_2 (Conv3D)            (None, 10, 80, 80, 32)    13856     \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 10, 80, 80, 32)    0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_23 (Batc (None, 10, 80, 80, 32)    128       \n",
      "_________________________________________________________________\n",
      "max_pooling3d_2 (MaxPooling3 (None, 5, 40, 40, 32)     0         \n",
      "_________________________________________________________________\n",
      "conv3d_3 (Conv3D)            (None, 5, 40, 40, 64)     55360     \n",
      "_________________________________________________________________\n",
      "activation_3 (Activation)    (None, 5, 40, 40, 64)     0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_24 (Batc (None, 5, 40, 40, 64)     256       \n",
      "_________________________________________________________________\n",
      "max_pooling3d_3 (MaxPooling3 (None, 2, 20, 20, 64)     0         \n",
      "_________________________________________________________________\n",
      "conv3d_4 (Conv3D)            (None, 2, 20, 20, 128)    221312    \n",
      "_________________________________________________________________\n",
      "activation_4 (Activation)    (None, 2, 20, 20, 128)    0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_25 (Batc (None, 2, 20, 20, 128)    512       \n",
      "_________________________________________________________________\n",
      "max_pooling3d_4 (MaxPooling3 (None, 1, 10, 10, 128)    0         \n",
      "_________________________________________________________________\n",
      "flatten_8 (Flatten)          (None, 12800)             0         \n",
      "_________________________________________________________________\n",
      "dense_13 (Dense)             (None, 256)               3277056   \n",
      "_________________________________________________________________\n",
      "batch_normalization_26 (Batc (None, 256)               1024      \n",
      "_________________________________________________________________\n",
      "dropout_10 (Dropout)         (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_14 (Dense)             (None, 256)               65792     \n",
      "_________________________________________________________________\n",
      "batch_normalization_27 (Batc (None, 256)               1024      \n",
      "_________________________________________________________________\n",
      "dropout_11 (Dropout)         (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_15 (Dense)             (None, 5)                 1285      \n",
      "=================================================================\n",
      "Total params: 3,638,981\n",
      "Trainable params: 3,637,477\n",
      "Non-trainable params: 1,504\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "\n",
    "conv_3d17=ModelConv3D17()\n",
    "conv_3d17.initialize_path(project_folder)\n",
    "conv_3d17.initialize_image_properties(image_height=160,image_width=160)\n",
    "conv_3d17.initialize_hyperparams(frames_to_sample=20,batch_size=20,num_epochs=30)\n",
    "conv_3d17_model=conv_3d17.define_model(dense_neurons=256,dropout=0.5)\n",
    "conv_3d17_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Params: 3638981\n",
      "Epoch 1/30\n",
      "34/34 [==============================] - 161s 5s/step - loss: 1.9077 - categorical_accuracy: 0.3786 - val_loss: 2.4778 - val_categorical_accuracy: 0.4200\n",
      "\n",
      "Epoch 00001: saving model to model_init_2021-02-0809_28_54.580541/model-00001-1.91504-0.37557-2.47784-0.42000.h5\n",
      "Epoch 2/30\n",
      "34/34 [==============================] - 122s 4s/step - loss: 1.3692 - categorical_accuracy: 0.5239 - val_loss: 1.0299 - val_categorical_accuracy: 0.5800\n",
      "\n",
      "Epoch 00002: saving model to model_init_2021-02-0809_28_54.580541/model-00002-1.39126-0.52036-1.02993-0.58000.h5\n",
      "Epoch 3/30\n",
      "34/34 [==============================] - 134s 4s/step - loss: 1.2076 - categorical_accuracy: 0.5883 - val_loss: 0.9487 - val_categorical_accuracy: 0.6600\n",
      "\n",
      "Epoch 00003: saving model to model_init_2021-02-0809_28_54.580541/model-00003-1.18657-0.59050-0.94872-0.66000.h5\n",
      "Epoch 4/30\n",
      "34/34 [==============================] - 133s 4s/step - loss: 1.0710 - categorical_accuracy: 0.6133 - val_loss: 1.2644 - val_categorical_accuracy: 0.5300\n",
      "\n",
      "Epoch 00004: saving model to model_init_2021-02-0809_28_54.580541/model-00004-1.06018-0.61614-1.26439-0.53000.h5\n",
      "Epoch 5/30\n",
      "34/34 [==============================] - 133s 4s/step - loss: 1.0201 - categorical_accuracy: 0.6218 - val_loss: 1.4520 - val_categorical_accuracy: 0.4400\n",
      "\n",
      "Epoch 00005: saving model to model_init_2021-02-0809_28_54.580541/model-00005-1.02503-0.62066-1.45200-0.44000.h5\n",
      "Epoch 6/30\n",
      "34/34 [==============================] - 133s 4s/step - loss: 0.9005 - categorical_accuracy: 0.6678 - val_loss: 1.0039 - val_categorical_accuracy: 0.6100\n",
      "\n",
      "Epoch 00006: saving model to model_init_2021-02-0809_28_54.580541/model-00006-0.91035-0.66365-1.00392-0.61000.h5\n",
      "Epoch 7/30\n",
      "34/34 [==============================] - 132s 4s/step - loss: 0.8167 - categorical_accuracy: 0.6928 - val_loss: 0.9218 - val_categorical_accuracy: 0.7100\n",
      "\n",
      "Epoch 00007: saving model to model_init_2021-02-0809_28_54.580541/model-00007-0.82227-0.68929-0.92178-0.71000.h5\n",
      "Epoch 8/30\n",
      "34/34 [==============================] - 132s 4s/step - loss: 0.7722 - categorical_accuracy: 0.7292 - val_loss: 1.0493 - val_categorical_accuracy: 0.6700\n",
      "\n",
      "Epoch 00008: saving model to model_init_2021-02-0809_28_54.580541/model-00008-0.70071-0.73906-1.04929-0.67000.h5\n",
      "Epoch 9/30\n",
      "34/34 [==============================] - 132s 4s/step - loss: 0.8656 - categorical_accuracy: 0.6939 - val_loss: 1.0264 - val_categorical_accuracy: 0.6700\n",
      "\n",
      "Epoch 00009: saving model to model_init_2021-02-0809_28_54.580541/model-00009-0.84267-0.70287-1.02644-0.67000.h5\n",
      "Epoch 10/30\n",
      "34/34 [==============================] - 132s 4s/step - loss: 0.7804 - categorical_accuracy: 0.7134 - val_loss: 0.9340 - val_categorical_accuracy: 0.7500\n",
      "\n",
      "Epoch 00010: saving model to model_init_2021-02-0809_28_54.580541/model-00010-0.79075-0.71041-0.93396-0.75000.h5\n",
      "Epoch 11/30\n",
      "34/34 [==============================] - 131s 4s/step - loss: 0.6567 - categorical_accuracy: 0.7528 - val_loss: 0.8981 - val_categorical_accuracy: 0.7300\n",
      "\n",
      "Epoch 00011: saving model to model_init_2021-02-0809_28_54.580541/model-00011-0.64224-0.75490-0.89810-0.73000.h5\n",
      "Epoch 12/30\n",
      "34/34 [==============================] - 130s 4s/step - loss: 0.6726 - categorical_accuracy: 0.7539 - val_loss: 0.8986 - val_categorical_accuracy: 0.7100\n",
      "\n",
      "Epoch 00012: saving model to model_init_2021-02-0809_28_54.580541/model-00012-0.64775-0.76018-0.89860-0.71000.h5\n",
      "Epoch 13/30\n",
      "34/34 [==============================] - 131s 4s/step - loss: 0.6265 - categorical_accuracy: 0.7822 - val_loss: 0.9436 - val_categorical_accuracy: 0.6700\n",
      "\n",
      "Epoch 00013: saving model to model_init_2021-02-0809_28_54.580541/model-00013-0.60446-0.78507-0.94361-0.67000.h5\n",
      "Epoch 14/30\n",
      "34/34 [==============================] - 131s 4s/step - loss: 0.6374 - categorical_accuracy: 0.7609 - val_loss: 1.1277 - val_categorical_accuracy: 0.6500\n",
      "\n",
      "Epoch 00014: saving model to model_init_2021-02-0809_28_54.580541/model-00014-0.57724-0.77149-1.12771-0.65000.h5\n",
      "Epoch 15/30\n",
      "34/34 [==============================] - 131s 4s/step - loss: 0.4948 - categorical_accuracy: 0.8149 - val_loss: 0.8702 - val_categorical_accuracy: 0.7200\n",
      "\n",
      "Epoch 00015: saving model to model_init_2021-02-0809_28_54.580541/model-00015-0.49061-0.81448-0.87017-0.72000.h5\n",
      "Epoch 16/30\n",
      "34/34 [==============================] - 130s 4s/step - loss: 0.4858 - categorical_accuracy: 0.8260 - val_loss: 0.9621 - val_categorical_accuracy: 0.7000\n",
      "\n",
      "Epoch 00016: saving model to model_init_2021-02-0809_28_54.580541/model-00016-0.48421-0.82579-0.96214-0.70000.h5\n",
      "Epoch 17/30\n",
      "34/34 [==============================] - 131s 4s/step - loss: 0.4536 - categorical_accuracy: 0.8311 - val_loss: 0.9509 - val_categorical_accuracy: 0.7000\n",
      "\n",
      "Epoch 00017: saving model to model_init_2021-02-0809_28_54.580541/model-00017-0.42723-0.83937-0.95086-0.70000.h5\n",
      "Epoch 18/30\n",
      "34/34 [==============================] - 130s 4s/step - loss: 0.4704 - categorical_accuracy: 0.8297 - val_loss: 0.9522 - val_categorical_accuracy: 0.7300\n",
      "\n",
      "Epoch 00018: saving model to model_init_2021-02-0809_28_54.580541/model-00018-0.39869-0.84615-0.95218-0.73000.h5\n",
      "Epoch 19/30\n",
      "34/34 [==============================] - 130s 4s/step - loss: 0.4479 - categorical_accuracy: 0.8366 - val_loss: 0.6865 - val_categorical_accuracy: 0.7700\n",
      "\n",
      "Epoch 00019: saving model to model_init_2021-02-0809_28_54.580541/model-00019-0.42508-0.84087-0.68651-0.77000.h5\n",
      "Epoch 20/30\n",
      "34/34 [==============================] - 131s 4s/step - loss: 0.4536 - categorical_accuracy: 0.8300 - val_loss: 0.7400 - val_categorical_accuracy: 0.8000\n",
      "\n",
      "Epoch 00020: saving model to model_init_2021-02-0809_28_54.580541/model-00020-0.43063-0.84238-0.73998-0.80000.h5\n",
      "Epoch 21/30\n",
      "34/34 [==============================] - 130s 4s/step - loss: 0.3127 - categorical_accuracy: 0.8812 - val_loss: 0.8752 - val_categorical_accuracy: 0.7500\n",
      "\n",
      "Epoch 00021: saving model to model_init_2021-02-0809_28_54.580541/model-00021-0.31124-0.88235-0.87524-0.75000.h5\n",
      "Epoch 22/30\n",
      "34/34 [==============================] - 130s 4s/step - loss: 0.3990 - categorical_accuracy: 0.8698 - val_loss: 0.6950 - val_categorical_accuracy: 0.7800\n",
      "\n",
      "Epoch 00022: saving model to model_init_2021-02-0809_28_54.580541/model-00022-0.35836-0.88311-0.69495-0.78000.h5\n",
      "Epoch 23/30\n",
      "34/34 [==============================] - 129s 4s/step - loss: 0.4603 - categorical_accuracy: 0.8407 - val_loss: 1.0154 - val_categorical_accuracy: 0.6500\n",
      "\n",
      "Epoch 00023: saving model to model_init_2021-02-0809_28_54.580541/model-00023-0.45638-0.84087-1.01537-0.65000.h5\n",
      "\n",
      "Epoch 00023: ReduceLROnPlateau reducing learning rate to 3.9999998989515007e-05.\n",
      "Epoch 24/30\n",
      "34/34 [==============================] - 128s 4s/step - loss: 0.3143 - categorical_accuracy: 0.8749 - val_loss: 0.8151 - val_categorical_accuracy: 0.7200\n",
      "\n",
      "Epoch 00024: saving model to model_init_2021-02-0809_28_54.580541/model-00024-0.30972-0.88009-0.81515-0.72000.h5\n",
      "Epoch 25/30\n",
      "34/34 [==============================] - 131s 4s/step - loss: 0.3143 - categorical_accuracy: 0.8863 - val_loss: 0.7099 - val_categorical_accuracy: 0.7600\n",
      "\n",
      "Epoch 00025: saving model to model_init_2021-02-0809_28_54.580541/model-00025-0.30873-0.88763-0.70990-0.76000.h5\n",
      "Epoch 26/30\n",
      "34/34 [==============================] - 130s 4s/step - loss: 0.2952 - categorical_accuracy: 0.8940 - val_loss: 0.6786 - val_categorical_accuracy: 0.7700\n",
      "\n",
      "Epoch 00026: saving model to model_init_2021-02-0809_28_54.580541/model-00026-0.28148-0.89970-0.67865-0.77000.h5\n",
      "Epoch 27/30\n",
      "34/34 [==============================] - 130s 4s/step - loss: 0.2722 - categorical_accuracy: 0.9021 - val_loss: 0.6254 - val_categorical_accuracy: 0.8000\n",
      "\n",
      "Epoch 00027: saving model to model_init_2021-02-0809_28_54.580541/model-00027-0.27492-0.89970-0.62537-0.80000.h5\n",
      "Epoch 28/30\n",
      "34/34 [==============================] - 130s 4s/step - loss: 0.2615 - categorical_accuracy: 0.8966 - val_loss: 0.6795 - val_categorical_accuracy: 0.8000\n",
      "\n",
      "Epoch 00028: saving model to model_init_2021-02-0809_28_54.580541/model-00028-0.26356-0.89819-0.67949-0.80000.h5\n",
      "Epoch 29/30\n",
      "34/34 [==============================] - 130s 4s/step - loss: 0.2615 - categorical_accuracy: 0.9021 - val_loss: 0.6004 - val_categorical_accuracy: 0.8400\n",
      "\n",
      "Epoch 00029: saving model to model_init_2021-02-0809_28_54.580541/model-00029-0.26639-0.89970-0.60035-0.84000.h5\n",
      "Epoch 30/30\n",
      "34/34 [==============================] - 130s 4s/step - loss: 0.2586 - categorical_accuracy: 0.9032 - val_loss: 0.6486 - val_categorical_accuracy: 0.8200\n",
      "\n",
      "Epoch 00030: saving model to model_init_2021-02-0809_28_54.580541/model-00030-0.23481-0.91327-0.64857-0.82000.h5\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(\"Total Params:\", conv_3d17_model.count_params())\n",
    "history_model17=conv_3d17.train_model(conv_3d17_model,augment_data=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### End of Notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
